[
  {
    "path": "posts/2020-01-21-is-evolutionary-psychology-impossible/",
    "title": "Is evolutionary psychology impossible?",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2020-01-21",
    "categories": [],
    "contents": "\nSubrena Smith recently argued that “evolutionary psychology, as it is currently understood, is…impossible” (Smith 2019). I agree with most of Smith’s premises, which are based on the logic of evolution by natural selection, and I think other evolutionary psychologists would too. So why does Smith conclude that evolutionary psychology (EP) is impossible but I conclude the opposite? Well, actually, Smith provides an example of evolved psychology that she considers “highly plausible,” directly contradicting her strong conclusion that EP is impossible. As I will show, this highly plausible example shares essential features with broad swaths of research in EP.\nSmith does a decent job capturing the core tenets of EP: much like the rest of the body, the brain comprises many distinct functional components, termed psychological adaptations. These evolved by natural selection to solve particular computational problems related to survival and reproduction in ancestral environments, otherwise known as the environment of evolutionary adaptedness (EEA).\nAccording to EP, evidence for the existence of a psychological adaptation includes (1) evidence of a computational problem posed by the ancestral environment (e.g., recognizing different types of foods and other objects); (2) evidence or argument that solving this computational problem would have increased the biological fitness of ancestors with the trait relative to those without it (e.g., by enabling them to find more food); (3) evidence that humans have the computational ability to solve precisely this problem (e.g., are able to rapidly recognize a large number of objects), which would typically include details about the specific algorithms involved; and (4) evidence that the computational ability reliably develops in (almost) all humans (of at least one sex), in all environments that do not deviate too much from the EEA. Together, these are taken as evidence of design – qualities that have been recognized since antiquity to distinguish “beneficial” organism traits from other natural phenomena (Aristotle, Physics II 8), and for which Darwin’s theory of natural selection provides the modern scientific explanation (Darwin 1859; Williams 1966).\nVision and hearing are uncontroversial examples of psychological adaptations, whereas mate preferences that differ between the sexes are somewhat controversial examples.\nSmith’s main critique of EP involves what she calls the “matching problem.” For a cognitive trait such as object recognition to be an adaptation, it must be the case that not only does this trait reliably develop in modern humans, but it must have reliably developed in ancestral humans too, which Smith terms “strong vertical homology”:\n\nFor a contemporary trait to be a strong vertical homolog of an ancestral trait, the contemporary trait must be of the same kind as the ancestral one. It must also have the same function as the ancestral one, and must be related by descent to that ancestral trait as part of a continuous reproductive lineage extending back to the EEA. Additionally, it must be the case that the contemporary trait and the ancestral trait are of the same kind and have the same function because the present-day trait is descended from the ancestral trait.\n\nI agree, and I think most evolutionary psychologists would too. In essence, Smith has added more requirements to the already long list that distinguish adaptations from other traits.\nSmith then identifies the “matching problem”: how can we know that ancestral humans had the cognitive trait in question? After all, cognitive traits don’t fossilize. Furthermore, complex cognitive abilities can be acquired via learning. As one example, Smith points to reading.\nEP would exclude reading cognition as a possible psychological adaptation because reading abilities do not reliably develop in all humans – many populations are non-literate, and many individuals within populations can fail to learn how to read. Still, it’s not hard to imagine an alternate universe where EP emerges as a discipline after literacy had become essentially universal and the existence of non-literate populations had been forgotten. Reading would then satisfy all three criteria above: it’s a complex cognitive ability, it provides many benefits, and (in the alternate universe) it reliably develops in essentially all modern humans. Yet the inference that reading is an adaptation would be incorrect. As Smith argues:\n\n[E]volutionary psychological claims fail unless practitioners can show that mental structures underpinning present-day behaviors are structures that evolved in the EEA for the performance of adaptive tasks that it is still their function to perform. This is the matching problem.\n\nAt first, it seems like the matching problem is insurmountable. How can we possibly know the mental structures of ancestors living hundreds of thousands or millions of years ago?\nIn the face of such a barrier to scientific investigation, Smith seems to favor what she views as the evolutionary alternative to EP, that “evolution fashioned the human mind as a domain-general or modestly modular learning system.” This leaves the impression that because (in her view) EP hypotheses cannot be tested, they are therefore false, and that because (in her view) a domain-general learning system can be tested, it is more likely to be true. No such inference is possible, of course. At best, based on her analysis, we would have to admit that EP hypotheses could be true but we can’t collect some of the evidence needed to test them.\nFortunately, Smith herself provides a solution to the matching problem:\n\nTo appreciate the differences between good evolutionary biological inferences and the inferences made in evolutionary psychological studies such as the one described in the previous section, consider a highly plausible evolutionary account – the claim that the eye-blink reflex (corneal reflex) was selected for as a mechanism to protect the eye from injury (Hall 1945). There are several converging lines of evidence that give substance to this claim. The first is that the reflex is highly conserved, as it is found in all mammalian taxa, and even in other taxa such as avians. Thus, comparative methods suggest that the reflex is under genetic control and that it was retained in the lineage because of its function.1\n\nSo, it turns out that ordinary homology provides convincing evidence for strong vertical homology, and thus solves the matching problem.\nAlthough not acknowledged by Smith, evolutionary psychologists emphasize the importance of comparative data, including both homologies and analogies (see Figure 1).\n\n\n\nFigure 1: Schematic representation of the different forms of evidence used to evaluate the validity of psychological adaptations. AI: artificial intelligence. Figure and caption from Schmitt and Pilcher (2004).\n\n\n\nBroad swaths of evolutionary psychology draw on homologies between human psychology and the psychology of our primate and mammalian relatives. There is even an Oxford Handbook of Comparative Evolutionary Psychology (Vonk & Shackelford 2012).\n\n\n\nFigure 2: Oxford Handbook of Comparative Evolutionary Psychology, Vonk & Shackelford, Eds. (2012).\n\n\n\nExamples of comparative evolutionary psychology include spatial memory (Haun et al. 2006), pathogen avoidance (Schaller 2015), attachment and maternal care (Maestripieri & Roney 2006), the expression of emotions (Darwin 1872), prosociality (Silk and House 2012), the biological roots of music (Hagen and Hammerstein, 2009), and social learning (Whiten 2017). To the degree that the evidence supports homology (and can rule out analogy), we can be confident that human ancestors possessed the trait in question.\nKeep in mind that Smith is arguing that EP is impossible in principle. The strength or weakness of the empirical evidence in each of the above examples, which varies quite a bit, is therefore not at issue. At issue is whether it is possible to provide convincing evidence of homology, and thus that human ancestors possessed the cognitive mechanisms in question. The eye-blink and other examples make clear that it is, and that evolutionary psychologists are attuned to the theoretical importance of homologies between psychological traits in humans and those in our relatives. By Smith’s own criteria, much EP is possible.\nPutative psychological adaptations that are unique to the human lineage, however, present difficulties. One can no longer invoke homology as evidence that cognitive structures that reliably develop in modern humans also did so in ancestral humans. Language is an ideal example because language abilities are not present in chimpanzees or other primate relatives. Indeed, although some evolutionary psychologists argue that language has all the hallmarks of adaptation, such as complex cognitive design, fitness benefits, and reliable development in all humans (e.g., Pinker and Bloom 1990; Pinker and Jackendoff 2005), other evolutionary and cognitive researchers argue that there was no selection for language specifically and that it instead emerges, e.g., as a byproduct of cognitive abilities such as recursion (Hauser et al. 2002) or from learning and other cognitive biases and coordination with others (Christiansen and Chater 2008).\nAs Smith repeatedly notes, psychological adaptations, like other adaptations, are inherited genetically. She fails to acknowledge, though, that we now have the complete sequence of the human genome, and the genome contains information on human-specific positive selection (I blog about genetic evolution in the human lineage here). We do not yet have the knowledge to link the development of most human adaptations, psychological or physiological, to specific sequences in the genome. But when it comes to language, there is progress. Rare mutations in FOXP2, a transcription factor, appear to disrupt language development, suggesting that FOXP2 plays a critical role in language. There is evidence that transcriptional enhancers in the FOXP2 locus underwent accelerated evolution in the human lineage (Caporale et al. 2019).\nAt this point, such evidence is suggestive at best. Genes have multiple effects and it could be that positive selection on FOXP2 regulation was due to non-language or non-cognitive effects of FOXP2. It could also be that FOXP2 influences language via, e.g., its effects on aspects of cognition that are not specific to language, such as recursion, and it was these abilities that underwent recent positive selection. Still, these results indicate that in coming years it might be possible to test EP and other adaptationist hypotheses using genetic data.\nBut even without genetic data, it is possible to test adaptationist hypotheses about species-specific adaptations. Imagine, for instance, that species X, which is living in an environment disturbed by recent human activity, has a complex soft-tissue trait that reliably develops in all members of the species. This trait doesn’t fossilize, it does not appear in any other species, and its genetic basis is unknown. It could be that this trait is a non-functional developmental consequence of novel environmental factors – perhaps chemical pollutants. But it could be an adaptation. Research on the trait, its relationship to what we know about the species’ ancestral environment, how the species makes a living, and on the underlying developmental mechanisms, as well as theoretical advances, can either indicate design or the lack thereof, and thus add weight to an adaptationist or non-adaptationist hypothesis, respectively. This approach characterizes research on menopause, which only occurs in humans and four species of toothed whales, and for which there are many competing hypotheses involving adaptation vs. byproduct of senescence (e.g., Kirkwood & Shanley 2010; Johnstone & Cant 2019). Yet according to Smith, in such cases scientists should just throw up their hands.\nSmith’s view of science is myopic. Abduction, i.e., inference to the best explanation, is the cornerstone of scientific methodology (Douven 2017). Scientific hypotheses compete with one another to provide the “best” explanation of some phenomenon, where “best” typically involves criteria like accurately predicting new and surprising observations, parsimony, and coherence. Abduction specifically does not require that scientists produce direct evidence for every single entailment of the hypothesis. The eye-blink adaptation hypothesis entails a genetic basis for the reflex, yet Smith is willing to accept the hypothesis without direct evidence for eye-blink genes. “Strong vertical homology” is just another of the many criteria any hypothesis of complex adaptation must meet, and not the most important one (that honor belongs to design). And, contra Smith, more such criteria make a theory more testable, not less, because there are now more ways to falsify it. EP hypotheses make many unique predictions that are testable in living humans, and are thus able to compete with other hypotheses to explain language and other cognitive phenomena.\nSmith, nevertheless, is right to draw attention to the importance of homology and the comparative method in testing adaptationist hypotheses. And to be fair, EP probably under-utilizes this powerful tool. She is wrong to ignore the many EP studies that do employ the comparative method, however, and she incorrectly concludes that if evidence for one of the many predictions of an adaptationist hypothesis is currently missing (e.g., evidence of “strong vertical homology”) it is therefore impossible to test that hypothesis against competing hypotheses.\nMany thanks to Laith Al-Shawaf for helpful comments on an earlier draft.\n2020/01/25: Added refs for menopause example.\nReferences\nCaporale, A. L., Gonda, C. M., & Franchini, L. F. (2019). Transcriptional Enhancers in the FOXP2 Locus Underwent Accelerated Evolution in the Human Lineage. Molecular biology and evolution, 36(11), 2432-2450.\nChristiansen, M. H., & Chater, N. (2008). Language as shaped by the brain. Behavioral and Brain Sciences, 31(5), 489-509.\nDarwin, C (1859) On the Origin of Species. John Murray.\nDarwin, C (1872) The Expression of the Emotions in Man and Animals. John Murray.\nDouven, Igor, “Abduction”, The Stanford Encyclopedia of Philosophy (Summer 2017 Edition), Edward N. Zalta (ed.), https://plato.stanford.edu/archives/sum2017/entries/abduction/.\nHagen EH and Hammerstein P (2009). Did Neanderthals and other early humans sing? Seeking the biological roots of music in the loud calls of primates, lions, hyenas, and wolves. Musicae Scientiae, 291-320.\nHaun, D. B., Call, J., Janzen, G., & Levinson, S. C. (2006). Evolutionary psychology of spatial representations in the hominidae. Current Biology, 16(17), 1736-1740.\nHauser, M. D., Chomsky, N., & Fitch, W. T. (2002). The faculty of language: what is it, who has it, and how did it evolve? Science, 298(5598), 1569-1579.\nJohnstone, R. A., & Cant, M. A. (2019). Evolution of menopause. Current Biology, 29(4), R112-R115.\nKirkwood, T. B., & Shanley, D. P. (2010). The connections between general and reproductive senescence and the evolutionary basis of menopause. Annals of the New York Academy of Sciences, 1204(1), 21-29.\nMaestripieri, D., & Roney, J. R. (2006). Evolutionary developmental psychology: Contributions from comparative research with nonhuman primates. Developmental Review, 26(2), 120-137.\nPinker, Steven, and Paul Bloom. “Natural language and natural selection.” Behavioral and brain sciences 13.4 (1990): 707-727.\nPinker, S., & Jackendoff, R. (2005). The faculty of language: what’s special about it?. Cognition, 95(2), 201-236.\nSchaller, M. (2015). The behavioral immune system. The Handbook of Evolutionary Psychology.\nSchmitt, D. P., & Pilcher, J. J. (2004). Evaluating evidence of psychological adaptation: How do we know one when we see one?. Psychological Science, 15(10), 643-649.\nSilk, J. B., & House, B. R. (2012). The Phylogeny and Ontogeny of Prosocial Behavior. The Oxford Handbook of Comparative Evolutionary Psychology, 381.\nSmith, S. E. (2019). Is Evolutionary Psychology Possible? Biological Theory. https://doi.org/10.1007/s13752-019-00336-4\nVonk, J., & Shackelford, T. K. (Eds.). (2012). The Oxford Handbook of Comparative Evolutionary Psychology. OUP USA.\nWhiten, A. (2017). Social learning and culture in child and chimpanzee. Annual Review of Psychology, 68, 129-154.\nWilliams, G. C. (1966) Adaptation and Natural Selection. Princeton University Press.\nSmith provides additional lines of evidence for an eye-blink adaptation that are either the same as standard adaptationist arguments and/or do not address the matching problem: “Second, it is clear that the eye-blink reflex protects the eye from injury in the taxa where it is found. Third, it is clear that the fitness of organisms that rely on vision would be impeded if there were not some mechanism for protecting the eye from injury. Fourth, the physiological mechanism underpinning the reflex is well understood. Fifth and finally, the reflex operates automatically, and is therefore mandatory. It is not “up to” the organism whose reflex it is. These five factors, taken together, support the claim that the eye-blink reflex in contemporary humans is strongly vertically homologous to the eye-blink reflex in earlier members of the lineage.\"\n\n↩︎\n",
    "preview": "posts/2020-01-21-is-evolutionary-psychology-impossible/../../images/Gabriel_Cornelius_von_Max.jpg",
    "last_modified": "2021-04-25T21:47:08-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/",
    "title": "About 90% of the genome is junk, which is very informative about ancestry but says little about biology",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2019-07-27",
    "categories": [],
    "contents": "\nThe genome, taken as a whole, has a profound influence on our biology. Many scientists, myself included, see it as a blueprint for the organism, or perhaps more accurately, as I explain here, a program that generates the entire organism. Why, then, have many geneticists concluded that about 90% of the human genome has no influence on the human organism whatsoever, that it’s “junk”? And if most of the genome is junk, how can it reveal anything about an individual’s ancestry?\nI’ll explain why most genetic differences among individuals and populations are probably differences in junk DNA, and therefore, perhaps counter-intuitively, are incredibly informative about ancestry but say little about biology.\nWhy is most of the genome thought to be junk?\nClue #1: huge variation in eurkaryotic genome size\nIn the 1940’s and 50’s, biologists determined that the quantity of DNA was remarkably constant in cells from the same species, and therefore dubbed this quantity the “C-value”, but that it was quite variable across cells from different species. Importantly, the C-value, which we now know is the size of the genome (i.e., the number of nucleotides, or “letters”), bears little obvious relationship to the complexity of the organism. Some amoeba, for instance, which are single-celled organisms, have genomes that are 100x larger than the human genome (Eddy 2012). The human genome contains eight times more DNA than that of a puffer fish but 40 times less than that of a lungfish, and among the >200 salamander genomes analyzed as of 2014, all are between 4 and 35 times larger than the human genome (Palazzo and Gregory 2014). See Figure 1 (note the logarithmic scale).\n\n\n\nFigure 1: Summary of haploid nuclear DNA contents (“genome sizes”) [in megabases] for various groups of eukaryotes. This graph is based on data for about 10,000 species. There is a wide range in genome sizes even among developmentally similar species, and there is no correspondence between genome size and general organism complexity. Humans, which have an average-sized genome for a mammal, are indicated by a star. Note the logarithmic scale. Figure and caption from Palazzo and Gregory 2014.\n\n\n\nWhy would the “blueprints” of some simple organisms be so much larger than those of more complex ones, and why would the genomes of very closely related species often differ so dramatically? It is hard to escape the conclusion that a substantial fraction of a large genome has little, if any, influence on the phenotype of the organism. What, then, explains the expansion in genome size in some lineages?\nClue #2: pervasive parasitic DNA\nA key property of DNA is that it can copy itself. Typically, this serves to make a new copy of the genome in a daughter cell, but sometimes sections of DNA copy themselves from one part of the genome into another, a type of mutation that often causes disease (Hancks & Kazazian, 2016). These sequences of DNA, termed transposons, are considered to be selfish or parasitic DNA because they have evolved features that increase their own replication at the expense of other genes in the genome, e.g., by inserting themselves into or near a gene and thus disrupting its function, consequently harming the organism itself.\nThere are two major classes of transposons. DNA transposons operate by a cut-and-paste mechanism – so-called “jumping genes” – and make up only about 3% percent of the human genome. Retrotransposons, in contrast, operate by a copy-and-paste mechanism, and therefore increase the size of the genome (see Figure 2). Retrotransposons make up at least 45% of the human genome (Cordaux and Batzer 2009).\n\n\n\nFigure 2: Retrotransposon. Figure from Wikimedia.org\n\n\n\nAlu and LINE-1 retrotransposons in the human genome insert perhaps as frequently as once every 20 births (Cordaux and Batzer 2009). Like other parasites, these copies can themselves make copies, and thus pose an extreme threat to other genes and to the organism itself. Many mechanisms have therefore evolved to neutralize them (Goodier, 2016). To illustrate: due to their activity over the last 150 million years, there are >500,000 LINE-1 elements in the human genome, but less than 100 copies are currently active, i.e., able to replicate within the genome (Cordaux and Batzer 2009).\nThe emerging picture, then, is that over evolution, lineages of retrotransposons have expanded, enlarging the genome (see Figure 3). Inserted elements that disrupted gene function were removed by negative selection (Rishishwar et al., 2017). Others were suppressed by cellular mechanisms, and then began to degrade due to accumulating mutations, becoming what we now call junk DNA (Rishishwar et al., 2017), although there is solid evidence that transposable elements have often been co-opted for the regulation of host genes (Chuong et al. 2017). Some particularly ancient retrotransposons might have degraded so extensively that they are no longer easily recognized. One analysis estimated that perhaps up to 70% of the human genome comprises such repetitive elements (Koning, Gu, Castoe, Batzer, & Pollock, 2011).\n\n\n\nFigure 3: The expansion of Alu elements in primates. The expansion of Alu subfamilies (Yc1, Ya5a2, Yb9, Yb8, Y, Sg1, Sx and J) is superimposed on a tree of primate evolution. The expansion of the various Alu subfamilies is colour coded to denote the times of peak amplification. The approximate copy numbers of each Alu subfamily are also noted. Mya, million years ago. Figure and caption from Batzer and Deininger 2002.\n\n\n\nClue #3 (which was misleading): only 1-2% of DNA codes for protein\nThe genetic code was cracked in the 1960’s: triplets of nucleotides (A,C,G,T) code for specific amino acids, the building blocks of proteins. Only about 1-2% of the genome codes for proteins, however. It was therefore often reported that the remaining 98-99% of the genome was junk, even though it was known as early as 1961 that some non-coding DNA regulated the expression of protein-coding DNA. By the early 2000’s, however, it was widely recognized that much non-coding DNA plays a critical regulatory role (Gerstein et al., 2007).\nThus, more than 1-2% of the genome is functional, but how much more?\nClue #4: about 90% of the genome shows no evidence of sequence conservation\nThe human genome has about 3 billion nucleotides. Every time the genome is copied, each nucleotide has a small probability that it will be miscopied, i.e., will mutate. In humans, there are a few dozen such mutations with each birth (Jonsson et al., 2017). Mutations that alter organism functions will undergo positive or negative selection. Because most such mutations are harmful — there are many more ways to break something than to improve it — they will typically disappear from the population via negative selection. A fertilized zygote with a mutation in a critical part of the genome might not even successfully develop. DNA sequences in which most mutations face negative selection are termed conserved or constrained.\nMutations that do not alter organism function, on the other hand, e.g., those that that occur in sequences with no influence on the phenotype, will not experience positive or negative selection and will therefore accumulate at a more-or-less constant rate. Junk DNA can therefore be distinguished from functional DNA by its lack of conservation across species.\nProtein coding sequences exhibit a surprising degree of conservation. Yeast and humans diverged about a billion years ago, for example, yet 23% of yeast genes have homologs in humans (i.e., the genes derive from a common ancestor). Of these genes, the amino acid sequences overlap, on average, by about 32%. Even more remarkable, after replacing 414 critical yeast genes with their human homologs, 47% of the human genes functioned and enabled the yeast to survive (Kachroo et al. 2015).\nA disadvantage of methods that assess DNA functionality by comparing degrees of sequence conservation across species is that they cannot identify functional sequences that might be recently acquired in a particular species. There might be functional sequences of DNA in the human genome, for instance, that do not appear in the genomes of chimps or other primates.\nTo assess what fraction of the human genome has been subject to negative selection on both long and short timescales, i.e., is constrained and is therefore likely to be functional, Rand et al. (2014) analyzed DNA sequences across the mammals in several functional classes (i.e., protein coding sequences vs. various types of regulatory sequences that might have been acquired recently). They found that 7.1– 9.2% of the human genome is presently constrained, fairly consistent with previous results that found that between 3% and 15% of the human genome was functional. As expected, protein coding sequences were highly constrained, whereas regulatory sequences experienced more rapid turnover (see Figure 4). This implies, of course, that >90% of human DNA is not functional, i.e., is junk (for similar recent estimates, see Ward and Kellis, 2012; Gulko et al., 2015; and references therein).\n\n\n\nFigure 4: Model-based inference of turnover by functional class [across the mammals]. Schematic summary of the fraction of constrained sequence that has been retained (saturated colours) or turned over (pastel colours) in the human lineage over time (X-axis, divergence time) and how it has been distributed across various categories of functional element. In addition to showing the reduced quantity of preserved constrained sequence with increasing divergence, we infer the reciprocal quantity of sequence that is assumed to have been gained over human lineage evolution. For consistency this approach requires mutually exclusive annotation sets, in contrast to those used in Figure 3, making the results not directly comparable. Overlaps between the major different annotations are shown in Figure S10. Figure and caption from Rands et al. 2014.\n\n\n\nThe ENCODE controversy\nENCODE is a large consortium of research groups whose goal is to “build a comprehensive parts list of functional elements in the human genome, including elements that act at the protein and RNA levels, and regulatory elements that control cells and circumstances in which a gene is active.” Their methods for identifying function primarily involve detecting evidence that a nucleotide participates in a biochemical RNA- and/or chromatin-associated event.\nIn 2012, ENCODE published their first analysis of functionality across the entire human genome. This was an enormously important milestone, but one claim stood out:\n\n“These data enabled us to assign biochemical functions for 80% of the genome” (p. 57).\n\nENCODE appeared to be challenging all the evidence I’ve reviewed here that most of the genome is junk. Several news commentaries trumpeted the death of the junk DNA concept (see refs in Graur et al., 2013).\nBiting critiques soon followed (Eddy, 2012; Doolittle, 2013; Graur et al., 2013; Niu & Jiang, 2013; and Palazzo & Gregory, 2014). These researchers noted that ENCODE employed an unusually broad definition of “function” based on the causal role account, which assigns “functions” to biochemical effects of nucleotide sequences, regardless of whether those effects evolved by natural selection. By ENCODE standards, the thumping sound of the heart is a “function” of the heart because the heart causes those sounds as part of a larger “noise-making” system, and the disabling effects of dangerous genetic birth defects are also “functional” (e.g., Graur et al., 2013). ENCODE’s approach does have some defenders, however (e.g., Germain et al., 2014).\nMuch of the ENCODE claim of functionality rests on evidence that a sequence is transcribed into RNA. According to the critics, transcription is an inherently noisy process that occurs in most of the genome regardless of evolved functionality, and that most transcripts are probably junk (e.g., Palazzo and Lee, 2015), although here, too, there is push back (e.g., Jandura and Krause, 2017).\nAlthough the ENCODE claim of 80% functionality has found some defenders, I favor a definition of “function” that is based on natural selection, and therefore remain skeptical that most of the genome is functional in this sense.\nWhy is junk DNA ideal for revealing ancestry?\nIf most of the human genome is junk DNA, as it appears to be, then most genetic differences among individuals and populations are differences in junk DNA that have no biological significance. It turns out, though, that lack of biological significance is ideal for inferring ancestry.\nEvery person inherits half their DNA from each parent. Each parent, in turn, inherited half their DNA from each of their two parents, and so forth. Thus, each section of one’s DNA has its own unique lineage of ancestors. Due to random copying errors, a small number of changes in DNA sequence are introduced with each transmission from parent to offspring. Over generations each section of DNA will therefore acquire a unique “fingerprint” of variations. Sections of DNA that are inherited from a recent common ancestor will have very similar fingerprints, whereas those inherited from a more distant common ancestor will have more distinct fingerprints. By comparing an individual’s patterns of variations with a large database of such patterns across the entire genome, from individuals living around the world, it is possible to estimate which sections of DNA share recent common ancestors from which locations. Because random errors accumulate with each generation according to the mutation rate, the degree of similarity or difference also provides an estimate of the number of generations, and hence years, to those common ancestors.\nTo illustrate with my two daughters: They each inherited one chromosome 6 from me, and one from my wife. My dad has Norwegian ancestry and my mom has Ashkenazi Jewish ancestry. So, with 50-50 probability (and ignoring recombination), the chromosome 6 my daughters inherited from me might more closely resemble chromosome 6’s from Northern Europe or those from central European Jewish communities. My wife has Mexican American background, with a great-great grandparent with African background. So the chromosome 6 they inherited from her might more closely resemble those of Spanish, Portuguese, or other Europeans (with 90.6% probability), Native Americans (with 6.25% probability), or Africans (with 3.125% probability). See Figure 5.\n\n\n\nFigure 5: Global Ancestry. The green arrows symbolize migration of early human ancestors out of Africa. The color mosaic denotes global population diversity resulting from various subsequent inter- and intra-continental and regional migrations. The pedigree represents the complex network of intermediate and recent ancestors that is the subject of individual genetic genealogy testing. Figure and caption from Royal et al. 2010.\n\n\n\nFigure 6 is the output of one popular computer program for determining ancestry from DNA sequences. Each thin vertical line is one individual, and the different colors represent the fractions of their DNA sequences that more closely match individuals from different regions. My daughters would be mostly green, with some purple and a dash of red. President Obama would (probably) have a line that is about half green and half red.\n\n\n\nFigure 6: Regional ancestry inferred with the frappe program at K = 7 (13) and plotted with the Distruct program (31). Each individual is represented by a vertical line partitioned into colored segments whose lengths correspond to his/her ancestry coefficients in up to seven inferred ancestral groups. Population labels were added only after each individual’s ancestry had been estimated; they were used to order the samples in plotting. Figure and caption from Li et al. 2009.\n\n\n\nSuch inferences of ancestry for each section of DNA assume that the patterns of variation in those sections result only from descent with random copying errors due to the mutation rate. These assumptions hold for junk DNA. If there were processes that caused sequences with distant common ancestors to closely resemble each other, however, or sequences with recent common ancestors to sharply diverge from one another, a geneticist might mistakenly conclude that some sequences have more recent vs. more distant common ancestors than they actually do, and incorrectly infer ancestry. In functional DNA, this can occur due to positive, negative, or frequency dependent selection, i.e., factors related to the environment rather than ancestry. Conserved sequences – those under negative selection – will have fewer differences than expected given the mutation rate, for instance, and will therefore appear to have a more recent common ancestor than they actually do.\n\nIn short, in functional DNA, patterns of variation due to ancestry (and also to population history) are confounded with patterns of variation due to natural selection (e.g., Akey et al. 2004, Sabeti et al. 2006, Nielsen et al. 2009).\nUsing pure junk DNA to infer ancestry would be ideal, and several studies have used Alu insertions to investigate human ancestry and demography (Rishishwar et al., 2017). Functional and non-functional DNA are intimately intertwined in the genome, however, and it is not entirely clear which non-coding sequences are functional and which are not. Ancestry is therefore typically inferred using common genetic variants across the genome (for a readable introduction to inferring ancestry from genetic data, see Royal et al., 2010). Given that most DNA is junk, most common genetic variants should be junk, but are they?\nMany studies have attempted to determine the functional consequences, if any, of coding and noncoding variants. Because the human genome is so poorly understood, these studies typically combine diverse sources of sequence, evolutionary, and functional information using use neural network and other machine learning methods to classify the variants according to their functional effects. One effort, Bromberg et al. (2013, p. 14255), concluded that:\n\n[V]ariants in seemingly healthy individuals tend to be neutral or weakly disruptive for protein molecular function. These variant effects are predicted to be largely either experimentally undetectable or are not deemed significant enough to be published. This may suggest that nondisease phenotypes arise through combinations of many variants whose effects are weakly nonneutral (damaging or enhancing) to the molecular protein function but fall within the wild-type range of overall physiological function.\n\nA recent review of methods assessing phenotypic effects of noncoding variants, on the other hand, was skeptical of their performance, concluding “A significant disconnect is found to exist between the statistical modelling and biological performance of predictive approaches” (Liu et al. 2019). Part of the problem is that the methods stumble with the severe class imbalance: the small number of noncoding variants with meaningful effects among a relatively large number without such effects.\nIn summary, evidence to date is consistent with the view that most human DNA sequences are junk and are evolving neutrally. Variants in these sequences are yielding a rich trove of information on human ancestry and population history, but don’t tell us much about our biology.\nNone of the foregoing calls into question the profound importance of functional DNA to human biology. I write about that here, highlighting a perspective on the genome that hasn’t gotten much press. And here I write about the relative influences of positive natural selection vs. random genetic drift on the evolution of the human lineage.\nConcluding remarks\nEach of our many ancestors indisputably lived at certain times and in certain places, and it’s pretty amazing that we can now glean those hard facts from spit in a tube. Living in a certain time and place, however, in and of itself, says nothing about the biology of those ancestors.\n\n\n\n",
    "preview": "posts/2019-07-27-about-90-of-the-genome-is-junk-which-is-very-informative-about-ancestry-but-says-little-about-biology/../../images/globalancestry.png",
    "last_modified": "2021-04-25T13:00:49-07:00",
    "input_file": {},
    "preview_width": 1010,
    "preview_height": 769
  },
  {
    "path": "posts/2019-07-12-should-scientific-publishing-move-to-github-and-friends/",
    "title": "Should scientific publishing move to Github and friends?",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2019-07-12",
    "categories": [],
    "contents": "\nTL;DR: Open access publishing has very high administrative overhead and is therefore too expensive. Github and similar services have substantial and perhaps insurmountable technical and funding advantages as publishing platforms. Scientific publications should therefore be git repos, created by the researchers themselves, that contain the manuscript, data, and analysis code, and that are hosted on, e.g., github, gitlab, bitbucket, sourceforge, and a few others. A ‘journal’ would just be a managed collection of repos. Reviews would be handled via issues. Long-term archiving and minting of doi’s would be handled by zenodo or equivalent data archiving services. Journal reputations would be based on the reputations of the editors, who are typically researchers themselves.\nJuly 17, 2019: Added links at the end to researchers and journals that are already publishing on Github.\nElsevier, the world’s largest publisher of scientific articles, just cut off access to its journals by the University of California, one of the world’s largest producers of scientific articles, because UC objected to Elsevier’s increasingly exorbitant subscription fees. Meanwhile, many funders of scientific research, mostly in Europe but also including the Gates Foundation, have signed on to Plan S, which stipulates that research that is funded with money from supporting institutions must be published in open access journals or platforms.\nOpen access journals are expensive, though, typically charging $1000 or more per article. The reason they are so expensive is that administration and software development is expensive. arXiv.org, the famous physics/math preprint server that hosts articles for free, has a relatively small leadership team of six people, yet salaries alone amount to more than $1.3 million/year. Add in indirect costs, and the total is about $2 million/year, covered by grants, memberships, and Cornell. Their servers and misc expenses are less than 1/10 of the total.\nPLOS, PeerJ, and other open access journals cover their substantial administrative and development costs with publication fees that range from $1000-$3000/article, which is about what traditional journal publishers like Elsevier charge for open access.\nThe Center for Open Science (COS) and osf.io offer free preprint, preregistration, and file hosting services (full disclosure: I use osf.io, and we received one of their $1000 preregistration awards). They currently have about 50 employees and are spending in the neighborhood of $7 million/year, which, as far as I can tell, comes mostly from grants. As COS itself admits, sustainability is a major concern, and will probably involve charging fees to stakeholder communities, e.g., universities.\nIn sum, the multiple open science initiatives each have their own admin teams, incurring high administrative overhead, and are chasing a relatively small pool of users, many of whom have little funding or incentive to contribute to these public goods.\nHow does the open source software community do it for free?\nThe open source software community, like the science community, wants to give its products away for free in exchange for prestige. Their efforts have transformed the planet. Unlike science, however, open source developers pay nothing to publish their highly technical “documents” (code). How do they do it? In a word, Github.\nIf you don’t know what Github is I describe it in a bit more detail below. For now, think of it as an online service for collaborating on software development. The open source community is allowed to use Github for free because the tech industry benefits tremendously from open source code and the talent that produces it. Open source is therefore subsidized by the fees commercial firms pay to use Github, which has estimated annual revenues of $250 million, and was acquired last year by Microsoft for $7.5 billion. Microsoft’s annual revenue is $110 billion. Gitlab, a similar service, has $10.5 million in annual revenue, and recently received $100 million in venture capital. Atlassian, which owns Bitbucket, yet another such service, offers a number of commercial collaboration services and has about $1 billion in annual revenue.\nAs of this writing, Github alone has over 30 million users working on close to 100 million projects. The technological and financial investment in these platforms and the economies of scale are orders of magnitude larger than those enjoyed by any open science initiative.\nCould scientific publishing move to Github?\n\n\nIn 2011, Marcio von Muhlen argued that academia needed a Github of science. He made three key points:\nPublishing is central to Academia, but its publishing system is outclassed by what Open Source software developers have in GitHub\nGitHub’s success is not just about openness, but also a prestige economy that rewards valuable content producers with credit and attention\nOpen Science efforts like arXiv and PLoS ONE should follow GitHub’s lead and embrace the social web\nEach of these points is just as true today as it was then. The only thing I would add is that the Github of science should be…Github. The costs of hosting scientific articles on Github or similar services, such as Gitlab and Bitbucket, would be a rounding error.\nMoreover, much of science’s computational infrastructure is already developed on Github and friends. This includes python and scipy, machine learning frameworks, key r packages, and much more.\nJust like it benefits from open source software, the business community benefits tremendously from science. Instead of researchers paying the scientific publishing oligopoly hefty fees to publish tax-funded research, commercial businesses would subsidize the (small) cost for researchers to publish their research on git hosting services.\nSounds fair to me.\nThis proposal is not without risk. Microsoft’s purchase of Github, for instance, immediately raised concerns that it would use its control over Github to harm competitors that rely on the platform. What would stop Microsoft or Atlassian from exploiting their control of a scientific publishing platform? It’s also not clear that any of the git hosting services, which are investing heavily in growth, are profitable yet.\nWhile these risks shouldn’t be ignored, I think they are no larger (and are probably much smaller) than the risks of using platforms, such as osf.io, that have no long-term funding plan in place. Researchers would retain copyright over their publications that carry open source licenses. Because git repos are an open standard, it’s trivial to host them on multiple platforms and to archive them on multiple data archiving services. Furthermore, the fees that Github, Gitlab, and Bitbucket charge commercial users are pretty small. Some of the plans are as low as $25-$50/user/year. Gitlab has a completely open source version of its cloud platform.\nIn summary, there is substantial overlap between programming and the data analysis and modeling that is at the heart of much science. Github and similar hosting services are used by millions of programmers and thousands of researchers every day. These services provide all the features necessary to support scientific journals: online submission, review, revision, and publication. Their business models appear viable, at least judged by the venture capital they are attracting, they are investing heavily in their IT infrastructure, and they enjoy huge economies of scale. Researchers and universities should not use their limited funds to support a small scientific publishing oligopoly that provides little added value. Nor should they fund multiple administrative teams at various open science initiatives that are reinventing the wheel. Instead, the tech industry and broader business community that benefits so heavily from science can subsidize the relatively small costs of publishing scientific research on shared IT platforms like Github, Gitlab, and Bitbucket. Who knows, the synergies of hosting scientific publications might be valuable enough that these services would even compete to attract scientists by developing science-specific features.\nNow down to brass tacks. How would this actually work?\nWhat is git?\nSkip this part if you already know what git is.\nComputer code is just a bunch of text files in a directory. Collaborating on code requires some way to allow multiple programmers to access and edit these files, track changes to them, and revert back to previous versions, if necessary. The solution that has almost universally been adopted is a distributed version control system (DVCS), and one in particular: git.\nThe basic workflow is as follows: a programmer installs git on her own computer, and then starts a new software project by creating one or more text files in directory. She then runs the git initialization command, which creates a hidden directory inside that project directory. That directory is now a git repository (a repo). She creates a new feature in her software by editing one or more text files in the directory on her own computer, and commits those changes using another git command. This tracks the changes she’s made to each file, stamps the changes with the time, date, and her identity, and stores the changes in the hidden directory.\nThe programmer and her collaborators can then use git and numerous third-party tools to inspect precisely what changes have been made to which files, when, and by whom. Think of it like track-changes in Word, but on steroids.\nWhy is a git repo a good format for a scientific publication?\nA research publication is just a bunch of files in a directory. These include the manuscript itself, the figure files, supplementary information files, and sometimes data and data-processing code files. In some cases, such as writing a single-authored commentary in Word, git would provide few, if any, advantages to the researcher. In many other cases, however, such as working with collaborators on an empirical study that analyzes data using, e.g., python, R, or matlab, git would provide the same enormous advantages that it provides to software developers. The researchers can easily track who is making what changes to which files, when, and where. Thousands of researchers are already using git for exactly this purpose. In addition, the manuscript itself can be written using text-based formats like \\(\\LaTeX\\) or markdown, and all the advantages of git when writing code also apply to these text-based documents.\nThe most important advantage of git, though, is that it is a open standard, based on open software, that is used by millions of software developers and researchers around the world, and there is a large and growing IT infrastructure that can ‘speak’ git. This means that researchers can conduct their analyses on their own computers and then seamlessly share them with the public via services such as Github.\nWhat is Github?\nSkip this part if you know what Github is\nWhen the programmer or researcher is ready to share her work, she uploads her git repo to a hosting service, which allows others to view the code, copy it to their own computers using git (cloning), make their own commits to the code using git, and then share any improvements they’ve made using git. Think of git + git hosting as track-changes combined with dropbox, but on steroids.\nThe most popular git hosting service is Github.com, a proprietary cloud service, recently purchased by Microsoft, that ‘speaks git.’ (For a somewhat rah rah version of the Github story, see this.) Gitlab is another up-and-comer that, unlike Github, provides a pretty full-featured open source version of its platform.\nThese hosting services also offer an issue tracking feature, which is basically a discussion board where developers or users can report bugs and request new features (issues).\nOh, and just to repeat, all open source projects can use Github and similar platforms for free.\nCreating a scientific journal on top of Github\nTypically, a researcher submits her manuscript to a journal by using its clunky web interface to upload a manuscript file, such as a Word doc or PDF, one or more figure files, one or more supplementary information files, and perhaps even data and code files. Numerous forms must be filled out that duplicate information in the manuscript itself. The process can easily take the better part of an afternoon. Yuck.\nIf git were the standard format for a research manuscript, the researcher would simply organize all the relevant files in a git repo on her own computer, and then push it to Github with single command or click of the mouse. Github et al. are fully capable of providing both html and pdf versions of scientific articles, just as all scientific publishers do today (this blog is hosted on Github). As long as scientific publications use an open source license (and why wouldn’t they?), hosting would be free.\nBut how to create a journal that would comprise hundreds or thousands of repos created by diverse researchers? Github, Gitlab, and Bitbucket have a feature that allows a group of individuals to manage multiple repositories. Github calls these ‘organizations’, Gitlab calls them ‘groups’, and Bitbucket calls them ‘teams.’ I’ll use the Github terminology. To create a new journal, an editor and her associate editors create an organization on Github.\nTo get the journal off the ground I think the editor would have to be a highly regarded researcher in her discipline, which would create immediate buzz and trust. I suspect this is key to the whole idea.\nTo submit an article to the journal, a researcher uploads a repository with all her data, analysis scripts, and a final version of her paper in pdf or html format to her own github account. She also assigns it an open source license. She then messages the editor to consider her repo for publication. The editor looks at the repo and decides if it is suitable for her journal. If so, she clones the repo into her organization.\nAt this point, the repo might be private (not accessible to the public). The Editor then recruits reviewers, who create issues on the issue tracker for the repo. Each issue is one comment/critique that the author(s) will have to address.\nThe authors make changes to their manuscript and code to address each issue, and then submit their revisions via, e.g., a pull request. If they are using a text-based format for their manuscript, Github will display the revisions similar to track-changes in Word.\nOnce the editor determines that an issue has been adequately addressed, she closes it. When all issues are closed, the editor decides if she wants to publish the study. If so, she gets a doi for it via, e.g., zenodo, so the study is citable, and then makes the repo public.\nDone!\nOther researchers can fork the repo, or star it if they think its cool. Because the code is under an open source license, others can modify it and publish their own analysis (with proper attribution, of course).\nThere are many other possibilities for creating a scientific prestige economy on Github, but I’ll leave discussion of those for another time.\nResearchers and journals that are already publishing on Github\nandrewgyork.github.io\nhttp://rescience.github.io\nhttps://www.arfon.org/chatops-driven-publishing\nhttps://manubot.org\nPublishing with Jupyter notebooks:\nThe Scientific Paper Is Obsolete (Atlantic)\nBy Jupyter–Is This the Future of Open Science?\nReproducible academic publications\n\n\n\n",
    "preview": "posts/2019-07-12-should-scientific-publishing-move-to-github-and-friends/../../images/githubrepo.png",
    "last_modified": "2021-04-25T12:56:49-07:00",
    "input_file": {},
    "preview_width": 1134,
    "preview_height": 970
  },
  {
    "path": "posts/2019-06-20-a-theory-of-natural-selection-5th-century-bc/",
    "title": "A theory of natural selection, 5th century BC",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2019-06-20",
    "categories": [],
    "contents": "\n\n\n\nHow came the bodies of animals to be contrived with so much art, and for what ends were their several parts? Isaac Newton, Opera Omnia, IV. 237.\nI’ve always been curious how far back in history the tool-like or machine-like properties of living things were recognized, and how these were explained.\nIt turns out that Darwin was scooped over 2000 years ago.\nPlato clearly recognized that animals have functional traits that are usually, but not always, explained by the benefit they provide to the animal itself. Plato’s Protagoras, written c. 390 BC, relates a version of the Prometheus creation myth in which various properties of animals are explained by their role in animal survival, e.g., an animal’s strength, speed, size, or weaponry is there to protect the animal. Thick hair or hard skins are there to protect against the seasons sent by Zeus. The animals are appointed different foods: some grass, others fruits. Those who eat other animals are few in number, whereas prey are bestowed with fertility so as to preserve the species.\nPlato, however, did not limit beneficial “design” to living things, nor did animal traits always benefit the animal itself. In the Timaeus, written many years later, Plato proposed that a divine and supremely good Demiurge (craftsman) created the universe in precisely such a way that the universe as a whole, as well as its various parts, produce a vast number of good effects. He created the sun, moon, and stars to mark time, for instance, which itself came into being as an image of eternity (Zeyl and Sattler 2017). He created eyes for sight, “to the end that we might behold the courses of intelligence in the heaven, and apply them to the courses of our own intelligence which are akin to them,” that we “might imitate the absolutely unerring courses of God and regulate our own vagaries” (emphasis added).\nAncient atomists such as Democritus, a contemporary of Plato, eschewed such teleology. Instead, according to them, the universe is explained by the movements and interactions of a small number of indivisible particles in a void. The atomists offered ingenious arguments to explain many natural phenomena in terms of interactions of these indivisible particles – atoms – which have only a few intrinsic properties like size and shape, and strike against one another, rebounding and interlocking in an infinite void. Critically, there is no Demiurge. Causation is due only to blind necessity or chance (Berryman 2016).\nAristotle, perhaps writing around 330 BC, grappled with both the atomists and with Plato. To explain some phenomena, even those that benefited humanity, he invoked atomist arguments. Rain, for example, occurs of necessity, and itself has necessary and chance effects (Physics II 8):\n\n[W]hy should not nature work, not for the sake of something, nor because it is better so, but just as the sky rains, not in order to make the corn grow, but of necessity? What is drawn up must cool, and what has been cooled must become water and descend, the result of this being that the corn grows. Similarly if a man’s crop is spoiled on the threshing-floor, the rain did not fall for the sake of this – in order that the crop might be spoiled – but that result just followed.\n\nParts of animals, though, according to Aristotle, cannot be explained (only) by blind necessity or chance. Teeth, for instance:\n\n…are admirably constructed for their general office, the front ones being sharp, so as to cut the food into bits, and the hinder ones broad and flat, so as to grind it to a pulp…. (Parts of Animals III 1).\n\nBecause teeth grow this way in (almost) all humans, Aristotle goes on to argue, this pattern cannot be due to chance, contrary to the atomists. Instead, using a series of abductive arguments, Aristotle concludes that the parts of animals can only be explained by their purpose, a final cause (Ariew 2002). But no inference is made to a Platonic demiurge. Aristotle’s teleology, unlike Plato’s, is local, not global; it is immanent in the organism, not external to it (Ariew 2002; Schiefsky 2007).\nHow, though, did the atomists, who aimed to explain everything by blind necessity or chance, and certainly did not invoke final causes, explain the parts of animals? In a brief but remarkable passage, Aristotle refers to an argument by Empedocles, who lived in the 5th century BC:\n\nWherever then all the parts [of animals] came about just what they would have been if they had come be for an end, such things survived, being organized spontaneously [i.e., by chance] in a fitting way; whereas those which grew otherwise perished and continue to perish, as Empedocles says his ‘man-faced ox-progeny’ did. (Physics II 8)\n\nWhen I read this, my heart skipped a beat. That is natural selection.\nCould I have just made the remarkable discovery that Darwin was scooped by Empedocles in the 5th century BC? Uh, no. A quick google revealed that Darwin himself cited this passage in the preface to 4th edition of Origin as a historical forerunner to his theory (for details, see Gotthelf 2012). Even the Wikipedia article on natural selection mentions it. Dang!\nIt was Empedocles who claimed that everything is composed of exactly four elements – fire, air, earth, and water – which are moved by two opposing forces, Love and Strife. The four elements combine under the force of Love, and separate under the force of Strife (Perry 2016). Empedocles set forth his philosophy in poetry, only fragments of which survive. The lines describing the origins of animal parts are pretty trippy (Perry 2016):\n\nHere sprang up many faces without necks, arms wandered without shoulders, unattached, and eyes strayed alone, in need of foreheads (B 57).\n\nUnder the force of Love, these parts randomly combine:\n\nMany creatures were born with faces and breasts on both sides, man-faced ox-progeny, while others again sprang forth as ox-headed offspring of man, creatures compounded partly of male, partly of the nature of female, and fitted with shadowy parts. (B 61)\n\nIt is these random creatures that differentially survive if, by chance, they were organized in a “fitting way.”\nThis left me wondering: in a materialist account of the universe, which would only arise again in the West in full force following Galileo and Newton, is the idea of natural selection, in some sense, inevitable?\nReferences\nAriew, A. (2002). Platonic and Aristotelian roots of teleological arguments. Functions: New readings in the philosophy of psychology and biology, 7-32.\nBerryman, Sylvia, “Ancient Atomism”, The Stanford Encyclopedia of Philosophy (Winter 2016 Edition), Edward N. Zalta (ed.). https://plato.stanford.edu/archives/win2016/entries/atomism-ancient/\nGotthelf, A. (2012). Teleology, First Principles, and Scientific Method in Aristotle’s Biology. Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199287956.001.0001\nParry, Richard, “Empedocles”, The Stanford Encyclopedia of Philosophy (Fall 2016 Edition), Edward N. Zalta (ed.), URL = https://plato.stanford.edu/archives/fall2016/entries/empedocles/.\nSchiefsky, M. (2007). Galen’s teleology and functional explanation. Oxford Studies in Ancient Philosophy, 33, 369-400.\nZeyl, Donald and Sattler, Barbara, “Plato’s Timaeus”, The Stanford Encyclopedia of Philosophy (Winter 2017 Edition), Edward N. Zalta (ed.). https://plato.stanford.edu/archives/win2017/entries/plato-timaeus/.\n\n\n\n",
    "preview": "posts/2019-06-20-a-theory-of-natural-selection-5th-century-bc/../../images/empedocles.jpg",
    "last_modified": "2021-04-25T12:49:32-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-03-21-measles-mothers-leadership-and-the-evolution-of-big-brains/",
    "title": "Measles, mothers, leadership, and the evolution of big brains",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2019-03-21",
    "categories": [],
    "contents": "\nNancy Pelosi recently won a major conflict with Trump over border wall funding, and her victory flipped the media narrative. Instead of the insider who was out-of-step with both mainstream Americans and the progressive wing of her party, Pelosi was now seen as a masterful leader. The source of Pelosi’s moxie, according to many articles that appeared at the time, is that she’s a mother of five, and the daughter of another powerful women who raised six kids. Mothering and leadership, it seems, go together. What’s the link?\nMy grad student Zach Garfield and I believe we might have the answer. Despite the fact that almost all leaders in all societies are men, natural selection for leadership abilities might have been strongest in women, at least initially (preprint). I started thinking about this problem almost 20 years ago when the New Yorker and W.W. Norton published an article and book by Patrick Tierney that tried to profit from a measles vaccine scare, similar to the efforts of Andrew Wakefield and Jenny McCarthy (the latter two have been depressingly successful – there is a measles outbreak near our campus in southwest Washington).\nHere’s the story of how an attempt to make a buck off a vaccine scare unearthed an important but virtually unknown theory by one of the fathers of American genetics, James Neel.\nLate in the summer of 2000, an email spread like, well, measles, through the anthropological community. Sent by two credulous anthropologists, it advertised an upcoming book and New Yorker article by Patrick Tierney that accused James Neel, an acclaimed geneticist, and Nap Chagnon, a well-known but controversial anthropologist, of deliberately administering a dangerous measles vaccine to native South Americans. This vaccine could supposedly cause or exacerbate a deadly measles epidemic, allowing Neel and Chagnon to study the effects of the disease in an indigenous population so as to test Neel’s sinister eugenics theory.\nPrior to this I was vaguely aware of Neel’s work on the genetics of Native South Americans. Chagnon, on the other hand, I knew well. He was my MA thesis adviser. Our department chair therefore asked me and Michael Price, another grad student who had worked with Chagnon, to investigate. Along with John Tooby, a colleague of Chagnon (and chair of my PhD committee), we starting retrieving all the documents cited in Tierney’s meticulously referenced book.\nThe vaccine story crumbled almost immediately. Tierney had deliberately misquoted all of his sources, each and every one of which documented the safety and effectiveness of the vaccine. You can read our report here. Neel and Chagnon’s vaccination program probably saved hundreds of lives.\nBut Neel’s sinister theory intrigued me. It had something to do with the evolution of human intelligence, but I had never seen it cited in any of the countless papers on the topic. Once I got hold of Neel’s publications and figured out what he was actually saying, as opposed to Tierney’s mangled version, I realized it was a damn good idea. Neel’s research in the early 1960’s had found that in Amazonian populations, headmen had more wives and children than other men, a pattern that has now been seen in many other populations. Neel reasoned that if this pattern characterized humans societies during our evolution, there would have been tremendous sexual selection for whatever trait(s) predisposed men to become leaders.\nSexual selection often results in exaggerated traits. In gorillas, for example, a single adult male typically has a harem of several females. This means that several other adult males do not have mates. Males therefore physically compete with other males for access to females. As a consequence, male gorillas are about twice as large as female gorillas, and have much larger canine teeth. Chimpanzee males are modesty larger than females, and also have much larger canines (Plavcan 2001).\nHuman males, on the other hand, are only 15% larger than females and their canines are only slightly larger (Plavcan 2001). What human trait might have become exaggerated due to sexual selection operating on headmen and other leaders? Human brains are about 3 times bigger than chimp or gorilla brains. Neel proposed that headmen become headmen because they’re smart. The dramatic increase in brain size in humans compared to other apes, according to Neel, was due to sexual selection for intelligent leaders.\nNeel only briefly sketched his theory. He didn’t explain how intelligence predisposes men to become leaders, or why leaders would attract more mates than other men.\nI gave these problems a bit of thought. First, what defines a “leader”? It seemed to me that leaders in small-scale societies are those that develop a reputation for making good decisions for the group. But why did this require exceptional intelligence? Making a decision that is good for oneself involves finding the option that maximizes an individual payoff. Making a decision that is good for the group, though, would involve searching over combinations of everyone’s options and payoffs. This could result in combinatorial explosion, a problem that might require a substantial increase in computational resources, i.e., a much bigger brain.\nThere was a problem with Neel’s theory, though. It seemed to predict that, just like the sex difference in body size in gorillas, there should be a sex difference in human intelligence, and there isn’t. I decided to put this project aside for awhile, but resolved to pick it up again at some point in the future.\nFast forward a decade. My new grad student Zach Garfield took up the project and dug into the ethnographic and theoretical literature on leadership. Leaders in traditional societies were indeed often seen as knowledgeable and intelligent. Existing evolutionary theories of leadership, though, could either account for leaders attracting followers, or leaders attracting mates, but not both.\nOur key insight came from old-school anthropology. Human societies have a nested social structure. Human families are nested within residential groups (e.g., hunter-gatherer bands), which are nested within larger, multi-community alliances and ethnic groups:\n\n\n\nFigure 1: Typical social structure of a hunter-gatherer society. An adult male and female cooperate to raise their children in a family. A small number of families coooperate within a band to hunt and gather food and raise their children. Multiple bands form an alliance to buffer variation in food and water supplies, and to defend territory. Figure modified from Parker et al. 2002.\n\n\n\nThis social structure seems to derive from several traits that distinguished ancestral humans from chimps and other apes. In hunter-gatherer societies, males and females typically form long-term pair-bonds to raise their joint offspring in families. These families cooperate to raise their kids (alloparenting), and to hunt and gather food in a band. Bands cooperate to buffer variation in access to resources and to defend territory.\nNotice that in Figure 1, half the group members are kids. Humans, unlike chimps and gorillas, have short interbirth intervals combined with an exceptionally long juvenile period. In natural fertility populations (no modern birth control), women typically have 5 or 6 six kids or more, all of whom will simultaneously depend on her for up to twenty years. Although there is a high child mortality rate, the majority of family members, and therefore group members, are usually kids, ranging from infants to teenagers. And who is making good decisions for this motley crew, day in, day out, morning to night? Their mothers.\nZach and I realized that mothers might be the archetypal leaders in ancestral human societies. Raising human children involves twenty years or more of cooperation between the mother and father. Given the nested social structure of human societies, the cognitive abilities that would be required to lead the family would also be required to lead the band. Almost all the literature on leadership has remarked on the huge male bias in leadership, completely missing the fact that in every society women, just like Nancy Pelosi, routinely lead their families.\nIn summary, Zach and I propose that during human evolution there was sexual selection on both males and females for cognitive traits that resulted in high-quality decision making for the group, be it the family, band, or larger political group. Men preferred women as mates who displayed evidence of high quality decision-making because their families would do better, and women preferred men who displayed the same. Women, who in natural fertility populations are giving birth every few years, would primarily lead at the family level, but would often transition to community leadership roles when they were older. Men with exceptional decision-making skills would rise to leadership positions in the community. Zach, in his research on leadership among the Chabu (a recently settled group of former hunter-gatherers), has found that men and women both occupy community leadership roles. Consistent with our reworking of Neel’s theory, male and female leaders tend to be married to each other.\nThis is the abstract of our paper (preprint here). Comments and criticisms welcome!\n\nLong before the term Machiavellian Intelligence was coined, James Neel was pondering the role of ‘princes’ in the evolution of exceptional human intelligence. The two cornerstones of Neel’s theory – leaders’ superior skills, knowledge, and intelligence (at least as judged by others), and their greater reproductive success – have been amply confirmed by subsequent research. Neither Neel nor later theorists, however, have adequately explained why knowledgeable, intelligent leaders are attractive both to followers and to mates. We aim to fill this gap by operationalizing leaders as individuals who regularly make decisions that benefit most members of the group. Because human nuclear families comprise two unrelated individuals who cooperate for twenty years or more to raise their joint offspring, and because families are nested within subsistence groups, which, in turn, are nested within larger security and political groups, good decision-making skills will provide large benefits to mates as well as to members of one’s subsistence group or larger security and political groups. We further argue that decision-making that benefits others as well as oneself (joint utility optimization – JUO) can be especially computationally complex, and therefore that sexual selection and biological market forces favoring these skills would favor increased brain size. Finally, because parents must make decisions for their cognitively immature offspring, good JUO and other leadership abilities might have initially undergone strong selection in mothers, who provide most of the childcare in natural fertility populations.\nDecision-making that benefits others is one example of a valuable computational service. Other important examples include threat and opportunity detection, gossip and information sharing, cultural transmission, story telling, medicinal knowledge, and advice and counsel. Providing computational services in exchange for a variety of benefits would have helped subsidize a large, energetically expensive brain. Individuals who provided particularly valuable services gained prestige, i.e., additional benefits from fellow group members.\n\n\n\n\n",
    "preview": "posts/2019-03-21-measles-mothers-leadership-and-the-evolution-of-big-brains/../../images/parent_offspring_conflict.svg",
    "last_modified": "2021-04-25T12:47:16-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-16-seven-reasons-why-most-major-depression-is-probably-not-a-brain-disorder/",
    "title": "Seven reasons why most Major Depression is probably not a brain disorder",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2018-12-16",
    "categories": [],
    "contents": "\nVirtually all mental health researchers accept that Major Depression\n(MD) is a mental disorder, i.e., a brain dysfunction. I argue that this\nwidespread belief should instead be treated as an untested\nhypothesis, and further, that this hypothesis is probably\nfalse. Instead, most MD in the general population is probably severe but\nnormal sadness or grief. Here are seven reasons why:\nMost Major\nDepression is caused by adversity\nOrdinary sadness and grief are caused by adverse events. I suspect\nthat a common view about MD is that it fundamentally different, striking\nwithout cause, out of the blue. Most studies of MD do not even bother to\nmeasure recent negative life events. There is a consensus, however, that\nMD, too, is caused in large part by adverse events, and that events of\nhigher severity increase the risk of MD. Many early studies found that\nabout 80% of cases of MD had evidence of at least one adverse event\n(compared to a much lower rate among non-cases). See Figure\n1.\n\n\n\nFigure 1: Life events and onset of major depression. Figure and\ncaption from Mazure\n(1998).\n\n\n\nMore recent studies using twin designs have found similar results,\nand additionally show the interaction between adversity and\nvulnerability factors such as female sex and neuroticism (see Figure\n2).\n\n\n\nFigure 2: Figure from Kendler et\nal. (2004).\n\n\n\nThe stressful life events in Kendler et\nal. (2004) included:\nassault (assault, rape, or mugging)\ndivorce/separation (divorce, marital separation, broken engagement,\nor breakup of other romantic relationship),\nmajor financial problem\nserious housing problems\nserious illness or injury\njob loss (laid off from a job or fired)\nlegal problems (trouble with police or other legal trouble)\nloss of confidant (separation from other loved one or close friend\nother than a spouse or partner)\nserious marital problems (involving a marital or marriage-like\nintimate, cohabiting relation- ship)\nrobbed\nserious difficulties at work\nserious trouble getting along with a close social partner\nserious personal crisis of a close social partner\ndeath or illness of a close social partner\nMost of us hit by such adverse events would also experience low mood,\nsadness, loss of interest, insomnia, and other symptoms of MD. In fact,\nthe study design of Kendler et\nal. (2004) instructed interviewers to rate the severity of the\nevents as “what most people would be expected to feel about an event in\na particular set of circumstances and biography….”\nThe causal effect of adversity on depression is most convincingly\ndemonstrated by the finding that independent adverse events — those,\nlike death of a loved one, that could not be caused by the depressed\nindividual — are powerful predictors of MD.\nDiagnostic\ncriteria for Major Depression were not developed to distinguish the ill\nfrom the healthy but instead to distinguish MD from other psychiatric\ndisturbances\nAlmost all research on Major Depression (MD) diagnoses it using\neither the Diagnostic\nand Statistical Manual (DSM) criteria, or the very similar International\nClassification of Diseases (IDC) criteria. The current DSM-V\ncriteria are basically the same as those in DSM-III, which initiated the\nmodern era of depression research. These criteria are listed in the\nright-most column in Figure 3.\n\n\n\nFigure 3: Historical origins of the symptomatic criteria\nfor Major Depression: Criteria proposed 1950-1980. Table and caption\nfrom Kendler et\nal. 2010.\n\n\n\nAt some point in their lives, everyone will experience at least one\nof these symptoms, and most of us will probably experience most of them.\nPrima facie, none indicate brain dysfunction. To be diagnosed\nwith MD under the DSM criteria, a person must be experiencing 5 or more\nsymptoms most of the day for at least two weeks, and at least one\nsymptom must be sad or depressed mood (dysphoric mood) or loss of\ninterest or pleasure.\nWhere did these criteria come from? You might think they came from\nstudies designed to distinguish mental illness from normal sadness and\ngrief. If so, you would be wrong. Instead, they come from studies that\nwere conducted among groups of individuals who had already been\ndetermined to suffer from a variety of severe psychiatric\ndisturbances (or physical illnesses). The goal of these studies was to\ndevelop criteria that would enable different psychiatrists to reliably\nprovide the same mentally ill patient with the same diagnosis, such as\nbipolar disorder or schizophrenia, not to distinguish the mentally ill\nfrom the healthy.\nSpecifically, the DSM-III criteria can be traced to Stone and Burris\n(1950), which was a clinical study of 50 selected cases; Cassidy et\nal. (1957), which was a quantitative study of one hundred\nmanic-depressive patients compared to fifty medically sick controls;\nFeighner et al. (1972), which was a study of 314 psychiatric emergency\nroom patients and 87 psychiatric inpatients. And Spitzer et al. (1975),\nwhich tested the reliability of the Research\nDiagnostic Criteria (RDC) with 218 psychiatric inpatients. See\nFigure 3.\nNone of the studies that defined MD as we understand it\ntoday included any healthy participants, nor any identified as\nexperiencing only ordinary sorrow, sadness, or grief. Hence, there is no\nreason to believe that, when applied to the general population, the\ncriteria developed in these studies would effectively distinguish the\ntiny minority of individuals with a genuine mental illness from the much\nlarger number of individuals who were suffering ordinary low mood,\nsadness, or grief.\nAs Kenneth Kendler, one of the world’s preeminent depression\nresearchers admits, “most of the diagnostic categories and the\ndiagnostic criteria they contain have been accepted for historical\nrather than strictly empirical reasons” (Lux and Kendler\n2010).\nDSM\ncriteria (mis)applied to community populations generated massive\nprevalence rates\nIt is no surprise, then, that when DSM criteria were first applied to\nthe general population they generated implausibly high prevalence rates\nof mental illness. Over a quarter of the population (28.5%) was\nidentified as suffering a mental illness in the last year, and nearly\nhalf the population (48%) as having suffered a mental illness in their\nlifetimes. For MD, up to 10% were identified to have suffered an episode\nin the last year, and 17% to have suffered MD in their lifetime. See\nFigure 4.\n\n\n\nFigure 4: Table from Regier et al. 1998\n\n\n\nRegier et\nal. 1998 acknowledged that these high prevalence rates called into\nquestion the validity of “diagnoses” based on DSM criteria in community\npopulations:\n\nAlthough it is possible that all of these community-based disorders\nare simply milder cases of essentially the same disorders seen in\nclinical settings, there are other possibilities as well. Based on the\nhigh prevalence rates identified in both the ECA and the NCS, it is\nreasonable to hypothesize that some syndromes in the community represent\ntransient homeostatic responses to internal or external stimuli that do\nnot represent true psychopathologic disorders. The human organism has a\nlimited repertoire of response patterns to various physical, biological,\nand emotional stresses. Transient changes in blood pressure, pulse rate,\nbody temperature, anxiety, or mood are not always indicators of\npathology but of appropriate adaptive responses. It is possible that\nmany people with currently defined mental syndromes (in particular among\nthe affective and anxiety disorders) not brought to clinical attention\nmay be having appropriate homeostatic responses that are neither\npathologic nor in need of treatment — eg, other equivalents of grief\nreactions that meet clinical criteria but are not considered pathologic\nif they are time-limited.\n\nSuch eminently reasonable interpretations of MD in community\npopulations have virtually disappeared from the scientific literature,\nand high prevalence rates are now reported without a bat of the eye. In\nthe US, for example, the government reports\nthat about 1 in 5 adolescent women (19.4%) suffered MD in the past year,\ni.e., putatively suffered a major disorder of the brain.\nReally?\nRegier, first author of the above quote, went on to co-chair the\nDSM-5 Task Force, which, ironically, was widely criticized for further\nmedicalizing normal reactions to common life experiences. The most\nprominent critic was Regier’s predecessor, Allen Frances, chair of the\nDSV-IV Task Force. See, for instance, his\nbook:\n\n\n\nRobert Spitzer, chair of the DSM-III Task Force, expressed similar\nworries:\n\n\nRegier and colleagues shot\nback that Frances’ and Spitzer’s criticisms were motivated by the\nloss of royalties from sales of DSM-IV products. Frances scoffed that these\nroyalties were a relative pittance.\nDepression is a continuum\nIf depression were a major brain disorder that could be identified by\ncounting up symptoms that are common experiences, you might expect that\nfolks with MD would stand apart from everyone else in the distribution\nof their symptoms. But they don’t. Although there is some debate, most\nstudies have found that depression is dimensional, i.e., that it is “a\nquantitative elevation on a continuum of depression-relevant features\nfound in all people” (Prisciandaro and\nRoberts 2005). As can be seen in Figure 5, there\nis no natural separation between those with lower depression scores and\nthose with higher.\n\n\n\nFigure 5: Distribution of scores from the Patient Health\nQuestionnaire (PHQ-9), with a conventional threshold of 10. Nationally\nrepresentative US data from NHANES 2011-2012.\n\n\n\nWhen Cassidy, developer of one of the historical antecedents to the\nDSM-III MD criteria (see Figure 3), was asked\nhow he decided on the threshold, he replied, “It sounded about right” (Kendler et al.,\n2010).\nThere is no principled reason to conclude that higher scores indicate\nbrain disorder instead of more severe sadness.\nMajor\nDepression usually remits in a few months\nMany articles on MD emphasize that it is a chronic disease. This is\nnot true for the majority of cases. The median duration of MD in a\nrecent study of a nationally representative community sample was 6\nmonths, and about 75% of cases remitted within a year. See Figure\n6.\n\n\n\nFigure 6: Survival curves of a cohort (n = 393) with newly\noriginated (first or recurrent) depressive episodes in the general\npopulation; +, censored cases. MinDD: minor depressive disorder. MDD:\nmajor depressive disorder. Figure and caption from ten Have et al. (2017).\n\n\n\nIn addition, the majority of individuals who suffer MD will have a\nsingle episode in their lifetimes. A recent study based on a large,\nnationally representative sample found that, among individuals in\nremission from MD at baseline, the cumulative recurrence rate was 13.4%\nat 10 years, and 27.1% at 20 years (Ten Have 2018).\nOnset of\nMajor Depression is common at all adult ages\nIf MD were a genuine brain disorder, its epidemiology might resemble\nthat of other genuine brain disorders, such as the epidemiology of\ndevelopmental brain disorders, which occur early in life, or the\nepidemiology of brain disorders related to aging, i.e., those that occur\nlate in life.\nTo compare MD with such brain disorders, I used the Institute for\nHealth Metrics and Evaluation data\nvisualization website to display results from the 2017 Global Burden\nof Disease Study. Here are the results for MD compared to three common\ndevelopmental brain disorders:\n\n\n\nFigure 7: Major depression incidence compared to epidemiology\nof brain disorders that appear to be due to developmental disruption.\nData from the Global Burden of Disease study 2017 and healthdata.org.\n\n\n\nAs you can see, new cases of MD are common starting in adolescence\nand throughout adulthood. In comparison, autism spectrum disorders are\nrelatively rare and present at birth (hence prevalence rather than\nincidence is reported). Bipolar disorder and schizophrenia are also\nrelatively rare, with peaks in incidence rates in late adolescence and\nearly adulthood.\nSimilarly, although incidence of MD increases with age, it does not\nresemble other brain disorders related to aging, such as dementia,\nParkinson’s or stroke, which are exceedingly rare until after the age of\n40 or 50:\n\n\n\nFigure 8: Major depression incidence compared to epidemiology of brain\ndisorders that appear to be due to aging. Data from the Global Burden of\nDisease study 2017 and healthdata.org.\n\n\n\nNeurophysiological\ndifferences associated with MD do not necessarily indicate brain\ndeficits\nMD’s status as a “real” illness is often justified by pointing to\nbiological differences associated with depression. The Mayo Clinic, for\nexample, on its info\npage for MD, only lists biological factors as causes of MD, such as\nphysical changes in the brain, brain chemistry, hormones, and genetics\n(adversity, in contrast, is a “risk factor”). Several biological\ndifferences associated with MD are depicted in Figure\n9:\n\n\n\nFigure 9: Biological systems involved in the\npathophysiology of MDD. Clinical studies in major depressive\ndisorder (MDD) and relevant animal models have identified\npathophysiological features in the central nervous system, as well as\nthe major stress response systems, such as the\nhypothalamic–pituitary–adrenal (HPA) axis, the autonomic nervous system\nand the immune system. In the central nervous system, altered\nneurotransmission and reduced plasticity are evident. These could\nunderlie functional changes in relevant brain circuits (for example,\ncognitive control and affective–salience networks), smaller regional\nbrain volumes (for example, in the hippocampus) and neuroinflammation,\nas confirmed in neuroimaging studies. Beyond the central nervous system,\nchronic hyperactivity impairs feedback regulation of the HPA axis, which\nis one of the most consistently reported biological features of MDD.\nWithin the immune system, substantial evidence supports increased levels\nof circulating cytokines and low-grade chronic activation of innate\nimmune cells, including monocytes. However, other aspects of immunity\nseem to be impaired as exemplified by reduced natural killer (NK) cell\ncytotoxicity and T cell proliferative capacity. Once it becomes chronic,\nboth HPA axis hyperactivity and inflammation might converge with\nalterations in the autonomic nervous system to contribute to central\nnervous system pathobiology as well as cardiovascular and metabolic\ndisease, which often co-occur with MDD. The sequence of events leading\nto changes in these interconnected systems and their exact relationship\nis not known. However, mechanistic studies in animals have shown that\nalterations in stress response systems can directly and indirectly\naffect the central nervous system (BOX 3). Conversely, chronic stress\nand associated changes in behaviour can reproduce many of the stress\nsystem alterations, including HPA feedback impairment and inflammation,\nwhich suggests a bidirectional link between central and peripheral\nbiological features of MDD. ACTH, adrenocorticotropin; CRH,\ncorticotropin-releasing hormone; TNF, tumour necrosis factor. Figure and\ncaption from Otte et\nal. (2016).\n\n\n\nThe brain, however, is a biochemical machine. It is not surprising\nthat individuals in different emotional states, such as depressed\nvs. non-depressed, have differences in brain neurophysiology.\nAll brain functions involve neurophysiological changes in the\nbrain. Your brain is undergoing countless chemical changes as you read\nthis post. If you remember anything you’ve read, for example, your brain\nhas undergone some long-term\nchemical changes in synaptic connections between neurons.\nDifferences in subjective experiences must be caused by physical\ndifferences in the brain. Indeed, if there weren’t biochemical and\nneurophysiological changes underlying MD this would be a shocking\nfinding that would shake our materialist conception of the brain to its\ncore.\nMoreover, none of the studies I’ve seen of neurophysiological and\nbiochemical differences in those with MD could distinguish differences\nfrom deficits, even in principle. Every study I’ve looked at has the\nfollowing design: a group of participants that meet diagnostic criteria\nfor MD are compared to a group of “healthy” controls, i.e., individuals\nwithout MD. But most of the individuals with MD are (1) experiencing\nsadness or low mood (one of two necessary symptoms), and (2) have\nsuffered recent adversity. Most members of the control group, in\ncontrast, are not experiencing sadness or low mood, and have not\nsuffered recent adversity. Hence, MD is almost completely confounded\nwith sadness and recent adversity.\nAll of the neurophysiological and other differences in Figure\n9 that are identified as “impairments” or\n“pathophysiological features” are simply differences whose implications\nare currently unknown. They could easily be some of the biological bases\nof normal sadness and other functional responses to\nadversity.\nSummary\nMD is caused by adversity, such as loss of a loved one; it is\ncharacterized by symptoms such as sadness, low mood, and loss of\ninterest, which most people experience when they experience adversity;\nit is diagnosed when the count of such common symptoms exceeds an\narbitrary threshold; it is common among adolescents and adults of every\nage in the general population; it usually lasts for no more than a year;\nand most people will experience at most one episode in their lifetimes.\nTaken together, these facts provide considerable evidence for the\nhypothesis that most MD in the general population is simply severe\nsadness or grief.\nI am far from the only one to make this argument. Horwitz and\nWakefield wrote a book on it (note the Forward by Robert Spitzer, chair\nof the DSM-III effort):\n\n\n\nIf these arguments are correct, why do mental health researchers\ncling to the axiom that MD is a serious brain disorder, instead of\ntreating this as a hypothesis? Here are a few possible reasons:\nMany psychiatrists form their initial intuitions about MD by working\nwith inpatients. MD in these clinical populations can look quite\ndifferent from MD in the general population. Because these individuals\nrefer themselves, or are referred by family members, for psychiatric\ntreatment, their MD tends to be chronic, recurring, and have little\nconnection to recent adverse events, making it seem much less like\n“normal” sadness or grief. Such rarer forms of MD are better candidates\nfor a genuine disorder.\n$$$. The NIH budget for depression research is approaching half a\nbillion dollars annually ($466 billion in\n2018). Sales of antidepressants are generating revenues of around\n$11.6 billion annually. And Big Pharma kicks\nback a lot of bucks to “thought leaders” in medicine, including\npsychiatry. The flow of all these dollars depends on MD being a serious\nillness rather than ordinary sadness.\nSzasz was\nright: psychiatry aims to control undesirable behavior, and severe\nsadness and grief are aversive to sufferers and often inconvenient for\nsocial partners and employers.\nIt’s politically incorrect to question the illness axiom, perhaps\nbecause it runs counter to the laudable effort to reduce the stigma of\nmental illness.\nRegarding #4, viewing mental illness as an ‘illness like any other’\nand promoting biogenetic causes have been thought to reduce stigma. Larkings and Brown (2018)\nconducted a systematic review of the impact of biogenetic beliefs\nregarding mental illness on stigma. They found, on the contrary, that\namong the public:\n\nOverall, these reviews suggest that there are mixed ramifications\nassociated with public endorsement of biogenetic causes. While\nbiogenetic causes might help reduce blame, they might also contribute to\nnegative consequences, such as increased perceived dangerousness, social\ndistance, and pessimism around prognosis.\n\nAmong the mentally ill and mental health professionals, things were\neven worse:\n\nThe present review indicates that the endorsement of biogenetic\ncauses does not reduce stigma in people with mental illness and in\nmental health professionals, and might actually increase stigma and\nnegative attitudes towards mental illness.\n\nFinally, even if most MD is normal sadness and grief sufferers will\noften still need help, sometimes from professionals. Much of my research\nand that of my grad students explores the possibility that some aspects\nof MD function to signal need to social partners. Treatment, though,\nwould focus on solving real-life problems rather than altering brain\nchemistry.\nYou can find papers on my evolutionary approach to depression\nhere:\nhttps://anthro.vancouver.wsu.edu/people/hagen/depression-and-other-mental-health-issues-in-evolutionary-perspective/\nAddendum (2018/12/21)\nIf most MD, as it is currently diagnosed, is not disorder, should we\nkeep calling it Major Depression? Wakefield’s answer, and I think it’s a\ngood one, is no. The term Major Depression should be reserved for\ngenuine disorder, leaving many, if not most, cases of MD in the\ncommunity as false positives (e.g., Wakefield and Schmitz 2013;\nWakefield\n2016). That is, the criteria in Figure 3\nare insufficient to distinguish functional responses to adversity, such\nas sadness and grief, from disordered responses. Additional criteria\nthat might reduce false positives include MD-levels of symptoms in the\nabsence of adversity, that the severity of the response is\ndisproportionate to the severity of the adverse event, persistence of\nsymptoms well past the adverse event (e.g., more than a year), or a high\nrate of recurrence.\n2019/09/22: rearranged order of sections; minor edits for\nclarity\n2022/05/22: updated epidemiology plots\n\n\n\n",
    "preview": "posts/2018-12-16-seven-reasons-why-most-major-depression-is-probably-not-a-brain-disorder/../../images/developmentaldisorders.png",
    "last_modified": "2022-05-22T12:56:56-07:00",
    "input_file": "seven-reasons-why-most-major-depression-is-probably-not-a-brain-disorder.knit.md",
    "preview_width": 1425,
    "preview_height": 750
  },
  {
    "path": "posts/2018-10-09-suicide-and-metoo/",
    "title": "Suicide and #MeToo",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2018-10-09",
    "categories": [],
    "contents": "\nWhat single recent event in a woman’s life is most strongly associated with her subsequent suicide attempt? A sexual assault.\nIn a large, nationally representive study in France, for instance, the odds that a woman would attempt suicide were 17 times higher if she had experienced a sexual assault in the last year than if she had not, the most potent risk factor among those examined. In fact, although sexual assault is associated with many poor mental health outcomes, such as depression, anxiety, and PTSD, it is most powerfully associated with suicidality.\nSexual assault often causes suicidal behavior, my graduate student Kristen Syme and I argue, precisely because women who report sexual assault are frequently not believed. An assault victim’s suicidal behavior is a last-ditch effort to convince others that she is telling the truth.\nOur hypothesis is based on costly signaling theory: When individuals have conflicts of interest and incentives to lie, costly signals can still be trusted.\nTo illustrate: Imagine that Molly is sexually assaulted by her mother’s boyfriend, Mike, but there are no witnesses. Molly tells her mother about the assault, but her mother doesn’t believe her. Molly blamed Mike for her parents’ divorce. The mother suspects Molly is falsely accusing Mike of sexual assault to force her to break up with him and spend more time with Molly.\nMolly, on the other hand, knows that Mike will probably attack her again. She is already traumatized, and if she gets pregnant by Mike, her life will be ruined. If she can convince her mother that she is telling the truth, she hopes her mother will put her daughter’s welfare first, and break up with Mike.\nMolly swallows a bottle of pills shortly before her mother gets home from work. Molly’s suicide attempt, we argue, is a costly signal to her mother that she is telling the truth.\nThe logic is as follows. For Molly, the cost of a suicide attempt is low: if her mother doesn’t get rid of Mike, Molly’s life is probably ruined anyway. Her future is dim, and she is indifferent between being raped again and dying.\nBut if Molly were lying, if she had not been attacked and did not face any risk of being attacked, then the cost of a suicide attempt would be high: Molly is a young healthy women who, yes, isn’t getting as much attention from her mother as she would like, but her future is bright. A suicide attempt is too costly for her.\nThe fact that Molly would only attempt suicide if she faced a real risk of future attack by Mike convinces her mother that she is telling the truth. Her mother breaks up with Mike.\nOur hypothesis requires that attempts are much more common than completions. The costly signal only succeeds if the victim survives her attempt and her social partners consequently make changes that benefit her. Attempts are indeed vastly more common than completions, especially among young women:\n\n\n\nFigure 1: Population rates of suicide attempts and completions in the US, 2001-2011. Note the especially high rate of attempts during the years of highest reproductive value: late adolescence to the mid forties. Data from CDC (2014). Non-fatal self-harm based on data from hospital emergency departments on confirmed or suspected injury or poisoning resulting from a deliberate violent act inflicted on oneself with the intent to take one’s own life or with the intent to harm oneself. Mortality data come from the National Center for Heath Statistics. Figure and caption from Syme et al. 2016.\n\n\n\nOur theory is far from proved. You can read more about it and supporting evidence in this paper and in an earlier paper on self harm. But, despite over half a century of research, the mainstream view that suicidality is a type of psychopathology, that victims’ brains are dysfunctioning and must be fixed with drugs or therapy, has also not come close to being proved.\nIf we are correct, there is nothing wrong with the victim. Instead, there is a profound problem in her social environment. The most effective response to her suicidal behavior would be to substantially improve her life.\nIn particular, victims of physical or sexual assault — perhaps the strongest risk factors for suicidal behavior — need those closest to them to believe them and protect them. Therapy can be enormously helpful, not because it fixes a broken brain, but because it provides exactly the understanding and support that victims desperately need.\nIt is time to ask: does the mainstream model of suicide, by focusing on the putative psychopathology of the victim, help silence her cry for help?\n\n\n\n",
    "preview": "posts/2018-10-09-suicide-and-metoo/../../images/cdcsuicideattempts.png",
    "last_modified": "2021-04-25T12:22:00-07:00",
    "input_file": {},
    "preview_width": 3600,
    "preview_height": 1800
  },
  {
    "path": "posts/2018-08-23-most-shooters-are-suicidal-would-arming-teachers-deter-them/",
    "title": "Most shooters are suicidal. Would arming teachers deter them?",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2018-08-23",
    "categories": [],
    "contents": "\n\n\n\n\nThe FBI recently released a report on pre-attack behaviors of active shooters. The report analyzes the 63 shooters between 2000 and 2013 who had case files, which contain interviews with friends, family members and other background information. The case files were used to identify behaviors and characteristics that might help identify potential shooters before they strike.\nThe statistic that jumped out at my grad student, Kristen Syme, and me was this: many, perhaps most, shooters were suicidal. Of the 35 shooters for whom a determination could be made, 30 had suicidal ideation or engaged in suicide-related behavior prior to the attack (there was no information for the other 28 shooters). Shooters’ circumstances matched those of other suicidal young people, including many cases in the ethnographic record: they were in conflict with powerful others (Syme, Garfield and Hagen, 2016). The FBI report found that most shooters were not motivated by ideology or hatred of a group but instead had strong personal grievances, such as being subjected to disciplinary actions at school or interpersonal conflicts.\nAlthough President Trump and others have argued that arming teachers would deter shooters, it’s hard to deter an individual who seeks death.\nCould arming teachers encourage suicidal shooters?\nA second key statistic from a different set of studies is that many police shootings are “suicide by cop.” That is, a suicidal person who can’t take his or her own life deliberately provokes a police officer to use lethal force against them, usually by threatening the officer with a loaded gun or other weapon. The typical victim of suicide-by-cop is a young adult white male with romantic relationship conflicts. Estimates of the percentage of police killings that are suicide-by-cop vary widely, from 10% to almost 50%, with several estimates in the 30-40% range (Patton and Fremouw).\nArming teachers means millions of teenagers who would only rarely interact with armed police would now regularly interact with armed teachers with whom they might be in conflict. Suicidality is not rare among teenagers. Of the 7.5 million boys in high school, about 10% have seriously considered suicide or made a plan, and 5% have attempted suicide (Lowry et al. 2014). Thus, arming teachers would dramatically increase the opportunities to commit suicide-by-cop, except in this case it would be suicide-by-teacher. If even a tiny fraction of suicidal male teenagers provoked shootouts with teachers and staff at their schools, or simply tried to obtain their guns, it would substantially increase the number of school shootings and deaths.\nWould arming teachers begin to create an informal institution of the active shooting?\nA more important consideration, perhaps, is the implication of arming teachers for our institutions of conflict resolution. The FBI report and Kristen’s work on suicide (Syme, Garfield and Hagen, 2016) both indicate that shooters in particular, and suicidal individuals more generally, are engaged in interpersonal conflict.\nAnthropologists have documented numerous examples of cultural institutions that aim to resolve personal conflicts, and prevent them from devolving into blood feuds that engulf entire communities. These institutions often involve personal combat.\nThe Yanomamo, for example, have a graded series of mechanisms to resolve conflict between two parties, ranging from side-slapping to club fights to ax fights. Phillip Walker documented healed depressed cranial fractures in prehistoric skeletal remains from the Channel Islands off the coast of southern California that could be evidence of club fighting similar to that observed among the Yanomamo.\n\n\n\nFigure 1: Club fight scars on a Yanomamo man. Figure from Chagnon.\n\n\n\nIn Europe and the United States, dueling with swords or pistols historically served to resolve personal conflicts. Though frequently outlawed, dueling persisted for centuries in Europe and was popular in the US, especially the South, up until the Civil War. The toll was substantial. According to Drake (2004), “between 1798 and the Civil War, the [US] Navy lost two-thirds as many officers to dueling as it did to more than 60 years of combat at sea. Many of those killed and maimed were teenage midshipmen and barely older junior officers, casualties of their own reckless judgment….”\nEven today, personal combat to resolve conflicts and restore personal honor is common in US teenagers. Most American males have witnessed, and probably participated in, clandestine fist fights arranged to take place after school. The key point is that aggrieved individuals will attempt to resolve their conflicts using the institutions provided by their culture. An angry Yanomamo would not think to challenge an opponent to rapiers at dawn, nor would a Renaissance nobleman have thought to challenge his opponent to a club fight. Instead, they, and we, choose from the options our cultures provide us.\nSemi-automatic weapons expand the scope of such conflict resolution strategies, enabling aggrieved individuals to effectively attack entire ingroups (school shootings?) or, perhaps as a way to increase one’s ingroup status, kill many members of an outgroup.\nMy concern is that arming teachers would not only fail to deter suicidal shooters and dramatically increase the exposure of suicidal teenagers to folks with loaded firearms, it would also be fateful step towards creating and advertising a new informal institution of conflict resolution that is similar to, but much more deadly than a duel: the active shooting.\nEdited Aug 24, 2018 to add high school suicide stats, Aug 25, 2018 to add subheadings and a bit more detail on dueling, and Aug 4, 2019 to add note on active shooting to resolve conflicts between individuals and groups. Edited Sep 9, 2021 to tweak title.\n\n\n\n",
    "preview": "posts/2018-08-23-most-shooters-are-suicidal-would-arming-teachers-deter-them/../../images/clubfightscars.jpg",
    "last_modified": "2021-09-09T06:20:44-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-25-the-universal-genetic-program-and-the-custom-built-phenotype-implications-for-race-and-sex/",
    "title": "The universal genetic program and the custom-built phenotype: implications for race and sex",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2018-06-25",
    "categories": [],
    "contents": "\nThe human genome, for many, represents differences, essential differences. The real spectre that such differences fuel racism, sexism, and eugenics has lead much of academia to downplay or even deny a role for the genome in human affairs.\nGeneticist David Reich confronted this spectre in his recent New York Times Op-Ed where he raised the alarm that scientists are on the verge of discovering genetic differences in intelligence, cognition, and behavior among the races:\n\nSo how should we prepare for the likelihood that in the coming years, genetic studies will show that many traits are influenced by genetic variations, and that these traits will differ on average across human populations? It will be impossible — indeed, anti-scientific, foolish and absurd — to deny those differences.\n\nI agree. A study of a large European sample, just published in Nature Genetics, found numerous genetic variants associated with intelligence (Savage et al. 2018). Another just-published study found that social mobility was associated with education-linked genetic variants (Belsky et al. 2018). It is not hard to imagine that similar studies using global samples might find even more such variants, and that the distribution of these variants might differ across populations.1\nReich suggests we can avoid the racist implications of such discoveries by learning from the example of the biological differences between males and females:\n\nThe differences between the sexes are far more profound than those that exist among human populations, reflecting more than 100 million years of evolution and adaptation. Males and females differ by huge tracts of genetic material — a Y chromosome that males have and that females don’t, and a second X chromosome that females have and males don’t.\n\n\n…\n\n\nHow do we accommodate the biological differences between men and women? I think the answer is obvious: We should both recognize that genetic differences between males and females exist and we should accord each sex the same freedoms and opportunities regardless of those differences.\n\nAccording to Reich, although we’re learning more about our genetic differences every day, when it comes to freedoms and opportunities we should just ignore all these new discoveries.\nI’m skeptical this approach will keep sexists and racists at bay.\nOne problem with Reich’s attempt at an analogy between sex and race is that his account of genetic differences between the sexes is deeply misleading. More importantly, Reich, though rightly drawing our attention to new and perhaps unsettling discoveries about the genome, is making the same fatal error many of us academics have made. By downplaying or ignoring the profound role of the genome in human affairs specifically, we have left a scientific vacuum that will be filled with racist and sexist claptrap.\nHere I want to sketch the scientific model of the genome that should fill that vacuum. Many of the key features of this model emerged from research on sea urchin development, which began in the 1840’s, more than a decade before Darwin published On the Origin of Species.\n\n\n\nFigure 1: Sea urchin development. Blue color indicates distribution of mRNA transcripts of activinB, a key signaling protein. Figure from Sethi et al. 2009.\n\n\n\n\nBefore tackling the genetics of sex and sea urchin development, though, let’s first get a handle on Reich’s scientific perspective on the genetic basis of race.\n\nHuman genetic differences\nRichard Lewontin famously argued that because the vast majority of human genetic variation is within populations and not between them, racial classification is of no “genetic or taxonomic significance.”\nI used to teach this, but it’s wrong.\nEdwards (2003), citing earlier analyses that Lewontin should have known about (and probably did), including those of Cavalli-Sforza and himself in the 1960’s and stretching back to Fisher in the 1920’s, noted that Lewontin had made a profound mistake:\n\n[Lewontin’s] conclusions are based on the old statistical fallacy of analysing data on the assumption that it contains no information beyond that revealed on a locus-by-locus analysis, and then drawing conclusions solely on the results of such an analysis. The ‘taxonomic significance’ of genetic data in fact often arises from correlations amongst the different loci, for it is these that may contain the information which enables a stable classification to be uncovered.\n\nCorrelations among genetic loci show clear, unmistakable population structure that more or less corresponds to what most Westerners think of when they think of race (e.g., African, Asian, and European):\n\n\n\nFigure 2: Principal components analysis of population structure in 554 individuals. First two principal components (PCs) are shown here. Each individual is represented by one dot and the color label corresponding to their self-identified population origin. The percentage of the variation in genetic distances explained by each PC is shown on the axes. Figure and caption from Xing et al. 2009.\n\n\n\n\n\n\nFigure 3: Regional ancestry inferred with the frappe program at K = 7 (13) and plotted with the Distruct program (31). Each individual is represented by a vertical line partitioned into colored segments whose lengths correspond to his/her ancestry coefficients in up to seven inferred ancestral groups. Population labels were added only after each individual’s ancestry had been estimated; they were used to order the samples in plotting. Figure and caption from Li et al. 2009.\n\n\n\nDiscerning population structure from genetic variation works best, however, when analyses are restricted to variation that is not under selection, the >90% of the genome that is “junk,” i.e., has no influence on the phenotype (non-functional DNA). The genetic basis of “Race,” in this case, cannot have anything to do with racial traits (and there are critiques of the genetic clustering algorithms, e.g., Lawson et al. 2018). I blog about this here.\nThere are also population differences, though, in the fraction of the genome that actually does something (functional DNA, currently estimated at about 8.2% of the entire genome; Rands et al. 2014). Some of those differences represent adaptations to local environmental conditions:\n\n\n\nFigure 4: Examples of human local adaptations, each labeled by the phenotype and/or selection pressure, and the genetic loci under selection. Figure and caption from Fan et al. (2016).\n\n\n\nAnd some differences reflect population-specific disease risk: the dramatic population expansions that have occurred since modern humans left Africa have resulted in large number of rare, and probably somewhat deleterious, mutations that tend to be population-specific (Keinan and Clark 2012).\nDespite their disagreements, Lewontin, Edwards, Cavalli-Sforza, and Reich all focus on genetic differences, the 4-5 million variant sites that we now know exist in each human genome:\n\n\n\nFigure 5: Variant sites per genome, relative to the reference human genome. Each symbol is one genome. Organized by population, and sorted by the number of variants. Figure from The 1000 Genomes Project Consortium, 2015.\n\n\n\nPopulation differences in functional DNA, in particular, are the basis of Reich’s warning that, if it hasn’t already, science will eventually produce evidence of “substantial biological differences among human populations.”\nThe question is, substantial relative to what?\nHuman genetic similarities\nFour-to-five million variants in each human genome sounds like a lot, until you realize that the human genome has about 3 billion nucleotides (bases), each of which could vary. Thus (somewhat naively) only about 0.0013–0.0016 of each genome varies. (The story is complicated by structural variants. See this footnote2.)\nThis means that all the genetic information about “race” — the population differences in demography, history, and adaptive and maladaptive differences — as well as individual differences, comes from a very small fraction of the genome, represented here by the orange square:\n\n\n\nFigure 6: The fraction of the genome that differs between any two individuals. Different parts of the genome vary in different individuals, of course, and these differences are distributed across the genome.\n\n\n\nWhat is striking, at least to me, is not our “substantial biological differences” but our substantial biological similarities. Reich can only conclude that our differences are substantial by ignoring all the green squares.\nThere is a good reason, though, why Reich and many other geneticists have ignored all the green squares and focused on the orange one: it’s much easier to analyze. In science, to determine if X is a cause of Y, which typically requires that we change X and see if Y changes, we instead often start by determining if differences in X correlate with differences in Y. This, of course, requires that there are differences in X and Y to begin with.\nWhen it comes to the orange square, nature has already set up the experiment for us. We simply need to match the 4-5 million genetic differences that already exist, to the geographical or heritable phenotypic differences that already exist. In practice this is not so simple (far from it), but the rapidly expanding number of complete genome sequences from populations around the world have rapidly expanded our ability to match genetic variants to heritable phenotypic variants such as differences in eye, hair, and skin color.\nDeciphering the biological “meaning” of all the green squares, on the other hand, faces immense conceptual and technical challenges. How does one determine which of the hundreds of millions of bases in functional DNA that don’t vary are responsible for the different functional parts of phenotype, such as hearts, livers, and lungs, that also don’t vary? A priori, any of the green squares could be involved in the development and functioning of any part (or all) of the phenotype.\nWhere to start?\nThe headlines about exciting discoveries in the orange square and the near absence of the same regarding each and every green square, have led the public, and many scientists too, to equate the orange square with the genome, and all but ignore the green squares. This imbalance, evident in Reich’s own op-ed, will “invite the racist misuse of genetics” that Reich rightly fears.\nHere I want to give a small taste of the science to decode the green squares that has been slow-cooking for decades, but hasn’t generated sexy headline after sexy headline because it involves, e.g., embryonic development in critters like sea urchins, fruit flies, frogs, nematodes, and mice, or arcane cellular functions like the xenobiotic induction of cytochrome P450 enzymes via nuclear PXR and CAR receptor activation in hepatocytes.\n\n\nThe genetic program (the green squares)\nIt has been recognized since antiquity that a seed autonomously develops into the same type of plant that produced the seed, and an egg into the same type of animal that produced the egg. We now know that seeds and eggs are single cells that contain an entire genome. Because adaptations evolve by the natural selection of DNA sequence variants, the functionality of an organism — it’s heart, liver, lungs, bones, muscles, etc. — is somehow encoded in its genome, and that functionality is “constructed” during development.\nAs the organism develops via cell division, the entire genome is copied into each daughter cell; hence, every cell (with a few exceptions) contains a complete and identical copy of the genome. During development, daughter cells begin to differentiate into different types. Since the genome has not changed, yet encodes the functionality of, e.g., neural vs. epithelial cells, it must be the case that different parts of the genome, and different combinations of those parts, are involved in the development of the different cell types.\nJacob and Monad’s (1961) Nobel prize winning discovery that a protein produced by one gene could regulate the expression of a different gene was a key breakthrough in understanding how different parts of the genome encoded different cellular functions. Moreover, this regulatory effect depended on environmental signals. Specifically, Jacob and Monad discovered a genetically encoded regulatory “circuit” in the bacteria E. coli, the lac operon, which produced an enzyme to metabolize lactose, a less valuable food, only if (1) lactose was present and (2) glucose, a more valuable food, was not. It turned out that such regulatory effects aren’t rare. Instead, they’re the whole enchilada.\nBased on this discovery, and the insight that similar regulatory mechanisms were operating during the development of the organism, Jacob and Monad (1961) and Mayr (1961) independently introduced the “genetic program” model, which proposes that the functional genome is something like a computer program that, starting from a single cell, creates an adult organism (for the history of this model, see Gann 2010 and Peluffo 2015). The genetic program comprises a large number of what are now usually called gene regulatory networks (GRNs; Levine and Davidson 2005):\n\n\n\nFigure 7: A gene regulatory network. Top: Schematic GRN mechanism. Bottom: Functional representation. Figures from Genomes to Life Program Roadmap, April 2001, DOE/SC-0036, U.S. Department of Energy Office of Science.\n\n\n\nA GRN is set of genes (protein coding sequences) and non-coding DNA sequences that influence the transcription of those genes to RNA and the translation of RNA to protein. These genetic elements interact in a circuit-like fashion to provide some cellular function, often in response to some environmental signal.\nDeciphering the GRNs that control development of the organism from zygote to adult is extraordinarily difficult. To validate that regulatory element X controls gene Y to provide cellular function Z at time t, one must first identify, out of hundreds of millions of nucleotides, candidates for all the regulatory genetic elements and targets. This is typically done by determining which genes are expressed in which cells and when, as seen for activinB in Figure 1. One must then show that perturbing regulatory element X changes expression of gene Y and thus changes cellular function Z at time t. Such perturbations cannot be done with human embryos (except, in some jurisdictions, at a very early developmental stage; see Rossant and Tam 2018), so almost all the work in this area uses model organisms.\nSea urchins have been especially important model organisms for the study of embryonic development since the middle of the 19th century because their eggs and embryos are relatively transparent, fertilization is external, and embryogenesis is rapid, allowing the early developmental process to be easily observed under a microscope (Ernst 1997).\nReich’s claim that sex differences in phenotypes are due to “huge tracts of genetic material” possessed by one sex and not the other was disproved over 100 years ago by classic experiments that found that sea urchin eggs with only a sperm nucleus or only a female pronucleus developed relatively normally, demonstrating that chromosomes from the female and male are developmentally and genetically equivalent (Ernst 1997).\nWe now know that in most mammals, including humans, sex is determined by the presence or absence of a single gene on the Y-chromosome, SRY, which is only possessed by males. A single gene cannot encode all the functional differences between males and females (e.g,. ovaries vs. testes), and furthermore, the Y-chromosome is very small, containing fewer than 100 genes (Jobling and Tyler-Smith 2017). Hence, SRY is simply a switch. Most of the sex-specific functionality of male and females comprises GRNs encoded in the autosomes, which are the chromosomes that are present in both males and females.\nSRY activates SOX9 on chromosome 17 (an autosome), which initiates development of the testes:\n\n\n\nFigure 8: Overview of sex determination in mice. Chronological flow of early mouse sex differentiation; the grey area indicates the period of sex determination. During mouse embryogenesis, bi-potential gonads (yellow) arise from the genital ridges by 10.5 days post coitum (dpc). In somatic cells of XY genital ridges, Sry expression (shown in dark blue beneath the schematic) starts at 10.5 dpc, reaches a peak at 11.5 dpc and then wanes by 12.5 dpc. A few hours later, Sox9 expression (shown in light blue beneath the schematic) is upregulated to induce differentiation of Sertoli cells. Sox9 expression peaks at 11.5-12.5 dpc, continues to be expressed postnatally and is supported by several positive-feedback loops (including FGF9, prostaglandin D2 and SOX9 itself), and SOX9 subsequently activates many male-specific genes, including Amh. At 12.5 dpc, testis cords have formed, and morphological differences between testis (blue) and ovary (pink) are evident. In the absence of SRY, genes such as Wnt4, Rspo1 and Foxl2 are expressed in a female-specific manner and induce ovarian development, as characterized by the expression of follistatin and many other ovary-specific genes. Abbreviations: Amh, anti-Müllerian hormone; dpc, days post coitum; FGF9, fibroblast growth factor 9; FOXL2, forkhead box L2; PGD2, prostaglandin D2; RSPO1, R-spondin 1; SOX9, SRY box containing gene 9; SRY, sex-determining region on the chromosome Y; WNT4, wingless-type MMTV integration site family, member 4. Figure and caption from Kashimada and Koopman 2010.\n\n\n\nSry is only active for a very short period of time, just enough to activate a GRN containing SOX9, which has feedback mechanisms to maintain expression levels of SOX9 even after Sry is no longer expressed.\nSome of the functional differences between males and females, such as testes and ovaries, develop early in gestation, as pictured here, whereas others, such as (in humans) upper body musculature, breasts, and pelvic changes, develop much later, during puberty.\nThus, sex differences in the phenotype are mostly due to huge tracts of genetic material encoding GRNs on the autosomes (reminder: possessed by both sexes) that are activated differently depending the presence or absence of a single genetic switch, SRY. In fact, there are XY individuals who develop as (mostly) normal women due to a mutation in SRY that prevents it from initiating testicular development. Although most XY women do not have functioning ovaries, there is one case of an XY individual who developed as a completely normal woman, with a uterus and histologically normal ovaries, due to a loss-of-function mutation in an autosomal gene that itself activates SRY (Biason-Lauber et al. 2009). There are also XX men. These examples prove that most of the functional differences between males and females are encoded in GRNs on the autosomes and the X-chromosome, and that SRY is just a genetic switch.3\nIn some species, the sex-determination switch is environmental, not genetic. In many reptiles, for example, male developmental pathways are triggered at one temperature and female pathways at another. A key molecular component in temperature-dependent sex determination in red-eared slider turtles has just been found, in fact (Ge et al. 2018).\nConceptually, then, there isn’t much difference between a genetic switch, such as SRY, and an environmental one, such as a temperature threshold: they both serve as inputs to GRNs that then generate sexually distinct phenotypes. Sex differences aren’t explained by genetic differences; instead, the universal genetic program has gotten different information (the presence or absence of a switch) and built different phenotypes.\nBased on studies of their roles in sea urchin embryo development, GRNs are often modeled as Boolean circuits:\n\n\n\nFigure 9: Structure and function of different types of subcircuit. (A) Positive feedback subcircuit. (B) Community-effect subcircuit. (C) Coherent feedforward subcircuit. (D) Incoherent feedforward subcircuit. (E) Mutual-repression subcircuit. (F) Double-negative gate subcircuit. All except the subcircuit in D are examples from the sea urchin endomesoderm GRN. (Left) The topologies of regulatory interactions in each subcircuit. (Right) The expression of each gene in the subcircuit under each condition, as determined by Boolean modeling. The indicated time steps do not represent real time. Blue, expression; gray, no expression. DN, double-negative gate; D/N, Delta/Notch signaling; Mat., maternal; Skel., skeletogenic; Ubi, ubiquitous activator. Figure and caption from Peter and Davidson 2017.\n\n\n\nHere is the network of such genetic subcircuits involved in the early development of the sea urchin embryo (different colors indicate the different subcircuits):\n\n\n\nFigure 10: Distribution of subcircuits in the endomesoderm GRN. Subcircuits of each type identified in the endomesoderm GRN model are color-coded as follows: pink, double-negative gate; dark blue, coherent feedforward subcircuit; light blue, community-effect subcircuit; yellow, positive feedback subcircuit; green, signaling interaction; red, toggle switch circuitry; and brown, mutual-repression subcircuit. For recent updates of the endomesoderm GRN model, see http://grns.biotapestry.org/SpEndomes/. Figure and caption text from Peter and Davidson 2017.\n\n\n\nNotice that the network takes inputs from the mother (upper left), in the form of maternal mRNA sequestered in the egg. More importantly, notice the complexity of the GRN governing early development in the sea urchin.\nThe custom-built phenotype\nThese examples motivate the genetic program model. The genome is an extraordinarily complex developmental program that is identical in all humans, including males and females and all populations, and which comprises a very large number of GRNs. These GRNs evolved by natural selection, and are therefore adaptations (for an excellent review of GRNs in evolutionary context, see Rebeiz et al. 2015). GRNs, which are often modeled as Boolean or continuous circuits (e.g., Karlebach and Shamir 2008), govern development of the zygote from a single cell to an adult, and could be active throughout life. Many GRNs take environmental cues as inputs, including environmental factors like temperature, chemical gradients within and between cells, inter- and intracellular signaling molecules, DNA methylation, and interactions among elements of the GRN mediated by, e.g., transcription factors and RNA, that then alter developmental trajectories in ways that would have increased fitness in the ancestral environment of that GRN (for an interpretation of this model in terms of evolutionary game theory, see Hagen and Hammerstein, 2005). Individuals who grow up in different language communities might have brains that differ in some respects, for example (e.g., Geary and Bjorklund 2000), as might those who grow up in different family environments (e.g., Quinlan 2003).\nHere is a dramatic example in water fleas. These two individuals are genetic clones. The morph on the left was exposed to a chemical cue from a predatory fish and therefore developed a protective (but “expensive”) helmet and long tail; the one on the right was not exposed to the cue, and could therefore divert helmet-building resources to other uses:\n\n\n\nFigure 11: Two individuals of a single clone of the Asian and African water flea, Daphnia lumholtzi. The individual on the left was exposed to chemical cues from predaceous fish (induced); the individual on the right was not (control). The sharp helmet and extended tail spine of the induced morph protectD. lumholtzi from fish predators. The uninduced form was formerly described as a different species (D. monacha Brehm 1912). Green (83), in an accurate and prophetic study, related the occurrence of both morphs to differences in fish predation. The induction of this morphological defense has now been implicated as a key factor in the success of D. lumholtzi invading North America (84). Figure and caption from Agrawal 2001.\n\n\n\nSome GRNs might also take as inputs the presence or absence of genetic elements that are under, e.g., frequency-dependent or local selection, interpretable as genetic switches, cues, or randomization devices; SRY is a clear example. Speculatively, there might be additional genetic switches or cues in humans that would generate distinct (possibly behavioral) phenotypes, as there are in some other species (e.g., Le Rouzic et al. 2015).\nThe point here is that such genetic elements, whose states vary among individuals, are often best interpreted as serving the same role as environmental cues: as information about upcoming selective conditions that serve as inputs for various universal GRNs (Leimar et al. 2006). The genetic program then builds a phenotype customized to “do well” in those selective conditions.\n\n\n\nFigure 12: The genetic program model of the genome. A universal genetic program comprising thousands of GRNs (green squares) reads many different types of inputs and to produce a phenotype that is custom-built for (its best guess about) the upcoming selective environmental conditions.\n\n\n\nImplications for sex and race\nHow does the genetic program model help us think about sex differences and racial differences? Regarding sex differences, the presence or absence of SRY is very important, yes, but for any individual woman or man, all the other inputs to the developmental program — the epigenetic switches, environmental cues, and perhaps other frequency dependent switches — are also very important. The genetic program determines a person’s phenotype using all its inputs, not just SRY. The same goes for race. Some parts of the orange square will have some influence on the phenotype, true, but all of the vastly greater number of green squares and their associated inputs will each have an important influence too. This is the critical background that is utterly missing from Reich’s op-ed.\nThere are therefore at least two better answers to Reich’s question: “How do we accommodate the biological differences between men and women,” and by extension, the different races? First, at the genetic level, the extraordinarily rich circuits encoded in the GRNs of the human genome are essentially identical in all humans, and, if the complexity of the GRN governing early development of the sea urchin is any guide, their informational content will vastly outweigh that contained in genetic differences. The weight of genetic evidence points to biological similarity, not difference.\nSecond, at the phenotype level, we all develop with a unique combination of inputs to those GRNs, and so each of our phenotypes is a unique variation on the human theme, custom-built by the universal genetic program to function “well” in our unique environmental circumstances. We could choose to discriminate on the basis of one such input, e.g., SRY, but then why not another, e.g., whether our parents were separated when we were young (Quinlan 2003)? If the genetic program model is correct, there are many possible biological distinctions that could be the basis of discrimination, each a consequence of environmental circumstance, and any one of us could find ourselves on the wrong side of the divide. If water fleas were as in-groupy/out-groupy as humans, the unhelmeted fleas could discriminate against those helmeted heathens, not realizing that their own offspring would also develop helmets should predatory fish appear. Rawl’s veil of ignorance is one approach to a just society of unique individuals.\n\n\nI do take Reich’s larger point, though, that it has been an enormous mistake on the part of many scientists and scholars to downplay, ignore, or deny the utterly central role of the genome in who we humans are. Perhaps this is because, today, we understand more about our relatively few genetic differences than we do about our overwhelming genetic similarities. Given that we still don’t fully understand the GRNs involved in sea urchin embryonic development, a system that has been intensively studied for years, it will almost certainly be many decades before we have even a rudimentary understanding of the human genetic program.\nNevertheless, the genetic program, or something like it, will eventually constitute the “meaning” of the functional genome as the product of three and a half billion years of evolution of the human lineage. The orange square will take its place mostly as a combination of the randomness inherent in evolution and as a record of the final few moments of our evolution, the last hundred thousand years since modern humans exploded out of Africa. It is therefore perhaps time that instead of fearing it, we consider embracing the human genetic program, including all of its inputs, as our common biological heritage, because it’s pretty clear that’s where the science is going to end up.\nRegarding the construct “intelligence,” my views correspond pretty closely to Cosma Shalizi’s, see, e.g., here, here, here, and here. A taste: “If we must argue about the mind in terms of early-twentieth-century psychometric models, I’d suggest that Thomson’s is a lot closer than the factor-analytical ones to what’s suggested by the evidence from cognitive psychology, neuropsychology, functional brain imaging, general evolutionary considerations and, yes, evolutionary psychology (which I think well of, when it’s done right): that there are lots of mental modules, which are highly specialized in their information-processing, and that almost any meaningful task calls on many of them, their pattern of interaction shifting from task to task.”↩︎\nMore than 99.9% of the 4-5 million variants are single nucleotide polymorphisms (SNPs) and short insertions or deletions. However, it has recently been discovered that larger variable chunks of DNA, termed structural variants (SVs), though very few in number (the typical genome has a few thousand), affect more bases. See Sudmant et al. 2015 and The 1000 Genomes Project Consortium, 2015. According to Sudmant et al. 2015, when collapsing mCNV (multi allelic copy-number variants) sites “carrying multiple copies as well as homozygous SVs onto the haploid reference assembly, a median of 8.9Mbp of sequence are affected by SVs, compared to 3.6 Mbp for SNPs.” Most bases affected by SVs are either due to deletions or copy number variants. There is evidence of purifying selection against deletions in functional regions of the genome, but less so for copy number variants, indicating relaxed constraints on the latter. Common SVs are shared across continents whereas rare ones tend to be continent-specific.↩︎\nThe Y-chromosome has a very complex structure, including pseudoautosomal regions that recombine with the X-chromosome. There are some genes in the male-specific region of the Y-chromosome other than SRY that do play critical roles in the male phenotype, however. Hence, some of the male genetic program is encoded on the Y-chromosome, and not on the autosomes or X-chromosome. For review, see Jobling and Tyler-Smith 2017.\n\n↩︎\n",
    "preview": "posts/2018-06-25-the-universal-genetic-program-and-the-custom-built-phenotype-implications-for-race-and-sex/../../images/Daphnia.card.jpeg",
    "last_modified": "2021-05-19T07:56:41-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-03-03-while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary/",
    "title": "While it may be true that Evolutionary Anthropologists consider themselves scientists...",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2018-03-03",
    "categories": [],
    "contents": "\nCasey Roulette, a\nformer PhD student of mine who is now an assistant professor at San\nDiego State University, recently received an email from a member of the Biology\nDepartment who was irate that Casey’s evolutionary anthropology\ncourse, Evolution of Human Nature, was being considered to\nfulfill “Natural Sciences” GE reqs. She informed him that she had\ncomplained to her Chair, who in turn complained to the Dean of the\nCollege of Sciences, and that “Both will be preparing a letter to be\nsent to the GE committee indicating that the College of Sciences does\nnot support this course as a GE course.” The email concluded:\n\nWhile it may be true that Evolutionary Anthropologists consider\nthemselves scientists and use the terms evolution and evolutionary, the\n“Evolutionary Biology” represented in this course does not reflect or\nrepresent modern Evolutionary Biology as defined by Evolutionary\nBiologists.\n\n\nIn our meeting with the Instructor, who I know is an Assistant\nProfessor, I thought it was not collegial to point out that his views on\n“evolutionary anthropology” are not considered accurate by those of us\ntrained in Evolutionary Biology.\n\n\n…\n\n\nIt is deeply concerning that students could leave this campus with\nsuch an erroneous understanding of such a fundamental scientific\nprocess.\n\nShe attached an evaluation from\nthe SDSU Department of Biology. The first part argued that, on\nprogrammatic grounds, the course did a good job fulfilling the Social\nScience reqs. I agree.\nThe second part, however, echoed the email, arguing on\nscientific grounds that the course did not qualify as a GE\ncourse in the Natural Sciences because it presented a view of biology\nthat was incomplete, biased and incorrect.\nThat’s odd. If true, Casey’s course shouldn’t qualify for GE in\neither the Social or Natural Sciences. In fact, his course shouldn’t be\ntaught at all.\nSDSU biologists’ first concern was that Casey presents evolution as\nsynonymous with adaptation:\n\nThe “Evolutionary” approach presented in this class seems to present\nbiological evolution as synonymous with adaptation via natural\nselection. Natural selection is but one of the five evolutionary forces\nthat determine patterns of variation within and among species. For\nexample, no information is provided on the very important role of Random\nGenetic Drift in shaping genetic variation.\n\n\nThe practice of interpreting biological patterns only through the\nlens of adaptation has been labeled the “Adaptionist program”. For four\ndecades, biologists have recognized the folly of this approach. The\nphrase “The Fallacy of Intuitive Evolutionary Thinking” has also been\nused to describe the assumption that every biological feature has been\noptimized by selection. Evolutionary biologists accept the power of\nDarwinian natural selection but we do so understanding that natural\nselection is a complex process.\n\nTheir second concern was that Casey does not discuss genetic drift,\nand its role in human evolution:\n\nThere is no discussion of the Neutral and Nearly Neutral models of\nEvolution, which have been the dominant models of molecular evolution\nfor 50 years.\n\n\nIt is also well established that Homo sapiens has an effective\npopulation size (Ne) of approximately 10,000. With such a small\neffective population size it is widely understood (among evolutionary\nbiologists) that selection is inefficient in H. sapiens. Because of this\nevolutionary biologists would expect to see high levels of both fixed\nand segregating neutral and nearly neutral (slightly deleterious)\nalleles within and between human populations. Our most current\nunderstanding of genetic variation in does not fit in to the pan\nselectionist approach presented in this class.\n\nBy “the pan selectionist approach” I suppose they are referring to\nthe focus of Casey’s course, Human\nBehavioral Ecology (HBE).\nThe debate\nover adaptationism, however, is an ongoing debate,\none that began more than a century ago. Each side has involved towering\nfigures in evolutionary biology like Fisher, Wright, Williams, Hamilton,\nMaynard Smith, Gould, Lewontin, and Kimura.\nThe neutralist–selectionist debate — are patterns of genetic\nvariation primarily explained by random genetic drift or natural\nselection? — is, again, a debate.\nDoes Casey’s syllabus\npresent both sides of these debates?\nThe assigned reading in Week 2 of the course is Stephen Jay Gould’s\nSociobiology:\nthe art of storytelling. It was Gould, of course, who with\nco-author Richard Lewontin, introduced the phrase “Adaptationist\nProgramme” in their hugely influential article The\nspandrels of San Marco and the Panglossian paradigm: a critique of the\nadaptationist programme. One of Gould’s key points is the\nimportant role of genetic drift.\nMore importantly, one of two required books for the course is Laland\nand Brown’s Sense\nand nonsense: Evolutionary perspectives on human behaviour. I\nwould count Laland as a consistent critic of the adaptationist program\n(in favor of his alternative, niche construction,\ndeveloped in collaboration with eminent population geneticist Marcus Feldman)\nwhose book is a good-faith effort to describe the controversies as they\napply to human evolution. Laland and Brown do discuss neutral theory,\ndrift and molecular evolution, especially their intriguing parallels\nwith cultural evolution. Casey also assigned chapter 7 of Evolution\nof Human Behavior, by Agustin Fuentes, who I would also count as a\ncritic of\nadaptationism.\nAlthough Casey’s course focuses on the evolution of the human\nbehavioral phenotype and not molecular evolution, I see no evidence that\nthe course, which also assigns Wrangham’s Demonic\nmales, is giving students a biased overview of the debates\nover adaptationism and neutralism vs. selectionism.\nAnd it’s not quite true that neutral models “have been the dominant\nmodels of molecular evolution for 50 years.” Instead, the intense debate\nhas perhaps resulted in a consensus. According to evolutionary\ngeneticists Charlesworth\nand Charlesworth, “From the late 1980s, the neutral theory came\nincreasingly to be used as a null hypothesis, against which alternative\nhypotheses could be tested, including the models of the effects of\nselection on neutral or nearly neutral variability at linked sites….”, a\npoint also made by Masatoshi\nNei and Kimura himself.\nAt this point I should admit that I’m an unreconstructed ultra-Darwinian\nFundamentalist who, each night, reads his daughters passages from\nthe Selfish Gene. Depression\nis an adaptation! Drug use is an\nadaptation! This was my logo for the first iteration of this\nblog:\nPanglossCasey, what’s with the balanced overview? Did I teach you\nnothing?\nIn my view, critics of adaptationism have things backwards. For\nadaptationists, the question is not, do constraints and noise play an\nimportant role in organism structure? The question is, why don’t\nconstraints and noise dominate organism structure? Constraints and noise\npermeate physical processes, yet organisms — intricate machines that\nsurpass all human technology — somehow manage to make precise copies of\nthemselves in hostile environments. It is exactly this problem,\nas I discuss in a bit more detail\nhere, that adaptationists are trying to solve.\n\n\n\nThe population genetics folks have it right: random genetic\nvariation, and more generally, noise, by-products, constraints, and\nthermodynamically favored physics and chemistry, should always be the\nnull hypotheses that an adaptationist hypothesis needs to beat.\nCourse approval at SDSU is a pretty trivial topic, but the evidence\nfor noise vs. selection in the human genome is not. I therefore thought\nI would use this post as an opportunity to learn a bit more about it.\nIt’s not meant to be a comprehensive review, just a taste of some major\nissues. I’m not a genetics guy — far from it — so if I make any\nmistakes, let me know in the comments.\nPopulation genetics 101\nThe SDSU biologists’ argument that, due to a small effective\npopulation size (\\(N_e\\)), selection is\n“inefficient” in H. sapiens rests on a classic result from\npopulation genetics. Each generation is a sample of alleles from the\nprevious generation. A particular allele might decrease the probability\nthat individuals with the allele reproduce (negative selection),\nincrease the probability (positive selection), or have no effect\n(neutral). This effect is quantified by the selection coefficient, \\(s\\), which is the difference in fitness,\n\\(W\\), between two alleles, the wild\ntype, A, and a mutant type, B:\n\\[\ns = W_B - W_A\n\\]\nEven when \\(s=0\\), the frequencies\nof A and B will change at least a little bit from generation to\ngeneration due to random differences in the survival and reproduction of\nindividuals with A vs. B. On average, however, the frequency of a\nneutral mutant allele in a new generation is, under some strong\nsimplifying assumptions, simply the frequency of the allele in the\noriginal generation.\nIf the population is small, however, then the “sample size” is small.\nJust as we learn in statistics, the smaller the sample size, the larger\nthe variance in the “sample estimate.” Thus, in small populations, the\nfrequency of a neutral allele will bounce around from generation to\ngeneration more than it would in large populations, eventually going to\neither 0 (lost) or 1 (fixed). In particular, the frequency of a new\nmutation is small, so there’s a good chance that it won’t be “sampled,”\nand will thus be lost.\nBut what if the allele isn’t neutral? Naively, if an allele has a\nnegative effect on fitness, \\(s<0\\),\nthen its frequency should go to 0, and if it has a positive effect on\nfitness, \\(s>0\\), then its frequency\nshould go to 1. Again, though, allele frequencies fluctuate randomly. If\nthese fluctuations are often larger than the systematic effects of\nselection, then a deleterious allele can become fixed, and a beneficial\nallele can be lost. This is much more likely to happen when a population\nhas a small effective population size (for review of the \\(N_e\\) concept, see Charlesworth 2009).\nGenerally, only when the magnitude of \\(s\\) is greater than twice the reciprocal of\neffective population size will the fate of an allele will be dominated\nby its fitness effects:\n\\[\n|s| > \\frac{1}{2N_e}\n\\]\nOtherwise, its fate will be dominated by drift. This is what SDSU\nbiologists were referring to: as \\(N_e\\) decreases, the fate of alleles will\nincreasingly be dominated by drift.\nThe importance of selection also depends on \\(s\\), however, the fitness of a new allele\nrelative to an existing allele, a fact that the SDSU biologists failed\nto mention.\n\n\n\n\n\nFigure 1, from Lanfear et\nal. 2014, illustrates the relationship between \\(N_e\\), \\(s\\), and the rate of evolution — the\nsubstitution rate vs. the mutation rate — conveniently using parameters\nthat approximate those for ancestral Homo. The top panels\nillustrate positive selection, and the bottom panels negative selection.\nThe left-hand panels show that the substitution rate increases not only\nas \\(N_e\\) increases, but also as \\(s\\) increases. The right-hand panels depict\nthe relationship between the ratio of substitution rate to the mutation\nrate (on the y-axis) and \\(N_es\\). For\n\\(N_es < 1\\) (to the left of the\ndotted line), the substitution rate is approximately the mutation rate\n(drift). For \\(N_es > 1\\) (to the\nright of the dotted line), the substitution rate either begins to\nskyrocket beyond the mutation rate (positive selection) or drop to 0\n(negative selection).\n\n\n\nFigure 1: The relationship between substitution rate (in\nsubstitutions per site per year) and effective population size (\\(N_e\\)) under genetic drift and natural\nselection (the \\(N_eRR\\)) [8]. These\nrelationships were calculated assuming a mutation rate of \\(1 \\times 10^{-9}\\) mutations per site per\nyear, approximately that found in humans. (A,B) show the substitution\nrate of mutations for a range of positive (A) and negative (B) selection\ncoefficients (denoted ‘s’). (C,D) show the same data, but in this case\nthe y-axis shows the substitution rate relative to the mutation rate,\nand the x-axis shows the product of \\(N_e\\) and the selection coefficient for\npositive (C) and negative (D) mutations respectively. A dashed line\nhighlights where \\(N_es = 1\\), below\nwhich mutations are often considered ‘effectively neutral’. Note that\ngenetic drift predicts a flat \\(N_eRR\\)\nfor neutral mutations, where \\(s =\n0.00\\) in (A,B). In (C,D), this is reflected by the substitution\nrate equaling the mutation rate, giving a value of 1 on the y-axis, when\n\\(N_es = 0\\). Figure and Caption from\nLanfear et\nal. 2014.\n\n\n\nSo, for humans, we need two pieces of evidence, (1) \\(N_e\\), and (2) the plausible range of\nvalues that \\(s\\) might take, referred\nto as the distribution of fitness effects (DFE) for new alleles.1\nEmpirical\nestimates of effective population size (\\(N_e\\))\nLike all other lineages, the human lineage extends back to the origin\nof life, \\(>3\\) billion years ago.\nIt is therefore informative to consider the \\(N_e\\) of our lineage over different periods\nof our evolution. Here is a relatively recent estimate of divergence\ntimes and \\(N_e\\)’s for the great apes,\nincluding humans, from Prado-Martinez et\nal. (2013):\n\n\n\nFigure 2: Population splits and effective population sizes\n(\\(N_e\\)) during great ape evolution.\nSplit times (dark brown) and divergence times (light brown) are plotted\nas a function of divergence (d) on the bottom and time on top. Time is\nestimated using a single mutation rate (μ) of \\(1 \\times 10^{-9}\\) \\(mut\\) \\(bp^{−1}\\) \\(year^{−1}\\). The ancestral and current\neffective population sizes are also estimated using this mutation rate.\nThe results from several methods used to estimate \\(N_e\\) (COALHMM, ILS COALHMM, PSMC and ABC)\nare coloured in orange, purple, blue and green, respectively. The\nchimpanzee split times are estimated using the ABC method. The x axis is\nrescaled for divergences larger than \\(2\n\\times 10^{-3}\\) to provide more resolution in recent splits. All\nthe values used in this figure can be found in Supplementary Table 5.\nThe terminal \\(N_e\\) correspond to the\neffective population size after the last split event. Figure and caption\nfrom Prado-Martinez et\nal. (2013).\n\n\n\nAnd here is a recent estimate of \\(N_e\\) for modern H. sapiens from\nabout 200,000 years ago to the present, from Schiffels and Durbin\n(2014).\n\n\n\nFigure 3: Estimates of \\(N_e\\) for\nhumans. The blue colors are African populations, and the other colors\nare non-African populations. The top panel used different data that\nincluded Native American haplotypes, with better resolution for older\npopulation changes than the bottom panel, but less resolution for more\nrecent changes. Figure from Schiffels and Durbin\n(2014).\n\n\n\nThus, based on estimated \\(N_e\\),\nand assuming a fixed DFE, the rate of evolution should have been\nrelatively high when the great apes diverged from other apes about 20\nmillion years ago, slowed when hominins diverged from chimpanzees at the\nend of the Miocene, slowed again, perhaps with the appearance of\nHomo around the beginning of the Pleistocene, and then very\nrecently accelerated in the late Pleistocene and Holocene.\nThe distribution of\nfitness effects (DFE)\nThe DFE of new mutations is not necessarily fixed, however, but could\nhave changed at different points in human evolution. Generally,\ndeleterious mutations are expected to greatly outnumber beneficial ones\nbecause there are more ways to break things than improve them,\nespecially if a population is well-adapted to its environmental niche.\nThat means the DFE will generally be shifted toward negative values.\nIf a population is moving into a new environmental niche, however\n(point A in Figure 4), the fitness of existing alleles might\ndrop, pushing the distribution of fitness effects of new alleles to\nhigher, often positive, values (distributions on the left). As the\npopulation adapts and reaches a fitness maximum (point B), new mutations\nwill again almost always be detrimental (distributions on the\nright).\n\n\n\nFigure 4: Fitness landscapes and the distribution of fitness effects.\nHypothetical distributions of fitness effects for populations of\ndifferent sizes at different positions on a simple fitness landscape. A\npopulation far from the optimum (A) has a certain proportion of\nmutations that confer increases in fitness. However, a population at the\nhypothetical optimum of the landscape (B) cannot increase it fitness, so\nall mutations are deleterious. In both cases, the proportion of\nmutations that fall into different categories (Box 2, main text) changes\ndepending on the effective population size. Note that, for simplicity,\nwe have drawn a fitness landscape that varies along a single dimension,\nbut the distributions we have drawn are more similar to those that would\ncome from higher-dimensional fitness landscapes. Furthermore, it is\nunlikely that any natural population sits at the precise optimum of any\nfitness landscape. The selection coefficients are shown on natural\nscales, not log-transformed scales. Figure and caption from Lanfear et\nal. 2014.\n\n\n\nThere are good reasons to believe that our lineage has undergone\nmultiple niche changes, e.g., when hominins diverged from the chimpanzee\nlineage toward the end of the Miocene, when Homo diverged from\nother hominins around the beginning of the Pleistocene, when modern\nhumans left Africa c. 50-100,000 years ago, and when transitioning to\nagriculture at the end of the Pleistocene and beginning of the Holocene\nc. 10,000 years ago. During these changes, the DFE could have been\nshifted to higher values, resulting in a higher rate of evolution\ndespite small \\(N_e\\).\nSeveral other mechanisms have been proposed that might either change\nor stabilize the DFE in different species with different \\(N_e\\) and different levels of organism\ncomplexity (Figure 5).\n\n\n\nFigure 5: Overview of the main predictions of five theoretical\nmodels regarding DFE differences between two species. Here, E[s] is the\naverage selection coefficient of a new mutation, and \\(N_e\\) is the effective population size.\nFigure and caption from Huber et al. 2017.\n\n\n\nHuber et\nal. 2017, for example, found that polymorphism data from humans,\nDrosophila, mice, and yeast best supported Fisher’s Geometrical\nModel, which represents phenotypes as points in a multidimensional\nphenotype space, whose dimensionality is termed “complexity.” Fitness is\na decreasing function of the distance from the optimal phenotype.\nBecause mutations in complex organisms are more likely to disrupt\nfunctionality, the average selection coefficient should be more negative\nin complex vs. simple organisms, a prediction supported by their data\n(Figure 6, panel A):\n\n\n\nFigure 6: Empirical support for FGM. (A) Both under the gamma DFE and\nthe Lourenço et al. DFE, estimated average deleteriousness of mutations\nincreases as a function of organismal complexity. (B) The shape\nparameter of the gamma DFE depends on the breadth of gene expression.\nTissue-specific genes have a smaller shape parameter (α) than broadly\nexpressed genes, supporting FGM. This pattern is consistent across\noverall expression levels. (C and D) By fitting the DFE of Lourenço et\nal., we can model slightly beneficial mutations in the DFE (green) that\nare thought to compensate for fixed deleterious mutations in species\nwith small population size. We find support for a larger proportion of\nslightly beneficial mutations in the DFE of (C) humans than in (D)\nDrosophila. Figure and caption from Huber et al. 2017.\n\n\n\nRacimo and\nSchraiber (2014) criticize empirical studies of DFE that rely on\nfitting the data to a single probability distribution, however, such as\nnormal or gamma. They found, instead, that in humans the DFE (for\ndeleterious mutations only) had a bimodal distribution, with a large\npeak centered on \\(s \\sim 0\\)\n(neutrality), and smaller peak between \\(-10^{-5}\\) and \\(-10^{-4}\\):\n\n\n\nFigure 7: Distribution of fitness effects among YRI polymorphisms in\nthe Complete Genomics dataset, partitioned by the genomic consequence of\nthe mutated site. The right panels show a zoomed-in version of the\ndistributions in the left panels, after removing neutral polymorphisms\nand log-scaling the x-axis. A) DFE obtained from the genome-wide\nmapping. B) Zoomed-in version of panel A. C) DFE obtained from the\nexome-wide mapping. D) Zoomed-in version of panel C. E) DFEs for exonic\nsites (nonsynonymous, synonymous, splice sites) obtained from the\nexome-wide mapping and DFEs for non-exonic sites (intergenic, UTR,\nregulatory) obtained from the genome-wide mapping. F) Zoomed-in version\nof panel E. Consequences were determined using the Ensembl Variant\nEffect Predictor (v.2.5). Codon and degeneracy information was obtained\nfrom snpEff. If more than one consequence existed for a given SNP, that\nSNP was assigned to the most severe of the predicted categories,\nfollowing the VEP’s hierarchy of consequences. NonSyn = nonsynonymous.\nSyn = synonymous. Syn to unpref. codon = synonymous change from a\npreferred to an unpreferred codon. Syn to pref. codon = synonymous\nchange from an unpreferred to a preferred codon. Syn no pref. =\nsynonymous change from an unpreferred codon to a codon that is also\nunpreferred. Splice = splice site. Figure and caption from Racimo and Schraiber\n(2014). https://doi.org/10.1371/journal.pgen.1004697.g002\n\n\n\nRacimo and\nSchraiber (2014) speculate that the absence of mutations with \\(s < -10^{-4}\\) might indicate a cutoff\nbetween weakly deleterious mutations that segregate in human populations\nand highly deleterious mutations that are quickly eliminated by negative\nselection.\nIn summary, the rate of evolution depends on both the DFE and \\(N_e\\), and involves both negative\n(purifying) and positive selection; the DFE likely depends on changes to\nthe environment, organism complexity, and perhaps other other factors\nlike robustness and back mutations. The human DFE is an active area of\nresearch.\nStanding variation and soft\nsweeps\nAn important further consideration is that the stochastic effects of\ndrift on the fate of beneficial alleles apply mainly to new mutations,\nbecause they are initially at very low frequency. Species harbor a large\nnumber of neutral, or nearly neutral, alleles, however, termed standing\nvariation, which, because they are already at high frequency, are much\nless likely to be lost via drift. Moreover, these alleles already exist,\nso there is no waiting time for new beneficial mutations to appear. When\na population moves into a new niche, these formerly neutral alleles can\nbecome either deleterious or beneficial and will respond quickly to\nselection, resulting in a soft sweep (Hermisson and\nPennings 2005).\nFigure 8, from Barrett and Schluter\n2008, illustrates that, for smaller values of \\(\\alpha_b = 2N_es_b\\) (where the \\(b\\) subscript indicates “beneficial”),\nbeneficial standing variants (solid line) have a very high fixation\nprobability relative to new mutations (dashed line):\n\n\n\nFigure 8: The probability of fixation of a single new mutation\n(dashed curve) compared with that of a polymorphic allele that arose in\na single mutational event (solid curve). \\(\\alpha_b = 2N_es_b\\), where \\(N_e\\) is the effective population size and\n\\(s_b\\) is the homozygous fitness\nadvantage. The form of the curve for standing variation in this example\nassumes that \\(N = N_e = 25,000\\), the\ndominance coefficient (h) = 0.5 and that beneficial alleles were\npreviously neutral. \\(\\alpha_b\\) is\nplotted on a logarithmic scale. Figure and caption from Barrett and Schluter\n2008.\n\n\n\nIn a study that used a novel machine learning technique to identify\nhard sweeps (new mutations that go to fixation under positive selection)\nand soft sweeps across the human genome, Schrider and Kern\n(2017) found that the vast majority, over 90%, were soft sweeps. In\naddition, patterns of variation in perhaps half the genome has been\naffected by a nearby sweep:\n\n\n\nFigure 9: The number of windows assigned to each class by S/HIC in\neach population.\n\n\n\nThis is a new technique based on simulated training data. Schrider\nand Kern acknowledge that their results\n\nmay be surprising given the apparently small effective population\nsize and low nucleotide diversity levels in humans. However, if the\nmutational target for the trait to be selected on is fairly large, then\nthe probability of a population harboring a mutation affecting that\ntrait may be appreciable.\n\nVariance in \\(N_e\\) can\nalso be important in determining the relative importance of hard\nvs. soft sweeps (Messer and Petrov\n2013). \\(N_e\\) estimated from\nsequence data can be dominated by short phases during which \\(N_e\\) was small, even though \\(N_e\\) was large for long periods during\nwhich adaptation by positive selection was much more likely.\nIn summary, human adaptation by natural selection was often via soft\nsweeps, in which \\(N_e\\) plays a much\nsmaller role.\nThe\neffects of population structure on the substitution rate\nAnother consideration is that the population genetics models of the\neffects of \\(N_e\\) and \\(s\\) on substitution rates make simplifying\nassumptions, such as random mating (i.e., a lack of population\nstructure), that might easily be violated in real populations. Recent\ntheoretical work has found that population structure can either suppress\nor amplify the effects of selection (e.g., Frean et\nal. 2013).\nPhenotypic evidence for\nhuman evolution\nThe final piece of background information to keep in mind is the\nphysiological, morphological, and behavioral evidence for human\nevolution that has informed the studies of evolutionary biologists from\nCharles Darwin in the 19th century to entire\ndepartments of evolutionary biologists in the 21st.\nPerhaps the two most dramatic phenotypic examples of human-specific\nadaptations are (1) bipedalism, and (2) the substantial increase in\nbrain size in humans relative to hominin ancestors and extant apes, most\nof which occurred since the first appearance of Homo over 2\nmillion years ago (Figure 10):\n\n\n\nFigure 10: The evolution of primate cranial capacity. Each dot is a\nfossil specimen. The x-axis is on a log scale. From Schoenemann\n(2013)\n\n\n\nTwo recent studies of variation in cranial dimensions in apes and\nhumans (Weaver and\nStringer 2015; Schroeder and von\nCramon-Taubadel 2017) found that, whereas great ape cranial\nevolution was largely characterized by strong stabilizing selection, the\ndivergence of Homo from its last common ancestor with\nchimpanzees was explained by strong directional selection.\nThe ability to learn language is a widely accepted cognitive\ndifference between humans and chimpanzees that is probably rooted in\nhuman encephalization, as might be the cognitive abilities underlying\ncumulative culture, which is arguably a uniquely human trait (for a\nreview of the evidence for cumulative culture in humans and other\nanimals, see Dean et\nal. 2013).\nThe key point here is that there is overwhelming phenotypic evidence\nthat human psychology did evolve under positive natural selection.\nDetailed comparisons of other important aspects of human and great\nape phenotypes are available in the Matrix of Comparative\nAnthropogeny (MOCA) of the Center for Academic Training and Research\nin Anthropogeny (CARTA).\nEmpirical\nevidence that links genetic evolution with phenotype evolution\nWe are only at the beginning of very long quest to link phenotypes,\nincluding the evolved behavioral\nand cultural\nphenotypes that are the topic of Casey’s course, to the genome, the\nfocus of the SDSU biologists’ letter. Nevertheless, we know a lot more\ntoday than we knew 10 or even 5 years ago.\nFunctional vs. junk DNA\nGenomes can be divided into two parts: functional DNA, which plays a\nprofound role in the phenotype, and “junk” DNA, which plays no role in\nthe phenotype. Functional DNA comprises protein coding regions and\nnon-coding regulatory regions. We now have a pretty good understanding\nof which DNA sequences code for protein. It is much more difficult,\nhowever, to distinguish regulatory DNA from “junk” DNA, and hence to\ndetermine the fraction of the genome that is functional, and therefore\nsubject to evolution by natural selectione.\nMost attempts to distinguish functional from junk DNA involve\nidentifying sequences that are conserved across species, and are\ntherefore presumed to be under purifying selection (constrained\nsequences) and thus functional, vs. those that are not conserved across\nspecies and so presumably have not been under purifying selection and\nare therefore likely non-functional “junk” (c.f., ENCODE).\nA recent estimate (Rands et\nal. 2014) is that 8.2% (7.1–9.2%) of the human genome is functional\n(constrained). Protein coding sequences comprise about 1% of the genome,\nand are highly conserved across the mammals (Figure 11,\nred). Non-coding regulatory sequences have much higher rates of turnover\n(turnover refers to the loss or gain of purifying selection at a\nparticular locus of the genome caused by changes in the physical or\ngenetic environment, or mutations at the locus itself, that switch it\nfrom being functional to being non-functional or vice versa). See Figure\n11:\n\n\n\nFigure 11: Schematic summary of the fraction of constrained sequence\nthat has been retained (saturated colours) or turned over (pastel\ncolours) in the human lineage over time (X-axis, divergence time) and\nhow it has been distributed across various categories of functional\nelement. In addition to showing the reduced quantity of preserved\nconstrained sequence with increasing divergence, we infer the reciprocal\nquantity of sequence that is assumed to have been gained over human\nlineage evolution. Figure and caption from Rands et\nal. 2014.\n\n\n\nBecause \\(>90\\%\\) of the human\ngenome appears to be non-functional, sequence variation in this portion\nshould closely follow the neutral model. Our focus, then, is on the\nevolution of the \\(\\sim 8\\%\\) of the\ngenome that is functional.\nConserved traits\nPerhaps the most important point is that evolutionary anthropologists\nare keenly interested in the adaptations we share with\nprimates, mammals, vertebrates, and so on. Examples include lacation, the immune system, vision and bitter taste\nand other plant toxin defense mechanisms that are central to Casey’s research. The major\nfeatures of these adaptations evolved long before the appearance of\nHomo, with its small \\(N_e\\),\nbut would need to have been maintained by purifying selection during the\nevolution of Homo.\nMuch of the genetic basis of adaptations we share with other mammals\nand primates almost certainly lies in the \\(\\sim 2\\%\\) of constrained sequence we share\nwith all other mammals and the \\(\\sim\n6-7\\%\\) we share with other primates (out of 8.2% total; Figure\n11).\nMoreover, many complex adaptations have evolved features that enable\nthem to adjust ontogenetically to local environmental conditions, an\nability that I and many others have framed\nin strategic terms. Examples include the immune system and induction of\nxenobiotic metabolizing enzymes. Humans would not be doing too\nhorribly in a wide range of environments even if there were no\nhuman-specific adaptations.\nHuman-specific\nselection in regulatory sequences\nDespite the low \\(N_e\\) in\nHomo, there is increasing genetic evidence for positive natural\nselection in the human lineage since our divergence from chimpanzees.\nGiven that protein-coding sequences are highly conserved across the\nmammals, and that most functional DNA comprises regulatory sequences,\nmost human-specific adaptations should be grounded in changes to\nregulatory sequences.\nHuman accelerated regions (HARs) are short, evolutionarily conserved\nDNA sequences that have acquired significantly more DNA substitutions\nthan expected in the human lineage since divergence from chimpanzees.\nHARs are often, but not always, the product of positive natural\nselection (other mechanisms include relaxation of constraint, which\nallows a region to acquire more mutations than it would under purifying\nselection, and GC-biased gene conversion; Franchini and Pollard\n2017).\nFranchini and\nPollard 2017 summarized multiple studies that attempted to discover\nHARs, which differed in the number of species considered to determine\n“conserved” (which ranged from a few primate species to multiple\nprimate, mammalian and vertebrate species), sequence filtering criteria\n(e.g., including or excluding coding sequences), and statistical tests\nfor acceleration. Although each study identified a large number of HARs,\nthere was only limited overlap in the identified regions. See Figure\n12:\n\n\n\nFigure 12: Identification of human accelerated elements. Top: the four\ndifferent approaches used to identify human accelerated regions. Some\nkey differences include (i) the conserved elements used as candidates to\nidentify HARs (which depend on multiple sequence alignments, methods to\ndetect conservation, and whether human was masked in the alignments),\n(ii) bioinformatics filters that aim to restrict to non-coding elements\nand/or remove assembly or alignment artifacts, and (iii) tests used to\ndetect acceleration. Bottom: overlap of the different datasets of human\naccelerated regions. Abbreviations: ANC accelerated conserved non-coding\nsequences [20]; HACNS human accelerated conserved non-coding sequences\n[23]; HTBE human terminal branch elements [21]. HARs include the\noriginal HARs [19] and the second generation HARs or 2xHARs [100].\nFigure and caption from Franchini and Pollard\n2017.\n\n\n\nMost studies of HARs used only sequence data. Some recent studies,\nthough, also incorporated expression data. Gittelman et al. 2015,\nfor example, used maps of DNase I\nhypersensitive sites (DHSs) from ENCODE and the Roadmap Epigenomics\nProjects. DHSs are regions of chromatin that serve\nas markers of regulatory DNA. Of 2,093,197 DHS loci, 113,577 exhibited\nsignificant constraint across the primates. DHSs active in fetal cell\ntypes, especially fetal brain cells, showed the highest levels of\nconservation in non-human primates. Of the conserved loci, 524 were\naccelerated in human evolution (haDHSs), evolving at approximately four\ntimes the neutral rate in the human lineage, mostly but not exclusively\nunder positive selection, while other primate lineages evolved at less\nthan half of the neutral rate. Gittelman et al. found that haDHSs tend\nto target developmentally and neuronally important genes relative to\nconserved DHSs, which themselves are already highly enriched for these\ncategories.\nIn another review of several previous studies of HARs, Levchenko et al. 2018 noted\nthat of the 3500 candidates identified so far, most are in non-coding\nregions, estimates of the fraction under positive selection range from\n15%-85%, many are active in the brain, consistent with this major\nphenotypic difference between humans and chimpanzees, and about 7-8% are\nnot shared with Neanderthals or Denisovans, consistent with the time of\ntheir divergence from the modern human lineage.\nRecent human evolution\nHuman \\(N_e\\) started a dramatic\nincrease some 40-50,000 years ago as humans entered multiple new\nenvironments. Hawks et\nal. (2007) predicted and found that “selection has accelerated\ngreatly during the last 40,000 years.” Co-authors Cochran and Harpending\nfollowed up with a book, The 10,000 year explosion: How civilization\naccelerated human evolution (you can find my\nreview of it here). Fan et al. (2016)\nreview many examples of recent, population-specific human adaptations to\nlocal environments, such as the arctic, tropical rainforests, and high\naltitude:\n\n\n\nFigure 13: Examples of human local adaptations, each labeled by\nthe phenotype and/or selection pressure, and the genetic loci under\nselection. Figure and caption from Fan et al. (2016).\n\n\n\nWrap up\nEvolutionary anthropologists and human behavioral ecologists\ninvestigate the relationships between human environments and humans’\nintricate molecular, cellular, anatomical, and behavioral phenotypes,\nmost of which we inherited from primate, mammalian and earlier ancestors\nthat evolved long before our lineage experienced a low \\(N_e\\). The extent to which a small \\(N_e\\) limited human-specific evolution is\nstill under investigation. It certainly didn’t reduce it to zero, and\nthere are many other factors that might have accelerated it. There is\nabundant phenotypic evidence for human-specific adaptations, after all,\nand there is increasing genetic evidence that positive selection played\nan important role in our evolutionary history, especially selection on\nstanding variation, which is not limited by low \\(N_e\\).\nReading through the papers I discussed here, I was encouraged that\nfolks studying human genetic variation seemed interested in, not hostile\nto, the many adaptationist hypotheses put forward by evolutionary\nanthropologists and behavioral ecologists. Charlesworth and\nCharlesworth, for instance, highlight the importance of kin\nselection and evolutionary game theory, two theoretical foundations of\nbehavioral ecology. O’Bleness et al. (2012)\ngive a nod to Lieberman’s endurance running hypothesizes and many other\nphenotypic comparisons of humans and non-human primates.\nSo what’s up with the SDSU biologists? The debates over adaptationism\nand neutralism vs. selectionism have all the hallmarks of a sectarian\nconflict. Contrary to our rhetoric, academics are among the most\nethnocentric of an infamously ethnocentric species. Perhaps — dare I say\nit? — ethnocentrism is part of human nature. If so, it’s true of\nus all.\nBut, among the primates, our species also evolved a unique ability\nfor group alliances. The\nSDSU biologists are doing cool stuff.\nCasey is doing cool\nstuff. Knowing Casey as I do, he doesn’t need my advice, but I’ll\ngive it anyway: I doubt the SDSU biologists speak with one voice. There\nis as much diversity of opinion within groups as there is between them,\nmaybe more. You are one of the very few SDSU social scientists who has a\nprofessional interest in what the SDSU biologists are doing. I suspect\nmany of them recognize that.\n2018/7/15: minor edits to this post to improve clarity.\n\nThere is debate about the human\nmutation rate. See Scally\nand Durbin 2012 and Scally 2016.↩︎\n",
    "preview": "posts/2018-03-03-while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary/../../images/pangloss.png",
    "last_modified": "2022-06-17T06:16:42-07:00",
    "input_file": "while-it-may-be-true-that-evolutionary-anthropologists-consider-themselves-scientists-and-use-the-terms-evolution-and-evolutionary.knit.md",
    "preview_width": 660,
    "preview_height": 285
  },
  {
    "path": "posts/2018-01-16-our-statistics-dont-suck-our-theories-do/",
    "title": "Our statistics don't suck, our theories do",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2018-01-16",
    "categories": [],
    "contents": "\n\nEvery genuine test of a theory is an attempt to falsify it, or to refute it. Karl Popper, 1963.\n\n\n\n\nMost social scientists test a substantive hypothesis, which we will call \\(H_1\\), by instead testing a null hypothesis, \\(H_0\\), which is usually something like “the mean of X equals 0” or “the correlation of X and Y equals 0.” If the probability of the data1 under \\(H_0\\) is low (e.g., \\(p < 0.05\\)), the researcher rejects \\(H_0\\), which is taken as evidence in favor of \\(H_1\\).\nThis approach, termed null hypothesis significance testing (NHST), has been subjected to scathing criticism. In the social sciences, for example, we know \\(H_0\\) is false before we collect a single data point — the mean of X is never exactly 0, the correlation of two variables is never exactly 0, and so forth. Hence, the rejection of such a null, which Cohen (1994) termed a “nil” hypothesis, just depends on sample size:\n\nThus far, I have been considering \\(H_0\\)’s in their most general sense–as propositions about the state of affairs in a population, more particularly, as some specified value of a population parameter. Thus, “the population mean difference is 4” may be an \\(H_0\\), as may be “the proportion of males in this population is .75” and “the correlation in this population is .20.” But as almost universally used, the null in \\(H_0\\) is taken to mean nil, zero….My work in power analysis led me to realize that the nil hypothesis is always false.\n\nFurther, the rejection of a nil \\(H_0\\) (which we already know to be false) is exceedingly weak evidence in favor of our pet hypothesis, \\(H_1\\). First, rejection of a nil is also probably consistent with many other hypotheses. Worse, if \\(H_1\\) predicts, e.g., the mean of X is greater than 0, the chance of being right is 50-50. Even with this incredibly weak standard, lots of social science studies don’t replicate.\nWe social scientists might be forgiven for concluding that NHST is deeply flawed. We would be wrong. The problem, instead, is our weak theories.\nIn the natural sciences, such as physics and chemistry, theories typically predict numerical values for various parameters. Sometimes the predicted value might be 0. Under Einstein’s theory of special relativity, for instance, the speed of light in the direction of the earth’s motion will not differ from the speed of light perpendicular to the earth’s motion. Other times, the predicted values are different from 0. Einstein’s theory of general relativity, for example, predicts that the sun will bend star light by a specific (non-zero) amount. In either case, the predicted value is a substantive \\(H_0\\) (not a nil) that can be meaningfully tested with NHST.\nUnlike rejecting a nil, which social scientists usually take as evidence supporting their theory, rejecting a substantive \\(H_0\\) rejects the theory! If the difference of the speed of light in one direction vs. another is greater than 0 \\((p < 3 \\times 10^{-7})\\), then special relativity is wrong!2 If the sun bends light more or less than the predicted amount \\((p < 3 \\times 10^{-7})\\) then general relativity is wrong!3 In the natural sciences, NHST is often a powerful tool to challenge theories, not support them (see Figure 1).\n\n\n\nFigure 1: Testing nils vs. substantive nulls. Black vertical line: null. Red vertical line: mean of data. A: Rejecting the nil that the mean = 0 is taken as support for a theory predicting that the mean has some value > 0. B: Rejecting the substantive null that the mean = 0 rejects the theory. C: Failing to reject the substantive null that the mean = 3 fails to reject the theory. All data simulated.\n\n\n\nFalsifying a theory is an essential step toward developing a better theory.\nThis critical difference in the use of NHST in the natural vs. social sciences was pointed out by Paul Meehl in 1967:\n\nBecause physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching 1⁄2 of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by “success” is very weak, and becomes weaker with increased precision. “Statistical significance” plays a logical role in psychology precisely the reverse of its role in physics.\n\nMost theories in the social sciences, including my own, are so weak that they can only predict that a value will be positive or negative, nothing more (e.g., Figure 1A). Such vague predictions make it harder to falsify these theories, therefore impeding development of better theories.\nYet it is certainly possible to develop social science theories that predict specific values, and thus expose themselves to meaningful challenges with NHST.\nMy graduate student Aaron Lightner and I, for example, recently missed an opportunity to do better science by following standard practice and testing a nil, when we now realize we should have also tested a substantive null. We conducted a classic framing effect study in which we predicted that participants would make different monetary offers in an ultimatum game if it was framed as a currency exchange than if it was “unframed.” We set up our statistical test in the standard social science way. \\(H_0\\) was a “nil”: no difference in mean offers in the framed vs. unframed condition.\nWe found a huge effect size (\\(d\\sim2\\); see Figure ??), and our p-value was very small \\((p = 1.8 \\times 10^{-31})\\). We therefore rejected the nil and concluded that there was a difference in mean offers. Publication! \nWe could have put our theory to a much more severe test, however, by proposing a substantive \\(H_0\\). In our study, participants self-reported what they thought was a fair offer, and they also made actual monetary offers. Our scientific (not statistical) hypothesis was that actual offers would match self-reported fair offers. These, in turn, would match a cultural norm for offers in currency exchange that differed from offers usually seen in the ultimatum game. If a participant reported that 3% was a fair offer, for instance, then he or she should have offered 3%.\nAs our substantive \\(H_0\\), we therefore should have proposed this: There will be no difference between actual offers and self-reported fair offers. If we had, we would have found that the probability of our data under such an \\(H_0\\) was very small4, leading us to reject \\(H_0\\), and therefore to reject our theory. Specifically, our data showed that although many actual offers were exactly, or very close to, self-reported fair offers (dots that fall on, or close to, the diagonal line), as our theory predicted, many other actual offers differed substantially from self-reported fair offers (dots that are far from the diagonal line), contrary to our prediction:\nSelf-reported fair offers vs. actual offers in a framed ultimatum game experiment. There were two framed conditions: banker and customer. From Lightner et al. 2017: http://dx.doi.org/10.1098/rsos.170543We don’t even need to compute a p-value to see that we can reject our substantive \\(H_0\\). Our scientific theory is wrong.\nRejecting a theory is cause for excitement, however, not despair. If a physicist did an experiment that convincingly rejected the null that the speed of light in a vacuum is the same in all inertial frames of reference, she would set off an explosion of research in her discipline and end up with a Nobel Prize.\nMake no mistake: Aaron and I were delighted that our study confirmed our prediction that offers in the framed conditions would deviate substantially from the unframed condition (clustering near 0 and 1, instead of near 0.50 as in the standard ultimatum game). But, contrary to our prediction, it also found that many participants said that it would be fair to offer X, but then made a very different offer. Why? Great question for a new study!\nIn the social sciences we need more “Holy Hand Grenade” theories — theories that do not simply make the very weak prediction that, e.g., \\(\\bar{X}>0\\), which barely deserves to be called a prediction, but instead predict, e.g., that the number shall be three, no more, no less. Such theories can be subjected to severe tests (i.e., potentially falsified) using NHST. Rejecting a substantive \\(H_0 = 3\\), or failing to reject it, would then represent real scientific progress, not the flip of a coin.\nMore precisely, for some test statistic \\(z\\), the p-value is the probability of finding a value of \\(z\\) equal to or more extreme than that observed, under the assumption that \\(H_0\\) is true.↩︎\nIn a recent example, the OPERA experiment mistakenly observed neutrinos traveling faster than light.↩︎\nPhysicists usually demand much smaller p-values than social scientists, e.g., the \\(5\\sigma\\) rule.↩︎\nTo compute confidence intervals, we would need to estimate the precision of our measurements, e.g., how closely do self-reported fair offers correspond to participants’ actual beliefs about fair offers? For a detailed example of parameter estimation for a simple physics problem, see Aguilar et al. 2015.\n\n↩︎\n",
    "preview": "posts/2018-01-16-our-statistics-dont-suck-our-theories-do/../../images/F11.large.jpg",
    "last_modified": "2021-04-25T11:25:38-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-01-01-data-dredging-is-the-dionysian-soul-of-science/",
    "title": "Data dredging is the Dionysian soul of science",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2018-01-01",
    "categories": [],
    "contents": "\nGood and Evil Angels. Plate 4 of The Marriage of Heaven and Hell. From The William Blake Archive: http://www.blakearchive.orgYin and Yang. Apollo and Dionysus. Heaven and Hell. Id and Superego. Reason and Emotion. Spock and McCoy. Many intellectual and moral frameworks are structured around two, often opposing, elements. If one element gains the upper hand over the other, beware. William Blake, for example, in The Marriage of Heaven and Hell, took aim at the imbalance he saw in Christian theology. God, Heaven, and the Good were the “passive that obeys reason,” our Apollonian side. Satan, Hell and Evil were the “active springing from Energy,” wrongly suppressed by the Church, the Dionysian excess that “leads to the palace of wisdom.”\nData analysis in the social sciences has two forms, one “good” that is highly developed and has many rules that supposedly will lead us to truth, and one “bad” that lives in the shadows, has few if any rules, and is frequently, but wrongly, vilified. This imbalance is crippling the social sciences.\nAngelic confirmation\nScience often proceeds in roughly three parts: notice a pattern in nature, form a hypothesis about it, and then test the hypothesis by measuring nature. The challenge in testing our hypotheses is that the world is noisy, and it can be very difficult to distinguish the signal — the patterns in our data that are stable from sample to sample — from the noise — the patterns in our data that change randomly from sample to sample.\nStatistics, as a discipline, has focused almost exclusively on the challenges of the third part, hypothesis testing, termed confirmatory data analysis (CDA). CDA is (and must be) the epitome of obedience: obedience to reason, to logic, to complex rules and to a meticulous, pre-specified plan that is focused on answering perhaps a single question. It is Apollonian, governed by the “good” angels of Blake’s Heaven.\nAlthough CDA is one of science’s crown jewels, it is not designed to use data to discover new things about the world, i.e., unexpected patterns in our current sample of measurements that are likely to appear in future samples of measurements. CDA can reliably distinguish signals from noise in a sample of data only if the putative signal is specified independently of those data. If, instead, a researcher notices a pattern in her data, and then tries to use CDA on those same data to determine if the pattern is a signal or noise, she will very likely be mislead.\nIn a largely futile attempt to insure that hypotheses are independent of the data used to test them, statistics articles and textbooks disparage the discovery of new patterns in data by referring to it with derogatory terms such as dredging, fishing, p-hacking, snooping, cherry-picking, HARKing (Hypothesizing After the Results are Known), data torturing, and the sardonic researcher degrees of freedom.\nNotice something? The first, and arguably most important step in a scientific investigation is to identify an interesting pattern in nature, yet we are taught that it is wrong use our data to look for those interesting patterns.\nThat, my friends, is insane.\nThe Devil’s exploration\nJohn Tukey, one of the most eminent statisticians of the 20th century, recognized that his discipline had put too much emphasis on CDA and too little on exploratory data analysis (EDA), his approving term for data dredging, which he defined thus:\nIt is an attitude, AND\nA flexibility, AND\nSome graph paper (or transparencies, or both).\n\nNo catalog of techniques can convey a willingness to look for what can be seen, whether or not anticipated. Yet this is at the heart of exploratory data analysis. The graph paper—and tansparencies—are there, not as a technique, but rather as a recognition that the picture-examining eye is the best finder we have of the wholly unanticipated.\n\nOr, as he wrote in his classic text on EDA:\n\nThe greatest value of a picture is when it forces us to notice what we never expected to see.\n\nFor Tukey, data analysis was a critical tool for discovery, not only confirmation. As he put it, “Finding the question is often more important than finding the answer.” And “The most important maxim for data analysis to heed, and one which many statisticians seem to have shunned, is this: ‘Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.’”1\nI want to draw out what I see as the radical implications of some of Tukey’s mains points for norms in the social sciences.\nThe world, especially the world of human cognition and behavior, is far more complex than any of us can imagine. To have any hope of understanding it, to discover the right questions, we have no choice but to collect and explore high quality data. Although running small pilot studies is tempting because they take little time and few resources, they can be worse than useless. The precision of our estimates goes as the square root of sample size. EDA on small, noisy data sets will only lead us down blind allies. Alternatively, because we social scientists get credit for confirmation, and exploration is actively discouraged, we disguise our shameful explorations as confirmations, all dressed up with stars of significance. And then those “effects” don’t replicate\nThe solution is obvious: we must put at least as much effort into exploration and discovery as we put into confirmation, perhaps more. We will need to collect and explore large sets of data using the best measures available. If those measures do not exist, we will need to develop them. It will take time. It will take money.\nBut let’s face it: discovery is the fun part of science. EDA draws on the energy, instinct, and rebelliousness of Blake’s Devil and Nietzsche’s Dionysus, that heady mix of intuition, inspiration, luck, analysis, and willingness to throw received wisdom out the door that attracted most of us to science in the first place.\nThe marriage of CDA and EDA\nBlake, Nietzsche, and perhaps all great artists and thinkers recognize that there must be a marriage of Heaven and Hell, that neither the Dionysian nor the Apollonian should prevail over the other. Tukey understood well that “Neither exploratory nor confirmatory is sufficient alone. To try to replace either by the other is madness. We need them both.”\nMadness it may be, but without institutional carrots or sticks, EDA will remain in the shadows, a pervasive yet unacknowledged practice that undermines rather than strengthens science.\nOne carrot would be an article type devoted to exploratory research. It might be worthwhile, though, to wield a stick.\nTukey argues that “to implement the very confirmatory paradigm properly, we need to do a lot of exploratory work.” The reason is, there is “no real alternative, in most truly confirmatory studies, to having a single main question—in which a question is specified by ALL of design, collection, monitoring, AND ANALYSIS” (caps in the original).\n\nAnswering just one question with statistical test requires decisions about, e.g., the sample population, sample size (which is based on estimated effect sizes and power), which control variables to include, choice of instruments to measure the variables, which model to fit and/or test to perform, whether and how to transform certain variables (e.g., log or square root transform), and whether to include interactions and which ones. To believe the standard textbooks, we can do all that with a single sample of data while at the same time avoiding the temptation to use any of these researcher degrees of freedom to p-hack.\nHah!\nIf the replication crisis has taught us anything, it is that our statistical tests are surprisingly fragile: small modifications to our procedures can have a large influence on our results. It must therefore become a basic norm in much of science that a confirmatory study – especially one reporting p-values – must preregister “ALL of design, collection, monitoring, AND ANALYSIS.” Everything. In detail.\nA good confirmatory study, then, is completely specified. Running it should be like turning a crank. As Tukey said (caps in original):\n\nWhatever those who have tried to teach it may feel, confirmatory data analysis, especially as sanctification, is a routine relatively easy to teach and, hence,\n\n\nA ROUTINE EASY TO COMPUTERIZE.\n\nThe standard I’m personally aiming for (but have not quite yet achieved) is to preregister our R code.\nIt will be impossible to achieve this ideal without EDA — without first looking at data to evaluate and optimize all the decisions necessary to run a high quality confirmatory study. The stick I envision is that every confirmatory study would be required to have, at a minimum, two samples and two analyses. The first sample would be for EDA, the second for CDA. Every paper reporting results of a confirmatory study must also report the preceding EDA that justified each study design decision. Because the EDA would include estimates of effect sizes, each paper would contain an attempted replication of it’s main result(s).\nIn some cases, it will be possible to divide a single sample in two, and first perform EDA on one portion, and then CDA on the other. In other cases, it will be possible to use existing data for the EDA, and new data for the CDA. In many other cases, however, researchers will simply have to collect two (or more) samples. Requiring that every paper include an EDA on one sample and a subsequent CDA on a separate sample could cut researchers’ publication productivity in half. It could easily more than double their scientific productivity, however, their publication of results that will replicate.\nNotes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTukey was not the first to recognize the importance of exploring data, nor to clearly distinguish exploration from confirmation. De Groot made these points in 1956, for example, and even then he noted they were not new. Statistician Andrew Gelman recently raised the issue on his blog. Unlike others, however, Tukey devoted a chunk of his career to developing and promoting EDA. Much of his writing on the topic has an aphoristic flavor, which reminded me of Blake’s Proverbs of Hell. I recommend you read Tukey (1980); it’s short, with no math.↩︎\n",
    "preview": "posts/2018-01-01-data-dredging-is-the-dionysian-soul-of-science/../../images/BlakeGoodAndEvilAngels.jpg",
    "last_modified": "2021-04-25T11:26:47-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-05-academic-success-is-either-a-crapshoot-or-a-scam/",
    "title": "Academic success is either a crapshoot or a scam",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2017-12-05",
    "categories": [],
    "contents": "\nNature just published five brief commentaries by statisticians on the reproducibility crisis:\n\nAs debate rumbles on about how and how much poor statistics is to blame for poor reproducibility, Nature asked influential statisticians to recommend one change to improve science. The common theme? The problem is not our maths, but ourselves.\n\nThe problem is ourselves, but it has little to do with: a failure to adjust for human cognition (Leek), relying on statistical “significance” (McShane and Gelman), failure to appreciate false positive risk (Colquhoun), or not sharing analysis and results (Nuijten) (although addressing all these things would be good). Goodman, who fingers academic norms, comes closest, but doesn’t identify the real culprit.\nThe problem, in a nut shell, is that empirical researchers have placed the fates of their careers in the hands of nature instead of themselves.\nLet me explain.\nAcademic success for empirical researchers is largely determined by a count of one’s publications, and the prestige of the journals in which those publications appear (and the grants that flow from these). Prestigious journals, in turn, typically only publish papers that they deem to be reporting important new facts about the world.\nIn my field of anthropology, the minimum acceptable number of pubs per year for a researcher with aspirations for tenure and promotion is about three. This means that, each year, I must discover three important new things about the world.\nIs that realistic?\nResearch outcomes are stochastic – if we knew with 100% certainty what the outcome would be, why research it? The whole point of research is learn something new. When researchers begin a study they have some level of uncertainty – perhaps great, perhaps small – about the outcome. We all hope for a sexy outcome, but we all know that we might not get it.\nLet’s say I choose to run 3 studies that each has a 50% chance of getting a sexy result. If I run 3 great studies, mother nature will reward me with 3 sexy results only 12.5% of the time. I would have to run 9 studies to have about a 90% chance that at least 3 would be sexy enough to publish in a prestigious journal.\nI do not have the time or money to run 9 new studies every year.\nI could instead choose to investigate phenomena that are more likely to yield strong positive results. If I choose to investigate phenomena that are 75% likely to yield such results, for instance, I would only have to run about 5 studies (still too many) for mother nature to usually grace me with at least 3 positive results. But then I run the risk that these results will seem obvious, and not sexy enough to publish in prestigious journals.\nTo put things in deliberately provocative terms, empirical social scientists with lots of pubs in prestigious journals are either very lucky, or they are p-hacking.\nI don’t really blame the p-hackers. By tying academic success to high-profile publications, which, in turn, require sexy results, we academic researchers have put our fates in the hands of a fickle mother nature. Academic success is therefore either a crapshoot or, since few of us are willing to subject the success or failure of our careers to the roll of the dice, a scam.\nThe solution is straightforward: Although we have almost no control over the sexiness of our outcomes, we have almost full control over the quality of our studies. We can come up with clever designs that discriminate among popular hypotheses. We can invent new measures of important phenomena, and confirm their validity, reliability and precision. We can run high powered studies with representative samples of key populations. We can use experimental designs. We can pre-register our hypotheses and statistical tests. In short, we need to change the system so academic researchers are rewarded for running high quality studies with these sorts of attributes, regardless of outcome.\nChanging the incentives to reward high quality studies rather than sexy results would have enormously positive effects for science. Researchers will be able to respond to these incentives in ways that improve science while also advancing their careers. Under the current outcome-based incentives, in contrast, researchers often have little choice but to screw science to advance their careers.\nChanging the incentive system won’t be easy. No longer will we be able to easily assess our colleagues based on their number of pubs, weighted by journal impact factors. Instead, we will have to assess them based on the quality of their studies: the importance of the question addressed, the sampling strategy and sample size, the measurements and their ability to discriminate among hypotheses, and the data analysis. All these will have to be recorded in detail, even if there were no sexy results.\nChanging the incentive system might not only help solve the replication crisis, it might also help solve the serials crisis – the recent dramatic increase in the cost of subscribing to scientific journals.\nScientific publishing is an oligopoly. In the social sciences, 5 publishers – Elsevier, Taylor & Francis, Wiley-Blackwell, Springer, and Sage Publications – publish about 70% of all papers:\nPercent of journal articles in different disciplines that are published in the big 5. Figure from https://doi.org/10.1371/journal.pone.0127502These publishers are exploiting their monopolies on journals and journal papers to charge high fees, which are mainly paid by university libraries. Reed-Elsevier’s profit margins, for example, exceed those of Apple, Google, or Amazon:\nReed-Elsevier profits. Left: entire business. Right: scientific, technical and medical division. Figure from: https://doi.org/10.1371/journal.pone.0127502.Student tuition, grant, and endowment dollars are being funneled to highly profitable corporations that add only questionable value to the science they publish.\nNature, a big money maker for Springer (one of the scientific publishing oligopolists), tapped 5 statisticians for comment because it is worried about the replication crisis, and with good reason. Nature, as we all know, is the king of kingmakers in science because it only publishes the sexiest of sexy results. Social scientists, myself included, crave a publication in Nature, which can make one’s scientific career. But if those results are often hacked and cannot be replicated, Nature’s status will plummet, and along with the profits it generates for Springer. Yet Nature and the statisticians seem completely oblivious to the irony that it is the prestige of publishing sexy results in high profile journals like Nature that is the central cause of the replication crisis.\nUltimately, though, we academic researchers are responsible for both the replication and serials crises because we created (or bought into) a system that rewards sexy results over quality measurements of the world.\n\n\n\n",
    "preview": "posts/2017-12-05-academic-success-is-either-a-crapshoot-or-a-scam/../../images/journal.pone.0127502.g004.PNG",
    "last_modified": "2021-04-25T10:54:14-07:00",
    "input_file": {},
    "preview_width": 4400,
    "preview_height": 2814
  },
  {
    "path": "posts/2017-11-15-pca-new-coordinate-system-same-data/",
    "title": "PCA: new coordinate system, same data",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2017-11-15",
    "categories": [],
    "contents": "\nThere are many ways to teach Principal Component Analysis (PCA). This way is mine.\nThe first constellation I learned to recognize was the Big Dipper. In the evening it’s in one part of the sky, and in the early morning, another, but it’s still the Big Dipper. Same thing if I look at it while I slowly spin around. To be more specific, in each of the 3 figures below, the stars have different \\(x\\) and \\(y\\) coordinates, yet it is easy to recognize them as the same stars:\nBig DipperThe point is: we recognize the Big Dipper, not by the specific location of the stars in the sky, but by the location of each star relative to the others.\nThe first and most important step in understanding PCA is to think about your data in the same way that you think about constellations: it’s the relationships between your data points, not their individual values, that matters.\nIn a scientific study, we typically measure multiple values on each person (or population, or whatever), e.g., age, height, weight, sex, and so forth. The mental frame shift to make is to think about the collection of multiple values on a single person as a single data point — a single “star” in the sky — and all the data points as a constellation of stars in the sky. If we had two measurements per person – e.g., height and weight – then each person (each data point) has two coordinates; if we had three measurements per person – e.g., height, weight, and age – then each person (each data point) has three coordinates, and so forth.\nIn general, you can think about each “unit” in the data – person or observation or “row” – as one point in an \\(N\\)-dimensional Euclidean space, where \\(N\\) is the number of variables that you have measured. Viewed this way, the data has a “structure” determined by the relationships of each observations to the others that will be preserved even if the coordinate system is changed.\nHere is a concrete example with 2 variables per person – height and weight – and thus a 2-dimensional space of points:\n\n\n# Height and weight of !Kung individuals.\n# The !Kung are an ethnic group in\n# southwest Africa.\n# From Howell via McElreath:\nd <- readr::read_delim(\"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\", delim = ';')\nlibrary(ggplot2)\nggplot(d, aes(height, weight)) + \n  geom_point(alpha = 0.5) + \n  scale_x_continuous(limits = c(0,200)) +\n  scale_y_continuous(limits = c(0,70)) +\n  geom_point(x = 0, y = 0, colour = 'red', size = 3) +\n  labs(title = \"!Kung heights and weights\", subtitle = \"Each black dot represents one person.\\nThe red dot indicates the origin of the coordinate system.\")\n\n\n\n\nMost students would (correctly) interpret this plot as depicting the relationship between height and weight.\nThere is another way.\nAlthough each star in the Big Dipper is also described by two variables – an \\(x\\) and \\(y\\) coordinate (or an ascension and declination) – both variables are in the same units (angles). This is an important reason why we can think about the Big Dipper in space without thinking about the \\(x\\) and \\(y\\) values of each star.\nThe first step on our journey is therefore to put our height and weight variables into the same units by standardizing them (subtract the mean from each variable, and then divide it by its standard deviation):\n\n\n# Standardize each variable\nd$zheight <- scale(d$height)[,1]\nd$zweight <- scale(d$weight)[,1]\n# Plot\nggplot(d, aes(zheight, zweight)) + \n  geom_point(alpha = 0.5) +\n  geom_point(x = 0, y = 0, colour = 'red', size = 3) +\n  coord_fixed(xlim = c(-4, 3), ylim = c(-3, 3)) +\n  labs(title = \"Standardized !Kung heights and weights\", subtitle = \"Each black dot represents one person.\\nThe red dot indicates the origin of the coordinate system.\")\n\n\n\n\nMany folks would (correctly) interpret these new variables as transformed versions of the original data. However, I would like you to instead see this as a transformation of the coordinate system: we have translated the origin of the coordinate system to the middle of the data (the red dot), and we have put the \\(x\\) and \\(y\\) axes on the same scale (1 unit of x equals 1 unit of y). The data remain the same.\nThe translation of the origin to the center of the data is useful because positive values on the \\(x\\)-axis now indicate values that are greater than the mean, and negative values now indicate values that are less than the mean. The same goes for the \\(y\\)-axis. The translation of the origin makes it easy to identify individuals whose heights and weights are above or below average. Transforming the coordinate system (not the data!) can help us interpret the data – the new origin has advantages over the original origin.\nPutting the \\(x\\) and \\(y\\) axes on the same scale is useful because we can now more easily think about this 2d space as a uniform height-weight space, or height-weight continuum, independent of individual \\(height\\) and \\(weight\\) values.\nWe are ready for another transformation of the coordinate system: a rotation around the origin:\n\n\nangle <- -1.5 # angle of rotation in radians\n\n# New x and y coordinates after rotation\nd$x <- d$zheight * cos(angle) - d$zweight * sin(angle)\nd$y <- d$zheight * sin(angle) + d$zweight * cos(angle)\n\nggplot(d, aes(x, y)) + \n  geom_point(alpha = 0.5) +\n  geom_point(x = 0, y = 0, colour = 'red', size = 3) +\n  coord_fixed(xlim = c(-4, 3), ylim = c(-3, 3)) +\n  labs(title = \"Rotated !Kung heights and weights\", subtitle = \"Each black dot represents one person.\\nThe red dot indicates the origin of the coordinate system.\")\n\n\n\n\nHere are the pairs of coordinates of our first 6 data points in each of our 3 different coordinate systems (the original, standardized, and rotated coordinate system):\n\nheight\nweight\nzheight\nzweight\nx\ny\n151.76\n47.83\n0.49\n0.83\n0.86\n-0.43\n139.70\n36.49\n0.05\n0.06\n0.06\n-0.05\n136.52\n31.86\n-0.06\n-0.25\n-0.26\n0.04\n156.84\n53.04\n0.67\n1.18\n1.23\n-0.59\n145.42\n41.28\n0.26\n0.38\n0.40\n-0.23\n163.83\n62.99\n0.93\n1.86\n1.92\n-0.79\n\nAlthough the pairs of coordinates are radically different, we easily recognize the same constellation of data in the plots, regardless of coordinate system.\nWe have seen how translating the origin of the coordinate system to the center of the data helps us interpret the data. But how could rotating the coordinate system be helpful?\nIn most studies, we measure stuff because we know that the things we’re studying – people in this case – vary, and it is exactly this variation that we want to understand. What if we rotated the coordinate system so that the variance of the data was maximized along the \\(x\\)-axis? Then, in this rotated coordinate system, folks with large positive values on the \\(x\\)-axis would be maximally “different” from folks with large negative values on the \\(x\\)-axis in “height-weight” space. Differences in \\(y\\)-values would then be less important in distinguishing individuals.\nWe can find the rotation that maximizes variance along the \\(x\\)-axis by trial and error: simply choose different angles, compute the rotation, and then compute the variance or standard deviation along the \\(x\\)-axis. Rinse and repeat unit you find an angle that maximizes the standard deviation. There will be two such angles, each \\(\\pi\\) radians (180 degrees) apart:\n\n\n# Run this code over and over with\n# different values for the angle\n# until sd(d$x) is at a maximum.\nangle <- -0.78 # This angle comes close; -0.78 + pi would also come close\nd$x <- d$zheight * cos(angle) - d$zweight * sin(angle)\nsd(d$x) # We could also use var(d$x)\n\n\n[1] 1.393114\n\nA second way would be to use R’s optim function, which automates the above process:\n\n\n# This function rotates and then\n# computes sd along x.\n\nsd_x <- function(angle) {\n  sd(d$zheight * cos(angle) - d$zweight * sin(angle))\n}\n\n# This function finds the angle that \n# maximizes the above function\nopt <- \n  optim(\n    0, # Starting value of angle\n    sd_x, # The function to minimize\n    method = \"Brent\", # The optim procedure that works best in 1-D\n    lower = -pi,\n    upper = pi,\n    control = list(fnscale = -1) # Maximize instead of minimize\n    )\n\n\n\nAn angle of rotation (in radians) that maximizes sd along x:\nopt$par = -0.7853982\nThe maximized standard deviation:\nopt$value = 1.393134\nLet’s plot our data using that optimal angle of rotation:\n\n\n# one optimal angle in radians;\n# the other would be opt$par + pi\nangle <- opt$par \n\n# New x and y coordinates after rotation\nd$x <- d$zheight * cos(angle) - d$zweight * sin(angle)\nd$y <- d$zheight * sin(angle) + d$zweight * cos(angle)\n\nggplot(d, aes(x, y)) + \n  geom_point(alpha = 0.5) +\n  geom_point(x = 0, y = 0, colour = 'red', size = 3) +\n  coord_fixed(xlim = c(-4, 3), ylim = c(-3, 3)) +\n  labs(title = \"!Kung heights and weights rotated to maximize variance along x-axis\", subtitle = \"Each black dot represents one person.\\nThe red dot indicates the origin of the coordinate system.\")\n\n\n\n\nGuess what? The \\(x\\)-axis is principal component 1 (PC1), and the \\(y\\)-axis is principle component 2 (PC2), as we can confirm by comparing our results to those from R’s prcomp (principal component) function:\n\n\n# Compute PCA using the standard R function\nm <- prcomp(~ zheight + zweight, data = d)\nsummary(m)\n\n\nImportance of components:\n                          PC1     PC2\nStandard deviation     1.3931 0.24326\nProportion of Variance 0.9704 0.02959\nCumulative Proportion  0.9704 1.00000\n\n# Compare the standard deviations above with:\nsd(d$x)\n\n\n[1] 1.393134\n\nsd(d$y)\n\n\n[1] 0.2432649\n\nCompare our x & y values…\nx\ny\n0.9326787\n0.2409332\n0.0788411\n0.0052468\n-0.2244852\n-0.1354080\n1.3134063\n0.3613867\n0.4554072\n0.0890045\n1.9703735\n0.6604768\n… with those from prcomp\nPC1\nPC2\n-0.9326787\n-0.2409332\n-0.0788411\n-0.0052468\n0.2244852\n0.1354080\n-1.3134063\n-0.3613867\n-0.4554072\n-0.0890045\n-1.9703735\n-0.6604768\nThe minus signs are reversed because the axes are rotated 180 degrees, but the variance is still maximized along the x-axis (remember, there are 2 rotations that will maximize the variance).\nIn summary, principal components are simply a new orthogonal (perpendicular) coordinate system for your data, rotated so the variance of your data is maximized along the first axis (PC1); then, rotating around the first axis, the remaining variance is maximized along the second axis (PC2), which is perpendicular to the first; and so forth, until the directions of all axes are specified. Thus, there will be as many principal components as there are dimensions in your data (i.e., number of variables), and the variance will decrease across each successive component across each successive component.\nThere are many uses of this new coordinate system. In our example, 97% of the variance in our data falls along PC1. Thus, we might interpret PC1, which is a combination of height and weight, as something like size. By rotating our coordinate system, we have identified underlying “structure” in our data. For 2-d data like our example, PCA is not that useful. But when our data have many dimensions, PCA and related techniques can find structure that would be difficult or impossible to find without them.\nNote #1: The prcomp and other PCA functions do not find these rotations in the same way we did. Instead, they use methods like singular value decomposition, which you can read about on wikipedia.\nNote #2: You might have heard of rotation after PCA, or terms like varimax rotation. These also seek useful rotations, but are distinct from PCA. You can read more about them, and their relationship to PCA here, here, and here.\nNote #3: There is a great set of alternative explanations of PCA here .\n\n\n\n",
    "preview": "posts/2017-11-15-pca-new-coordinate-system-same-data/../../images/ConstellationBigDipper.png",
    "last_modified": "2021-04-25T10:07:19-07:00",
    "input_file": {},
    "preview_width": 1135,
    "preview_height": 480
  },
  {
    "path": "posts/2017-10-18-put-your-data-in-an-r-package/",
    "title": "Put your data in an R package",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2017-10-18",
    "categories": [],
    "contents": "\nI used to write long R scripts that imported data files, created new variables, reshaped the data, reshaped it again, and spit out results along the way, all in one file.\nThat worked so long as I never wanted to use that data again. But what if I did? Should I just tack on more code for the new analysis, and then even more code for yet more analyses? That approach will litter your environment with objects that are irrelevant for, and might even interfere with, a particular analysis. Should I copy all the files into a new directory and then hack away at the code? Now I have two copies of the data – which one is definitive? Should I just treat the original data files as the data? Now I have to repeat the same initial processing steps every time I want to reuse that data.\nThe solution\nFor each new data set I create a new R data package. This package lives in my library along with ggplot2, dplyr, lme4, and all my other packages, and is accessible in any project or analysis with a simple:\n\n\nlibrary(mydatapackage)\n\n\n\nCreating a data package involves some small costs, but these are far outweighed by the benefits.\nPros\nYour data is cleanly separated from your analyses.\nYour data is easily accessible in any future project.\nPackages have a built-in documentation system so you can easily document all your variables. The documentation for each data frame (or other object) is accessible with ?my_df.\nPackages have a versioning system so you can keep track of new versions of your data package.\nShare your data with students and colleagues simply by sharing the package.\nArchive your data in a public repository simply by uploading the package.\nCons\nCreating a package is a few extra steps.\nEvery change to the data package requires a rebuild step before the changes are available in your analyses.\nIf you forget to rebuild, your analyses will be using the outdated version of your data, something that can be hard to detect.\nIn the early phases of an analysis, you will probably be moving code back and forth from your data package to your analysis until you find the sweet spot between processed data and analyzed data.\nHow to create a data package\nI assume you are using RStudio. Although RStudio can create packages using the GUI, I have gotten obscure errors using that feature. Therefore, do not create a new RStudio project using the GUI. Instead, run this code from the console:\n\n\n# Run these from the R console\n\n# Check that the `usethis` package is installed. If not:\ninstall.packages(\"usethis\")\n\n# Create new package. Directory must not exist. \n# This also creates a new RStudio project.\nusethis::create_package(\"path/to/my/data/package/\")\n\n\n\nOpen your new data package using “Open project…” in RStudio. Then run this code from the console:\n\n\n# Run this code after opening the new package in RStudio\n\n# Set up the data-raw directory and data processing script\n# You can use any name you want for your data\nusethis::use_data_raw(name = 'mydataset')\n\n# This script in the R directory will contain the documentation.\n# You can use any name you want.\nfile.create(\"R/data.R\")\n\n# Initialize git repository (optional)\nusethis::use_git()\n\n\n\nPut your data files into the data-raw folder. Your new package directory should look something like this:\nPackage layoutWrite your data processing code in a data-raw/mydataset.R script. It would look something like this:\n\n\n# data-raw/mydataset.R\n# Data import and processing pipeline\n\nlibrary(readr)\nlibrary(readxl)\n\nmydataset <- read_csv(\"data-raw/pendulum data.csv\")\ndemographics <- read_excel(\"data-raw/Demographics.xlsx\")\n\n# Data cleaning code here...\n# (Do NOT put data analysis code here!)\n\n# This should be the last line.\n# Note that names are unquoted.\n# I like using overwrite = T so everytime I run the script the \n# updated objects are saved, but the default is overwrite = F\nusethis::use_data(mydataset, demographics, overwrite = T)\n\n\n\nThat last line writes your R data frames and other data objects to files in the data directory (not the data-raw directory!). The data directory will be created if it does not exist.\nEdit your DESCRIPTION file as specified in Hadley Wickham’s book on R packages: http://r-pkgs.had.co.nz/description.html.\nYou won’t need Imports or Suggests.\nAt this point you should confirm that you can build and install your package. You should see a Build tab in RStudio. Open it and click Build and Install (older RStudio versions) or Install and Restart (newer RStudio versions), which will build the package and install it in your package library. You should see your package in the Packages tab, and you should have access to the data frame objects in any script where you include: library(mydatapackage).\nDocument your data\nOne of the biggest advantages of creating a data package is that it provides a very convenient system for documenting your data, and for accessing that documentation. Although they are not required, I recommend first installing this utility package:\n\n\ninstall.packages(\"sinew\")\n\n\n\nOpen your currently blank R/data.R file, which is where you will add the documentation using roxygen.\nThen use the makeOxygen function from the sinew package to create skeleton documentation, e.g., for the mydataset data frame:\n\n\nsinew::makeOxygen(mydataset, add_fields = \"source\")\n\n\n\nThis will print out a skeleton that you copy and paste into your currently empty R/data.R file:\n\n\n# This goes in R/data.R\n\n#' @title DATASET_TITLE\n#' @description DATASET_DESCRIPTION\n#' @format A data frame with 1559 rows and 5 variables:\n#' \\describe{\n#'   \\item{\\code{Time (s)}}{double COLUMN_DESCRIPTION}\n#'   \\item{\\code{Acceleration - x (m/s²)}}{double COLUMN_DESCRIPTION}\n#'   \\item{\\code{Acceleration - y (m/s²)}}{double COLUMN_DESCRIPTION}\n#'   \\item{\\code{Acceleration - z (m/s²)}}{double COLUMN_DESCRIPTION}\n#'   \\item{\\code{Acceleration - resultant (m/s²)}}{double COLUMN_DESCRIPTION} \n#'}\n#' @source \\url{http://somewhere.important.com/}\n\"mydataset\"\n\n\n\nDo that for each data frame in your package, adding each skeleton to R/data.R. Edit the parts in ALL_CAPS. sinew also provides RStudio addins, available in the Addins menu. Read their documentation to learn how to use them.\nFor a bit more detail on documenting data packages, see http://r-pkgs.had.co.nz/data.html.\nBuilding your data package\nThe final step in creating a data package is to build the package along with the new documentation.\nIn the Build tab, first select Document from the More menu – this will generate the documentation from from the roxygen markup you created above. Then click Install and Restart (or Build and Reload). You should now be able to access the help page for each data frame in the standard way:\n\n\n?mydataset\n\n\n\nYou’re done! Just remember, when you change something, update the version number in the DESCRIPTION file and rebuild your package.\n\n\n\n",
    "preview": "posts/2017-10-18-put-your-data-in-an-r-package/../../images/datapackage_screenshot.png",
    "last_modified": "2021-04-25T09:59:49-07:00",
    "input_file": {},
    "preview_width": 957,
    "preview_height": 471
  },
  {
    "path": "posts/2015-06-27-monkey-butts-menstrual-cycles-sex-and-the-color-pink-the-statistical-crisis-in-science/",
    "title": "Monkey butts, menstrual cycles, sex, and the color pink. The statistical crisis in science",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2015-06-27",
    "categories": [],
    "contents": "\nA couple of years ago, when I was starting a new project that used cross-national data, I picked up a popular statistics textbook by Andrew Gelman and Jennifer Hill because Gelman is known for his analysis of voting patterns across US states, which is conceptually similar to analysis of data across countries. I decided to google Gelman, a prominent statistics professor at Columbia, to see if he had papers on his website that would provide detailed, published examples of these kinds of analyses (’bout ready to head back to Facebook?).\nIt turned out Gelman is a prolific blogger. At the time, in Slate and on his blog, Gelman was accusing authors of four papers of shoddy stats. One paper was the infamous paper on ESP by Daryl Bem that had already been debunked. But the three other papers were on evolutionary psychology, and two authors of one paper were my former advisors John Tooby and Leda Cosmides.\nThat got my attention.\nMy first impression was that Gelman had found a new way to attack ev psych: cherry pick a few EP papers with questionable stats, and use those to tar the field.\nMost of the discussion on Gelman’s blog involved the paper by Alec Beall and Jessica Tracy, “Women more likely to wear red or pink at peak fertility,” published in Psychological Science, one of psychology’s top journals. The inspiration for the study was the fact that “females in many closely related species signal their fertile window in an observable manner, often involving red or pink coloration.”\nVictorian cartoonMonkey butts, menstrual cycles, sex, and the color pink. Nothing to mock there. However, the study is more important than it sounds. If Beall and Tracy were right, human female ovulation is not concealed after all.\nIn Slate, Gelman accused Beall and Tracy of conducting a fishing expedition — repeatedly making comparisons until they found one that was “statistically significant” and then reporting it as if they had predicted it in advance — a big, though often inadvertent, no-no that he claimed was widespread in science:\n\nThere’s a larger statistical point to be made here, which is that as long as studies are conducted as fishing expeditions, with a willingness to look hard for patterns and report any comparisons that happen to be statistically significant, we will see lots of dramatic claims based on data patterns that don’t represent anything real in the general population. Again, this fishing can be done implicitly, without the researchers even realizing that they are making a series of choices enabling them to over-interpret patterns in their data.\n\nEvery scientist knows that fishing is wrong, so Beall and Tracy were understandably upset that Gelman accused them of unethical conduct without talking to them first.\nI hadn’t met either author (so far as I know), so I googled them. According to Beall’s cv, he was a grad student and this was his first, first-authored paper. What a great introduction to academia! Get published in Psych Science on your first effort, only to be slammed by a prominent statistician, repeatedly. I became a regular reader of Gelman’s blog, and a week rarely goes by that he doesn’t work in some dig at the Beall and Tracy paper. In fact, I was inspired to write this post after a recent dustup over Beall and Tracy’s refusal to share their data with Gelman since Gelman wouldn’t tell them where he was publishing his critique (they have shared their data with others).\nGelman, who presented himself as a disinterested critic who only wanted to improve science, had several criticisms of the paper, but I am going to focus on one: Gelman guessed that Beall and Tracy had originally predicted that, like female chimpanzees who signal estrus with a prominent red swelling, women would tend to choose red shirts around the time of ovulation. Gelman surmised that after collecting their data Beall and Tracy noticed that there was no statistically significant tendency to wear red shirts during the fertile period (red only: p >.05), but if women wearing red shirts and women wearing pink shirts were combined into a single, new category (reddish shirts), there was a statistically significant effect (red + pink: p<.05).\nI was a little confused. Researchers often combine conditions and no one blinks an eye (and Beall and Tracy deny they did this). Further, there was no evidence that Beall and Tracy had repeatedly tested various combinations of shirt colors to find one that was “significantly” associated with the fertile period of women’s menstrual cycles, and then, after the fact, had come up with some theory to explain it, which would be a classic fishing expedition.\nGelman began to walk back his accusation that Beall and Tracy went fishing. Instead, in a paper with Eric Loken in American Scientist, and on his blog, Gelman claimed that multiple comparisons can be a problem, even when the research hypothesis was posited ahead of time and researchers only conduct one statistical test.\nHuh?\nGelman’s own stats textbook didn’t mention anything about this. In fact, in the textbook and in a publication, Gelman et al. claimed “we (usually) don’t have to worry about multiple comparisons,” an irony he himself noted. Further, why pick on a grad student? Or evolutionary psychology? The problem, if there was one, would be pervasive throughout all the sciences.\nMy first impression was right. A reader of Gelman’s wondered why he spent so much time criticizing a study on pink shirts instead of the statistically flawed medical research that actually harms people, and Gelman admitted he had an agenda: surprise, surprise, he doesn’t like evolutionary psychology:\n\nBut I do think these social psychology studies make a difference too, in that they feed the idea that people are shallow and capricious, that we make all sorts of decisions based on our animal instincts etc. Sure, some of that is true but not to the extent claimed by those clueless researchers.\n\n\nTo erroneously connect fat arms [the paper co-authored by Tooby and Cosmides] or monthly cycles to political attitudes is to trivialize political attitudes, and I think that’s a mistake, whatever your politics.\n\nWell, Gelman also makes mistakes: singling out a few articles on one side of a debate that supposedly have statistical flaws, but not looking at articles on the other side of the debate, obviously says nothing about who’s right (even assuming such a debate exists; Gelman knows very little about evolutionary psychology).\nStill, if Gelman’s general points were correct, I realized I could easily be making the same mistakes in my own research, and so too would a lot of other people. With some chagrin, I had to admit that although I was aware of the many debates surrounding Frequentist vs. Bayesian approaches to statistical inference, I had never thought critically about the foundation of them both: probability.\nFollowing pointers on Gelman’s blog, I started reading. Late to the party as always, I discovered that many statisticians, such as John Ioannidis, and other quantitative types, such as Uri Simonsohn, Joseph Simmons, and Leif Nelson, believed there was a statistical crisis in science, and that it was possible that most research findings were false. Yikes! After pouring through many articles and blogs, I concluded that although the tools statisticians have given us are powerful, they are very brittle and easy to break because probability is easy to get wrong.\nThe world is noisy and we humans have a propensity to see patterns in this noise where none exist. To distinguish signal from noise, most scientists therefore rely on the following expression:\nP(D|H0)\nwhich is the probability that your data (D) would turn out the way they did given some null hypothesis (H0), such as no difference in shirt color during peak fertility. If p is small, you’re looking at a signal and not noise. Yea! This is called Null Hypothesis Significance Testing (NHST).\nThere are many critics of NHST. But the problems that Gelman and others were highlighting actually did not involve NHST per se. Instead, they were making a “garbage in, garbage out” argument. The probabilities spit out by standard software packages — p-values — were, in too many cases, misleadingly small because scientists had inadvertently fed the software the answers they wanted to see.\nProbability is a subtle concept (and I’m probably going to screw it up right now!). To illustrate the problem Gelman and others have found, I will use a casino example. Imagine that you have a die you suspect is loaded. You roll it 2 times, and it comes up 3 each time. Can you reject the null that the die is fair? You might think that because the probability of rolling one three with a fair die is 1/6, and you’ve rolled two threes, the probability under the null is (1/6)^2, which is about 0.028, so you can reject the null hypothesis that the die is fair.\nIf that’s what you thought, you would be wrong, because it’s a trick question. If you had called two threes before rolling the die, that is, you suspected it was loaded to come up threes, and then it came up all threes, you could conclude that the die is (probably) loaded. But I asked you to test the null by computing the probability of two threes based on already having rolled two threes.\nHey, you might respond, it’s odd that the die came up three both times, right? Yes it is. It would also be odd if it came up two ones, or two twos, or two fours, etc., and I gave you no reason to suspect one of these possibilities over the others. The probability that it came up two ones OR two twos OR two threes OR two fours OR two fives OR two sixes is about 0.17, i.e., not odd at all. You can’t test the null by computing a probability as if you hadn’t seen the faces of the die if your choice of probability test is based on having seen those faces. This is the core problem.\nIn science, collecting data is the analog of rolling the dice. That’s what P(D|H0) means. To use this probability to distinguish signal from noise, we scientists must therefore make a very precise prediction before collecting or looking at the data because the p-values our stat programs compute are only accurate if our “call” was not influenced by the data.\nBut we all have looked at the data. Almost every statistics textbook, including Gelman’s, recommends that we pour over our data, checking distributional assumptions and so forth, before running any test. And many common choices during data analysis, such as controlling for potential confounds, looking at interactions, and combining conditions, can dramatically increase the chance of a false positive.\nCurrent best practice in the sciences is like first rolling the dice, then meticulously examining the numbers that land face up, but promising to not let anything we learn about those numbers influence any aspect of our call.\nThink any casino would play by those rules?\nGelman and others are not criticizing NHST per se (or rather, that’s a separate argument). They are showing how easy it is to inadvertently break NHST.\nIn another irony, Gelman grounds this crisis in human nature: just like the gambler who has the strong monetary incentive to beat the house, scientists can usually only publish (and thus get credit for) “statistically significant” results, and therefore have an incentive to find some justification for altering their predictions after looking at the data (e.g., red+pink, not just red). That is, many scientists are pursuing their self-interest and taking a benefit they don’t deserve, and Gelman’s cheater detection mechanisms are on full alert. Just the phenomenon that put evolutionary psychology on the map!\nAlthough deliberate or inadvertent cheating is certainly part of the story, I want to offer a different framing: scientists currently face an untenable tradeoff between learning about the world, and confirming what they’ve learned. If Beall and Tracy had discovered, rather than predicted, that women tend to wear pink shirts and red shirts during the fertile phase of their cycle, that would be important. Discovering that you need to control for, e.g., age, would be important. After all, aren’t scientists supposed to discover things? Combining the two colors, and studying “reddish” clothing, would be excellent science. So would controlling for previously unsuspected confounds. Unfortunately, these would break the computation of statistical significance that tells us this is a real effect and not just a fluke. Good science can be bad statistics, and good statistics can be bad science.\nScience is screwed.\nThere is a way out of this mess, as statistician John Tukey realized decades ago. I’ll discuss that in my next post. For now, I will just note that there is a long history of ridiculing approaches to human sexuality that take account of our primate heritage. As targets of such ridicule, Beall and Tracy are in pretty good company:\n\nIN the discussion on Sexual Selection in my “Descent of Man,” no case interested and perplexed me so much as the brightly-coloured hinder ends and adjoining parts of certain monkeys. As these parts are more brightly coloured in one sex than the other, and as they become more brilliant during the season of love, I concluded that the colours had been gained as a sexual attraction. I was well aware that I thus laid myself open to ridicule; though in fact it is not more surprising that a monkey should display his bright-red hinder end than that a peacock should display his magnificent tail.\n\nCharles Darwin (1876) Sexual Selection in Relation to Monkeys. Nature, 15, 18-19.\n\n\n\n\n",
    "preview": "posts/2015-06-27-monkey-butts-menstrual-cycles-sex-and-the-color-pink-the-statistical-crisis-in-science/../../images/Darwin_sexual_caricature.gif",
    "last_modified": "2021-04-25T08:43:58-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-05-16-pavlovs-dogs-dopamine-and-drug-use/",
    "title": "Pavlov's dogs, dopamine and drug use",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2015-05-16",
    "categories": [],
    "contents": "\n\nFindings from studies investigating only stimulants (generally cocaine or amphetamine) were often discussed as though they applied to all addictions, even though there was no evidence for such an assumption. (Nutt et al. 2015)\n\nIs it possible to get all the facts right, but still get the explanation wrong? That can happen to anyone. Nutt et al. (2015), writing in Nature Reviews Neuroscience, argue that it’s happened to an entire subfield, the neuroscience of substance use and addiction.\nOlds and MilnerThe neuroscience of substance use is grounded in Pavlovian concepts that wield powerful and persistent influences over the behavioral sciences. The story begins over 60 years ago when James Olds and Peter Milner inserted wires into the septal areas of the brains of four rats. The rats were then tested in a Skinner box with a lever (above, right). When the rats pressed the lever, a small voltage was applied to the wire. The rats responded by pressing the lever hundreds of times an hour. Olds and Milner concluded that the “control exercised over the animal’s behavior by means of this reward is extreme, possibly exceeding that exercised by any other reward previously used in animal experimentation” (Old and Milner 1954, p. 426).\nThe critical neurons turned out to be dopamine neurons in the midbrain. It was also discovered that various drugs of abuse increase dopamine release in this region. These and other findings lead Roy Wise and colleagues to propose the “hedonia” hypothesis: dopamine encoded reward itself; it was a “pleasure” molecule. If so, the role of dopamine in both reinforcement learning and drug use was clear: behaviors that resulted in food or sex – “natural” rewards – cause the release of dopamine in the MDS, whose pleasurable effects reinforce those behaviors. Drugs of abuse – conceptualized as “artificial” rewards that “hijack” the brain – also release dopamine, whose pleasurable effects similarly reinforce drug use.\nThus were born two intimately intertwined theories: the dopamine theory of reinforcement learning, and the dopamine theory of substance use and addiction, each deeply rooted in the stimulus-response paradigm at the core of behaviorism.\nTwenty years of experiments using these and other methods showed that although dopamine plays some important role in reinforcement learning, it does not directly mediate the hedonic effects of rewards. In an interview in Science (Wickelgren, 1997), Roy Wise acknowledged that the hedonia hypothesis was wrong (time to update the NIDA website!).\nOne alternative hypothesis is that mesolimbic dopamine mediates wanting, not liking. Key evidence for this hypothesis is that after 6-hydroxydopamine lesioning of their mesolimbic dopamine neurons, rats still seem to enjoy food and sex, but are no longer motivated to seek them out (Berridge 1996). On this view, drug-induced dopamine release hijacks the brain by inducing craving for the drug, but not pleasure at consuming it (Robinson and Berridge, 1993).\nAnother influential idea, based on electrophysiological recordings of dopamine neurons in monkeys, and with roots in the experiments of Pavlov, is that the phasic activity of mesolimbic dopamine neurons encodes reward prediction error: dopamine neurons spike with unexpected rewards, and their activity is suppressed with unexpected absence of rewards. But they don’t spike when monkeys get an expected reward (for an exhaustive review, see Schultz, in press). According to Glimcher (2011, p. 15647), who eloquently defends this theory,\n\n[I]ntertwining of theory and experiment now suggests very clearly that the phasic activity of the midbrain dopamine neurons provides a global mechanism for synaptic modification. These synaptic modifications, in turn, provide the mechanistic underpinning for a specific class of reinforcement learning mechanisms that now seem to underlie much of human and animal behavior. (emphasis added)\n\nThis ambitious theory has a harder time explaining how drugs hijack the brain (see Schultz, 2011 for some ideas).\nEnter David Nutt and colleagues. Whatever the role of dopamine in reinforcement learning, Nutt et al. argue there are problems with the dopamine theory of drug use that have been swept under the carpet. Foremost among these is that, in experiments in humans that directly measure mesolimbic dopamine concentrations using PET or SPECT scans, some popular drugs of abuse do not increase mesolimbic dopamine much. Here is the key figure in their paper (more negative binding values indicate increased dopamine release):\nNutt et al.As you can see, whereas amphetamine clearly increases dopamine, morphine and THC mostly do not (and results for nicotine are variable).\nNutt et al. argue that the relationship between stimulants and dopamine is not surprising because these drugs act directly on the dopamine system:\n\n[W]hat was overlooked was the fact that methylphenidate and other stimulants act specifically on the dopamine system to increase dopamine levels. Thus, dopamine must be the proximal mediator of any psychological response to stimulants, and it should not be surprising that the change in striatal dopamine release correlates with the subjective high. (emphasis added)\n\nLower availability of D2/D3 receptors is consistently associated with addiction to some drugs, like cocaine, which supports the dopamine theory of drug use. But this association also presents a paradox:\n\nIf dopamine acting through D2 and/or D3 receptors is necessary to experience a drug high, then lower receptor availability should result in less-rewarding rather than more-rewarding drug effects.\n\nLower D2/D3 availability is also not consistently associated with addiction to other drugs, such as opiates, cannabis, or nicotine.\nNutt et al. conclude:\n\nTellingly, the dopamine theory has not led to any new treatments for addiction. We suggest that the role of dopamine in addiction is more complicated than the role proposed in the dopamine theory of reward. We propose that dopamine has a central role in addiction to stimulant drugs, which act directly via the dopamine system, but that it has a less important role, if any, in mediating addiction to other drugs, particularly opiates and cannabis.\n\nUnlike Nutt et al., I don’t fault drug researchers for the dopamine theory. On the contrary, good theories are ambitious, with clear predictions that can be falsified by evidence. It is also easy, when in the grip of a compelling theory with much supporting evidence, to dismiss findings that contradict the theory.\nStill, by highlighting evidence that contradicts the dominant dopamine model of drug use, Nutt et al. bolster our critique of the dopamine model’s evolutionary rationale that drugs are evolutionary novel, provide no benefits, and “hijack” the brain via these dopamine circuits.\nThe image at the top of this post epitomizes an approach to studying evolved animal mechanisms, such as reinforcement learning mechanisms, that often abstracts away critical details of the environment that selected for the evolved mechanism. Most popular drugs are plant defensive chemicals, or their close chemical analogs (alcohol is the exception). Plant defensive compounds have infused animal diets since at least the time terrestrial plants and animals evolved, about 400 million years ago. My colleagues and I argue that neurobiological theories of drug use, and perhaps also the related theories that ground much animal behavior in a single neurotransmitter, would profit by incorporating, rather than ignoring, the long co-evolution of plants and animals.\nCarboniferiousReferences\nBerridge, K. C. (1996). Food reward: brain substrates of wanting and liking. Neuroscience & Biobehavioral Reviews, 20(1), 1-25.\nGlimcher, P. W. (2011). Colloquium Paper: Understanding dopamine and reinforcement learning: The dopamine reward prediction error hypothesis. Proceedings of the National Academy of Sciences, 108(Supplement_3), 15647–15654. http://doi.org/10.1073/pnas.1014269108\nHagen, E. H., Roulette, C. J., & Sullivan, R. J. (2013). Explaining Human Recreational Use of “pesticides”: The Neurotoxin Regulation Model of Substance Use vs. the Hijack Model and Implications for Age and Sex Differences in Drug Consumption. Frontiers in Psychiatry, 4. http://doi.org/10.3389/fpsyt.2013.00142\nOlds, J. (1958). Self-Stimulation of the Brain Its Use To Study Local Effects of Hunger, Sex, and Drugs. Science, 127(3294), 315–324. http://doi.org/10.1126/science.127.3294.315\nOlds, J., & Milner, P. (1954). Positive reinforcement produced by electrical stimulation of septal area and other regions of rat brain. Journal of Comparative and Physiological Psychology, 47(6), 419–427.\nNutt, D. J., Lingford-Hughes, A., Erritzoe, D., & Stokes, P. R. (2015). The dopamine theory of addiction: 40 years of highs and lows. Nature Reviews Neuroscience, 16(5), 305-312. http://doi.org/10.1038/nrn3939\nRobinson, T. E., & Berridge, K. C. (1993). The neural basis of drug craving: an incentive-sensitization theory of addiction. Brain Research Reviews, 18(3), 247-291.\nSchultz, W. (2011). Potential Vulnerabilities of Neuronal Reward, Risk, and Decision Mechanisms to Addictive Drugs. Neuron, 69(4), 603–617. http://doi.org/10.1016/j.neuron.2011.02.014\nSchultz, W. (in press). Neuronal reward and decision signals: from theories to data. Physiological Reviews. http://www.pdn.cam.ac.uk/staff/schultz/pdfs%20website/2015%20Schultz%20PhysiolRev%20in%20press.pdf\nWickelgren, I. (1997). Getting the brain’s attention. Science, 278(5335), 35-37.\nFigure 1: Modified from Olds, 1958.\nFigure 2: From Nutt et al., 2015.\nFigure 3: Modified from “Carboniferous Pteridophyta (After Dana)” from a the 1896 edition of Underwood’s Native Ferns and their Allies.\n\n\n\n",
    "preview": "posts/2015-05-16-pavlovs-dogs-dopamine-and-drug-use/../../images/olds_milner.2.jpg",
    "last_modified": "2021-04-25T08:46:12-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-05-15-im-a-sucker-for-a-good-theory/",
    "title": "I'm a sucker for a good theory",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2015-05-15",
    "categories": [],
    "contents": "\nI’m a sucker for a good theory.\nAfter getting my BA, I had no idea what I wanted to do with my life. A friend, perhaps exasperated with my refusal to commit to a career, introduced me to her brother-in-law, Bruce Novak, who had just gotten a position in the UC Berkeley Department of Chemistry. Bruce had recently finished his PhD at Cal Tech (under Grubbs, who, to name drop, would later win the Nobel Prize in Chemistry). Bruce needed someone to set up his lab and get projects started, and he asked me if I wanted to do it. I have no idea why he asked me – I had little more than a high school education in chemistry. Also, Bruce was a polymer chemist. Plastics. There’s a great future in plastics. Just not the future I wanted for myself. But hey, it was a paycheck, so I agreed.\nI soon realized that polymer chemistry was the ultimate geek discipline: you build cool stuff at the molecular level. Some molecules have these properties, others have those properties. If you can hook them together in a polymer, you’ve now got a material that has both.\nBruce had a very clever idea. Polyphenylene sulfide (PPS) is a tough, light brown, sulfur-based organic polymer that is normally an electrical insulator. PPS p-orbitals overlap, however. Oxidation removes electrons from the overlapping orbitals, which then form a molecule-wide electron conduction band, very similar to the conduction band in metals. Upon this so-called “doping,” PPS becomes dark and shiny (just like metal!), and conducts electricity along its backbone. Yes, you can turn some plastics into “metals,” retaining advantages of both.\nHere’s PPS (the bracket with the “n” subscript means repeat n times):\npolyphenylene sulfideThere was another group of sulfur-based molecules with a very similar chemical structure, triarylsulfonium salts, that were photo-reactive:\ntriarylsulfonium_reactionThe key, here, is that when you photolyze this molecule (shine light on it, hv), you knock off one of the rings, creating a radical cation – a charged molecule with an unpaired electron (the S+ with the dot over it). If we could make a polymeric form of this molecule, which we dubbed arylated PPS (APPS), it would be very similar to PPS. When we hit it with light, the unpaired electron on the polymer backbone should convert APPS into a conducting polymer – “photo-doping.” In other words, we could imprint conducting circuits directly into a non-conducting plastic simply by exposing it to light.\nHere was the reaction with which we hoped to create APPS (the polymer on the right hand side):\nreactionPPS was hard to work with. It would only dissolve in chloronapthalene at 220 C, conditions that destroyed the other starting material. After weeks of unsuccessful attempts, my undergraduate assistant, Anand Viswanathan, and I finally made something that looked like a light brown polymer, but unlike PPS, it dissolved in acetone at room temperature, as we predicted APPS would do.\nWhat was it? What is photoreactive? Would it conduct electricity? I quickly made a thin film of the stuff. After it dried, I grabbed the lab multimeter, put it on the ohm setting, and stuck the probes on the film. The display was blank – no measurable conductivity. Good. Then I shined a UV light on the film. It turned dark and shiny. I put the probes on again, and the multimeter display started to blink, settling down in the MΩ range. The film was conducting electricity. Not well–it was almost beyond the sensitivity of the meter–but it was conducting!\nMonths of work followed to confirm we’d made what we thought we’d made, that it reacted with light the way we thought it should, and that electrical conduction was based on that radical on the polymer backbone. I did countless NMR’s, UV/vis spectrograms, elemental analyses, and electron spin resonance analyses that all pointed to the same conclusion: we had made APPS, and when we hit it with UV light, we would cleave off a phenyl group, forming a radical cation on the polymer backbone. The material would become reflective (an indicator that an electron conduction band had formed), and it would conduct electricity. When we quenched the radical cation with water, we eliminated electrical conductivity.\nIt was time to submit, to Science. Bruce gave me the first author slot. He waited patiently for me to finish my draft, and then rewrote the whole thing (precisely one sentence of mine remained).\nWhile our paper was under review at Science, we wanted to determine the true conductivity of APPS, and increase it, if possible. The challenge was that as soon as we photolyzed the film, it would start to reflect light, limiting further photolyzation. We therefore had an extremely thin conducting layer, but our calculation of conductivity was based on the entire thickness of the, mostly unphotolyzed, film; our estimated conductivity value was therefore far too low. This problem was compounded by the fact that our low conductivity was near the limit that my multimeter could detect. Worse, the photolyzed film would immediately start to absorb moisture, which quenched the radical, killing the conductivity. We were confident that if we could solve these problems, we would achieve much higher conductivity than we had seen so far.\nI had tried various quick-and-dirty solutions. I flooded the film with a stream of nitrogen, to keep off moisture, but the building nitrogen was contaminated with water. I pumped my equipment into a dry box, but the static electricity played havoc with my ohm meter. Plus, it was hard to work with a delicate film using large rubber gloves. I built a spinner that created ultra thin films of APPS, but this didn’t seem to improve the conductivity values, probably because my meter wasn’t sensitive enough.\nWe finally decided to do things right. We needed to run our experiments under dry nitrogen from start to finish, with the thinnest possible film, and with a conductivity meter that could accurately measure the very low values we would get with such a thin film.\nWe bought what was basically a $10,000 ohm meter that was orders of magnitude more sensitive than my multimeter, and I built a special airtight chamber with a UV light underneath a quartz plate (which transmits UV light), flooded with nitrogen straight off a liquid nitrogen dewar (dry dry dry). We would spin an ultra thin film of APPS on a quartz slide and let it dry in the chamber under a flood of dry nitrogen. We would then lower the conductivity probes and turn on the UV light. If everything worked, we should measure the true, and hopefully much higher, conductivity.\nThe day I finally had everything assembled, we heard back from Science. Our paper was accepted! The copy editing on our article was even already complete. If we could get the higher conductivity numbers in the paper, that would be icing on the cake.\nI rushed into the lab and got the experiment running. I spun a fresh film and put it in the chamber. After a lengthy purge with dry nitrogen, I lowered the probes and measured the conductivity. Nothing. Good. I then flipped the switch on the UV light, and waited. Still no conductivity. Huh? I opened the unit to confirm that the probes were in contact with the film, and the conductivity meter started to register. Uh oh. I tried another film. Nitrogen purge + UV: zero conductivity. Open the unit: conductivity. I tried another film. Same thing: no conductivity until I exposed the film to air. And another. Same thing. The implication was obvious, and devastating to the project. I called Bruce in to show him what was happening. By this time, the entire lab had gathered around. Tom, my dour labmate, pronounced the epitaph: “It’s water.”\nSee that MX- in the figure, above right, hovering near the S+? APPS is a salt. Add water to salt, and you get electrical conduction, not by a cool electronic conduction mechanism along the polymer backbone, but by an ordinary ionic mechanism. All along we had been looking at ionic conduction in water absorbed from the air, not electronic conduction.\nWe thought we had ruled out ionic conduction. Ionic conduction would not make our film shiny and reflective. With an ionic conduction mechanism, which would not depend on the presence of a radical, water should have increased conductivity. When we deliberately added water to our film, though, it quenched the radical, and also the conductivity, as expected for the electronic mechanism. Somehow, the small amount of moisture in the air was enough to create a thin salty mixture that would conduct, but adding more water degraded the photolyzed polymer so it no longer did.\nThat afternoon, I flipped back through my lab book, looking at the notes I had taken on hundreds of experiments that mostly yielded positive results. How had I missed this? What I now saw was that every time I got a negative result under exceptionally dry conditions, I had dismissed it: the building nitrogen was wet; the static in the dry box was screwing with my electronics; the conductivity in such thin films was too low for my multimeter. Yet the truth was there, hovering like a Romulan battleship just at the edge of sensor range.\nThe frustrating thing was that all our chemistry was right. We had made what we thought we’d made – a brand new photopolymer – and it reacted with light the way we predicted. Every fact we reported, was, in fact, a fact (and our results were eventually published in a respectable journal). The problem is that the evidence for electronic conduction mechanisms in polymers is almost always circumstantial. The same evidence that demonstrated electronic conduction in other polymers was misleading in our case.\nI had allowed myself to be seduced by a very plausible, and seemingly well-supported, theory.\nDriving home that evening as the sun set over San Francisco Bay, I was upbeat, almost euphoric. The truth was, I didn’t want to be a polymer chemist. Over the past year, I had been sneaking off to attend classes by George Lakeoff, John Searle, and Brent Berlin. I was fascinated by theories of cognition and human nature, and I had been reading voraciously in linguistics, philosophy, and anthropology. If our work had been published in Science I would have felt obligated to continue on the project for at least another year, maybe more. Now, with our submission withdrawn and the project more or less dead, I was free.\nA few weeks earlier, the same friend who had introduced me to Bruce bought me a book she had seen in a Berkeley bookstore: Intimate Fathers by Barry Hewlett, a study of parenting among Aka pygmies. This has “Ed” written all over it, she said. She was right.\nI was jumping, though, from the frying pan into the fire. The cognitive and behavioral sciences, I would soon learn, are, like our APPS Science paper, based on seductive theories backed by circumstantial evidence.\n\n\n\n",
    "preview": "posts/2015-05-15-im-a-sucker-for-a-good-theory/../../images/Polyphenylene_sulfide.svg",
    "last_modified": "2021-04-25T08:48:29-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-04-19-is-pregnancy-immunosuppression-a-myth/",
    "title": "Is pregnancy immunosuppression a myth?",
    "description": {},
    "author": [
      {
        "name": "Ed Hagen",
        "url": "https://anthro.vancouver.wsu.edu/people/hagen"
      }
    ],
    "date": "2015-04-19",
    "categories": [],
    "contents": "\nMy grad student Caity Placek is working on behavioral immunity during pregnancy. Behavioral immunity refers to psychological adaptations that help defend against pathogens; avoidance of sick people would be a possible example.\nCaity’s results suggest that aversion to meat in pregnancy is related to infection risk, especially in the first trimester. This supports an influential hypothesis put forward in 2000 by Flaxman and Sherman that these aversions are due to the immunosuppression that also occurs early in pregnancy. The rationale is that, during human evolution, meat often harbored pathogens, so immunosuppressed pregnant women should have evolved to avoid it.\nThe idea that pregnant women are immunosuppressed originated with Peter Medawar, who won a Nobel prize for his work on tissue grafts. In 1953, Medawar pointed out that the fetus was like an organ transplant: with half of its genes coming from dad, it produced many proteins that were foreign to mom. Why, then, didn’t mom’s immune system attack the fetus as it would a transplanted organ? In an article billed as “the most influential contribution made to the development of the field of Reproductive Immunology” (Billington 2003), Medawar asked\n\nThe immunological problem of pregnancy may be formulated thus: how does the pregnant mother contrive to nourish within itself, for many weeks or months, a foetus that is an antigenically foreign body?\n\nFor the next 40 years, much research on this “immunological paradox” was conducted as if the fetal ‘allograft’ were like a surgically transplanted organ (Erlebacher 2013). One solution to the paradox offered by Medawar was “the immunological indolence or inertness of the [pregnant] mother.” In other words, to avoid rejecting the fetus as they would a transplanted kidney, pregnant women were naturally immunosuppressed.\nA more recent variant of this hypothesis involved the Th1-Th2 paradigm, in which Th1 cells express proinflammatory cytokines, whereas Th2 cells secrete cytokines promoting humoral immunity, and either pathway can down-regulate the other. The pregnancy state was thought to be a Th2 biased state, to avoid an inflammatory response to the fetus.\nBut are pregnant women immunosuppressed, or in a Th2 biased state? When Caity and I started looking into the literature on pregnancy immunosuppression, we found that opinion seems to have shifted dramatically since 2000. Recent reviews on the topic in the New England Journal of Medicine and the American Journal of Reproductive Immunology are heaping skepticism on the idea that pregnant women are immunosuppressed. For instance, Racicot et al. (2014), writing in the American Journal of Reproductive Immunology, state that\n\nThe old concept that pregnancy is associated with immune suppression has created a myth of pregnancy as a state of immunological weakness and therefore of increased susceptibility to infectious diseases. Today, there is increasing evidence suggesting that this concept is incorrect and the immune system during pregnancy is functional and highly active.\n\nIn the same journal, Silasi et al. (2015) write\n\nOne of the main hypotheses used to explain the increased risk for infection and mortality during pregnancy has been the concept of ‘pregnancy as an immune-suppressed condition’. The ‘paradox of pregnancy’ as a semi-allograft has been approached from the point of perspective of organ transplantation. The view of the fetus as an organ transplant, and the requirement of systemic immune suppression for the success of the transplant, led to the proposal of pregnancy as a condition of systemic immune suppression as a requirement for the success of the pregnancy. From this point of view, similarly as in immune-suppressed patients, pregnancy is in a state of weak immunologic protection.\n\n\nThis concept has been tested for many years in animal models as well as in patients with fertility problems. Unfortunately, after almost 50 years of research following this assumption, there is a lack of evidence to support this hypothesis.\n\nWriting in the New England Journal of Medicine, Kourtis et al. (2014) similarly conclude\n\nElucidation of the immunologic alterations and adaptations that occur during pregnancy suggests that older concepts of pregnancy as a state of systemic immunosuppression are oversimplified. A more useful model may be the view of pregnancy as a modulated immunologic condition, not a state of immunosuppression.\n\nThe evidence against systemic (emphasis on systemic) immunosuppression in pregnancy is pretty compelling.\nThere is little epidemiological evidence that pregnant women are more vulnerable to infection. In a review of pregnancy and infection, Kourtis et al. (2014) state “The fact that pregnant women do not seem, on the basis of epidemiologic evidence, to be more susceptible to infections in general also contradicts [the pregnancy immunosuppression] theory.” Keeping in mind that, for some infectious diseases, there are national and global surveillance programs involving millions of patients, and that public health researchers are particularly concerned about infections during pregnancy, this is a powerful blow to the theory.\nMy first thought was that maybe behavioral immunity compensates for immunosuppression, but no, pregnant women respond well to vaccines: “Adequate immunologic responses to vaccination in pregnant women have been demonstrated in several studies and for several pathogens” (Kourtis et al. 2014), and these responses do not appear to depend significantly on trimester (Pazos et al. 2012).\nFurther, researchers find that the immune system is “functional and highly active” during pregnancy (Racicot et al. 2014). And pregnant women’s immune systems do detect and respond to the fetus. The title of this paper says it all: Fetal-Specific CD8+ Cytotoxic T Cell Responses Develop during Normal Human Pregnancy and Exhibit Broad Functional Capacity (Lissauer et al. 2012).\nWhat changed?\nFirst, it is now recognized that many shifts in immunity during pregnancy are specific to the maternal-fetal interface (the uterus and placenta), and these must be clearly distinguished from systemic immune changes. In fact, the Th1-Th2 model of pregnancy was based mostly on studies of the maternal-fetal interface, and might not apply to systemic immunity (Pazos et al. 2012):\n\nMost evidence supporting a Th2 shifts derives from studies of [the maternal/fetal] interface rather than systemic immunity. Although inflammatory events have been shown to be important at critical times at the beginning and end of gestation, for the most part, the uterine environment is biased toward Th2 [34]. Arguments for a Th2 bias in the periphery are much more contentious [35].\n\nSecond, the fetus and placenta are now seen as an active players that generally cooperate with the maternal immune system to provide immunity during pregnancy (Mor et al. 2010).\nThird, earlier views were heavily influenced by studies of pregnancy complications and loss in humans, as well as mouse models, which might not translate to immune responses during healthy human pregnancies.\nWhat didn’t change?\nHow the mother tolerates the semiallogeneic fetus is still seen as an important and not fully understood problem.\nAlthough pregnant women are generally not more susceptible to infection, there is solid evidence that if they do become infected, the consequences are more severe. Infections with influenza, hepatitis E virus, herpes simplex virus, malaria, measles, smallpox, HIV, varicella, and coccidioidomycosis are all more severe in pregnant women (Kourtis et al. 2014).\nPregnant women do seem to be more susceptible to infection with a few pathogens, in particular malaria and listeriosis (Kourtis et al. 2014). Listeriosis is a food borne bacteria, often found on raw dairy products and meat. Hmmm.\nThere are shifts in immunity during pregnancy. A recent longitudinal study of healthy pregnancies, for example, found that “pregnancy is not a period of immunosuppression but an alteration in immune priorities characterized by a strengthening of innate immune barriers and a concomitant reduction in adaptive/inflammatory immunity in the later stages of pregnancy” (Kraus et al. 2012).\nWhere does this leave Caity? And Flaxman and Sherman? Here are a few obvious possible reinterpretations.\nIn the >100 million years since the evolution of viviparity, sophisticated (but not infallible) mechanisms have evolved in both mother and fetus to allow the immune system to be fully active against pathogens without jeopardizing the fetus. Many of these mechanisms are localized to the uterine environment.\nPregnant women might have evolved an aversion to meat, not because they are more vulnerable to infection, but because the costs of infection are higher during pregnancy. It is also possible that some pregnancy-related aversions are specific to particular pathogens that do more easily infect pregnant women.\nPregnancy-related shifts in immunity imply shifts in trade-offs: improvements to some aspects of immunity but detriments to others. These might be related to the energetic cost of pregnancy, to the challenges of a semiallogeneic fetus, and/or to the need for placental, fetal, and maternal immune mechanisms to coordinate. Perhaps, a la Haig, there are conflicts with paternal genes that pose special challenges.\nFinally, there are still many unknowns and much debate. To give the last word to Medawar, he believed the key factor ensuring the success of gestation was not maternal immunosuppression but rather “the anatomical separation of foetus from mother,” a basic conclusion that, according to Billington (2003), remains substantially valid to this day.\nReferences\nBillington, W. D. (2003). The immunological problem of pregnancy: 50 years with the hope of progress. A tribute to Peter Medawar. Journal of Reproductive Immunology, 60(1), 1–11. http://doi.org/10.1016/S0165-0378(03)00083-4\nErlebacher, A. (2013). Mechanisms of T cell tolerance towards the allogeneic fetus. Nature Reviews Immunology, 13(1), 23–33. http://doi.org/10.1038/nri3361\nFlaxman, S. M., & Sherman, P. W. (2000). Morning Sickness: A Mechanism for Protecting Mother and Embryo. The Quarterly Review of Biology, 75(2), 113–148. http://doi.org/10.2307/2664252\nKourtis, A. P., Read, J. S., & Jamieson, D. J. (2014). Pregnancy and Infection. New England Journal of Medicine, 370(23), 2211–2218. http://doi.org/10.1056/NEJMra1213566\nLissauer, D., Piper, K., Goodyear, O., Kilby, M. D., & Moss, P. A. H. (2012). Fetal-Specific CD8+ Cytotoxic T Cell Responses Develop during Normal Human Pregnancy and Exhibit Broad Functional Capacity. The Journal of Immunology, 189(2), 1072–1080. http://doi.org/10.4049/jimmunol.1200544\nMedawar, P. B. (1953, January). Some immunological and endocrinological problems raised by the evolution of viviparity in vertebrates. In Symp Soc Exp Biol (Vol. 7, No. 320, p. 38).\nMor, G., & Cardenas, I. (2010). The Immune System in Pregnancy: A Unique Complexity: IMMUNE SYSTEM IN PREGNANCY. American Journal of Reproductive Immunology, 63(6), 425–433. http://doi.org/10.1111/j.1600-0897.2010.00836.x\nPazos, M., Sperling, R. S., Moran, T. M., & Kraus, T. A. (2012). The influence of pregnancy on systemic immunity. Immunologic Research, 54(1-3), 254–261. http://doi.org/10.1007/s12026-012-8303-9\nPlacek, C. (2015) Fetal protection: The roles of social learning and innate food aversions in South India. The International Society for Evolution, Medicine, & Public Health Inaugural Meeting, March 19-21, 2015 in Tempe, Arizona.\nRacicot, K., Kwon, J.-Y., Aldo, P., Silasi, M., & Mor, G. (2014). Understanding the Complexity of the Immune System during Pregnancy. American Journal of Reproductive Immunology, 72(2), 107–116. http://doi.org/10.1111/aji.12289\nSilasi, M., Cardenas, I., Kwon, J.-Y., Racicot, K., Aldo, P., & Mor, G. (2015). Viral Infections During Pregnancy. American Journal of Reproductive Immunology, 73(3), 199–213. http://doi.org/10.1111/aji.12355\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-25T08:56:10-07:00",
    "input_file": {}
  }
]
